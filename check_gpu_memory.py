#!/usr/bin/env python3
"""
GPUæ˜¾å­˜æ£€æŸ¥å’Œç®¡ç†å·¥å…·
"""

import torch
import subprocess
import sys
import os

def check_gpu_memory():
    """æ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    print("ğŸ” æ£€æŸ¥GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    num_gpus = torch.cuda.device_count()
    print(f"ğŸ“Š æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU:")
    
    for i in range(num_gpus):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / 1e9
        
        # è·å–å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated(i) / 1e9
        reserved = torch.cuda.memory_reserved(i) / 1e9
        free = total_memory - reserved
        
        print(f"\n  GPU {i}: {props.name}")
        print(f"    æ€»æ˜¾å­˜: {total_memory:.2f}GB")
        print(f"    å·²åˆ†é…: {allocated:.2f}GB")
        print(f"    å·²ä¿ç•™: {reserved:.2f}GB")
        print(f"    å¯ç”¨: {free:.2f}GB")
        
        if free < 5.0:  # å°äº5GBå¯ç”¨æ˜¾å­˜æ—¶è­¦å‘Š
            print(f"    âš ï¸  æ˜¾å­˜ä¸è¶³ï¼")

def clear_gpu_cache():
    """æ¸…ç†GPUç¼“å­˜"""
    print("\nğŸ§¹ æ¸…ç†GPUç¼“å­˜...")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("âœ… PyTorch GPUç¼“å­˜å·²æ¸…ç†")
        
        # å†æ¬¡æ£€æŸ¥æ˜¾å­˜
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            free = (torch.cuda.get_device_properties(i).total_memory - 
                   torch.cuda.memory_reserved(i)) / 1e9
            print(f"  GPU {i} å¯ç”¨æ˜¾å­˜: {free:.2f}GB")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•æ¸…ç†")

def estimate_model_memory(model_size='7B'):
    """ä¼°ç®—æ¨¡å‹æ˜¾å­˜éœ€æ±‚"""
    print(f"\nğŸ’¾ ä¼°ç®—{model_size}æ¨¡å‹æ˜¾å­˜éœ€æ±‚:")
    
    if model_size == '7B':
        params = 7e9
    elif model_size == '1B':
        params = 1e9
    else:
        params = float(model_size.replace('B', 'e9').replace('M', 'e6'))
    
    # FP16å‚æ•°å­˜å‚¨
    param_memory = params * 2 / 1e9
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdaméœ€è¦2å€å‚æ•°ï¼‰
    optimizer_memory = param_memory * 2
    
    # æ¢¯åº¦
    gradient_memory = param_memory
    
    # æ¿€æ´»å€¼ï¼ˆä¼°ç®—ï¼‰
    activation_memory = param_memory * 0.5
    
    total_per_gpu = (param_memory + optimizer_memory + gradient_memory + activation_memory) / 4  # 4GPUåˆ†å¸ƒ
    
    print(f"  å‚æ•°å­˜å‚¨: {param_memory:.2f}GB")
    print(f"  ä¼˜åŒ–å™¨çŠ¶æ€: {optimizer_memory:.2f}GB") 
    print(f"  æ¢¯åº¦: {gradient_memory:.2f}GB")
    print(f"  æ¿€æ´»å€¼: {activation_memory:.2f}GB")
    print(f"  æ€»éœ€æ±‚: {param_memory + optimizer_memory + gradient_memory + activation_memory:.2f}GB")
    print(f"  æ¯GPUéœ€æ±‚(4å¡): {total_per_gpu:.2f}GB")

def kill_gpu_processes():
    """ç»ˆæ­¢å ç”¨GPUçš„è¿›ç¨‹"""
    print("\nğŸ”ª æ£€æŸ¥GPUè¿›ç¨‹...")
    
    try:
        # è¿è¡Œnvidia-smiè·å–è¿›ç¨‹ä¿¡æ¯
        result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,name,used_memory', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0 and result.stdout.strip():
            lines = result.stdout.strip().split('\n')
            print("å½“å‰GPUè¿›ç¨‹:")
            for line in lines:
                parts = line.split(', ')
                if len(parts) >= 3:
                    pid, name, memory = parts[0], parts[1], parts[2]
                    print(f"  PID {pid}: {name} (ä½¿ç”¨ {memory}MB)")
            
            # è¯¢é—®æ˜¯å¦ç»ˆæ­¢è¿›ç¨‹
            print("\nâš ï¸  å‘ç°GPUè¿›ç¨‹ï¼Œæ˜¯å¦ç»ˆæ­¢ï¼Ÿ(y/N)")
            # åœ¨è„šæœ¬ä¸­ä¸ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼Œåªæ˜¾ç¤ºä¿¡æ¯
            print("ğŸ’¡ æ‰‹åŠ¨ç»ˆæ­¢å‘½ä»¤: sudo kill -9 <PID>")
        else:
            print("âœ… æœªæ£€æµ‹åˆ°GPUè¿›ç¨‹")
            
    except FileNotFoundError:
        print("âŒ nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°")

def main():
    parser = argparse.ArgumentParser(description="GPUæ˜¾å­˜æ£€æŸ¥å’Œç®¡ç†")
    parser.add_argument("--clear", action="store_true", help="æ¸…ç†GPUç¼“å­˜")
    parser.add_argument("--estimate", type=str, default="7B", help="ä¼°ç®—æ¨¡å‹æ˜¾å­˜éœ€æ±‚")
    parser.add_argument("--kill", action="store_true", help="æ£€æŸ¥GPUè¿›ç¨‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ GPUæ˜¾å­˜ç®¡ç†å·¥å…·")
    print("=" * 50)
    
    check_gpu_memory()
    
    if args.clear:
        clear_gpu_cache()
    
    if args.estimate:
        estimate_model_memory(args.estimate)
    
    if args.kill:
        kill_gpu_processes()
    
    print("\nğŸ’¡ è§£å†³æ˜¾å­˜ä¸è¶³çš„å»ºè®®:")
    print("1. æ¸…ç†GPUç¼“å­˜: python check_gpu_memory.py --clear")
    print("2. ç»ˆæ­¢å…¶ä»–GPUè¿›ç¨‹: sudo kill -9 <PID>")
    print("3. å‡å°æ‰¹å¤§å°: ä¿®æ”¹configä¸­çš„batch_size")
    print("4. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: å‡å°‘æ¿€æ´»å€¼æ˜¾å­˜å ç”¨")
    print("5. ä½¿ç”¨DeepSpeed ZeRO: åˆ†å¸ƒå¼ä¼˜åŒ–å™¨çŠ¶æ€")

if __name__ == "__main__":
    import argparse
    main() 