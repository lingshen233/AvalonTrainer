#!/usr/bin/env python3
"""
å•å¡A800-80GBå†…å­˜è¯Šæ–­å·¥å…·
åˆ†æ80GBå¤§æ˜¾å­˜ç¯å¢ƒä¸‹çš„æœ€ä¼˜é…ç½®
"""

import torch
import psutil
import subprocess
import json
from configs.config_presets import CONFIG_PRESETS

def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        
        gpus = []
        for line in lines:
            if line.strip():
                index, name, used, total = line.split(', ')
                gpus.append({
                    'index': int(index),
                    'name': name,
                    'used_mb': int(used),
                    'total_mb': int(total),
                    'free_mb': int(total) - int(used),
                    'usage_pct': (int(used) / int(total)) * 100
                })
        return gpus
    except Exception as e:
        print(f"âŒ æ— æ³•è·å–GPUä¿¡æ¯: {e}")
        return []

def estimate_model_memory_single_gpu(config, batch_size=4):
    """ä¼°ç®—å•GPUæ¨¡å‹å†…å­˜éœ€æ±‚"""
    # 7Bæ¨¡å‹å‚æ•°é‡
    param_count = 7e9
    
    # FP16æ¯ä¸ªå‚æ•°2å­—èŠ‚
    model_memory_mb = (param_count * 2) / (1024**2)
    
    # æ¢¯åº¦å†…å­˜ï¼ˆFP16ï¼‰
    gradient_memory_mb = model_memory_mb
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯ä»¥å¸è½½åˆ°CPUï¼‰
    optimizer_memory_mb = 0  # CPUå¸è½½
    
    # æ¿€æ´»å€¼å†…å­˜ï¼ˆå•GPUï¼Œå¯ä»¥ç”¨æ›´å¤§çš„batch sizeï¼‰
    seq_length = config.get('max_seq_length', 1024)
    d_model = 4096
    n_layers = 32
    
    # ä¼°ç®—æ¿€æ´»å€¼å†…å­˜ - A800å¯ä»¥æ‰¿å—æ›´å¤§çš„æ‰¹æ¬¡
    activation_memory_mb = (batch_size * seq_length * d_model * n_layers * 2) / (1024**2)
    
    return {
        'model_mb': model_memory_mb,
        'gradient_mb': gradient_memory_mb,
        'optimizer_mb': optimizer_memory_mb,
        'activation_mb': activation_memory_mb,
        'total_mb': model_memory_mb + gradient_memory_mb + activation_memory_mb
    }

def get_a800_recommendations():
    """è·å–A800ä¼˜åŒ–å»ºè®®"""
    return [
        "ğŸ¯ A800-80GBä¸“ç”¨ä¼˜åŒ–ç­–ç•¥:",
        "  1. å……åˆ†åˆ©ç”¨80GBå¤§æ˜¾å­˜ä¼˜åŠ¿",
        "  2. ä½¿ç”¨ZeRO-2è€ŒéZeRO-3ï¼ˆå‡å°‘é€šä¿¡å¼€é”€ï¼‰",
        "  3. ä»…ä¼˜åŒ–å™¨CPUå¸è½½ï¼Œå‚æ•°ä¿ç•™åœ¨GPU",
        "  4. å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„batch size (4-8)",
        "  5. å¯ç”¨å®Œæ•´çš„1024åºåˆ—é•¿åº¦",
        "",
        "ğŸ”§ æ¨èé…ç½®:",
        "  - micro_batch_per_gpu: 4-8",
        "  - gradient_accumulation_steps: 2-4",
        "  - ZeRO stage: 2",
        "  - æ¿€æ´»æ£€æŸ¥ç‚¹: è½»åº¦ä½¿ç”¨",
        "  - åºåˆ—é•¿åº¦: 1024",
        "",
        "âš¡ æ€§èƒ½ä¼˜åŒ–:",
        "  1. å…³é—­ä¸å¿…è¦çš„æ¿€æ´»æ£€æŸ¥ç‚¹",
        "  2. ä½¿ç”¨æ ‡å‡†FP16è®¾ç½®",
        "  3. å¢åŠ CPUçº¿ç¨‹æ•°åˆ©ç”¨å¤šæ ¸",
        "  4. å¯é€‰æ‹©æ›´æ¿€è¿›çš„å­¦ä¹ ç‡",
        "",
        "ğŸš€ å¦‚æœè¿˜æœ‰ä½™é‡:",
        "  - å°è¯•micro_batch=8æˆ–æ›´å¤§",
        "  - å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼ˆd_state, expandï¼‰",
        "  - ä½¿ç”¨æ›´é•¿çš„åºåˆ—é•¿åº¦ï¼ˆ2048ï¼‰"
    ]

def suggest_batch_size(gpu_memory_gb):
    """æ ¹æ®æ˜¾å­˜å¤§å°å»ºè®®batch size"""
    if gpu_memory_gb >= 70:  # A800/A100-80GB
        return {
            'conservative': 4,
            'recommended': 6,
            'aggressive': 8,
            'max_seq_length': 1024
        }
    elif gpu_memory_gb >= 40:  # A100-40GB
        return {
            'conservative': 2,
            'recommended': 4,
            'aggressive': 6,
            'max_seq_length': 1024
        }
    elif gpu_memory_gb >= 24:  # RTX 4090/3090
        return {
            'conservative': 1,
            'recommended': 2,
            'aggressive': 3,
            'max_seq_length': 512
        }
    else:
        return {
            'conservative': 1,
            'recommended': 1,
            'aggressive': 2,
            'max_seq_length': 256
        }

def main():
    print("ğŸ® A800-80GBå†…å­˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # 1. ç³»ç»Ÿä¿¡æ¯
    print("\nğŸ“± ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUå†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    
    # 2. GPUä¿¡æ¯
    print("\nğŸ® GPUä¿¡æ¯:")
    gpus = get_gpu_memory_info()
    
    if not gpus:
        print("  âŒ æ— æ³•æ£€æµ‹åˆ°GPU")
        return
    
    gpu = gpus[0]  # å•GPU
    gpu_memory_gb = gpu['total_mb'] / 1024
    
    print(f"  GPU: {gpu['name']}")
    print(f"  æ€»å†…å­˜: {gpu_memory_gb:.1f} GB")
    print(f"  å·²ä½¿ç”¨: {gpu['used_mb']/1024:.1f} GB ({gpu['usage_pct']:.1f}%)")
    print(f"  å¯ç”¨: {gpu['free_mb']/1024:.1f} GB")
    
    # 3. A800ç‰¹æ®Šæ£€æŸ¥
    is_a800 = "A800" in gpu['name'] or "A100" in gpu['name']
    if is_a800:
        print(f"  âœ… æ£€æµ‹åˆ°é«˜ç«¯GPUï¼Œé€‚åˆè®­ç»ƒå¤§æ¨¡å‹")
    else:
        print(f"  âš ï¸  å½“å‰GPUä¸æ˜¯A800/A100ï¼Œå»ºè®®è°ƒæ•´é…ç½®")
    
    # 4. Batch sizeå»ºè®®
    print(f"\nğŸ“Š é’ˆå¯¹{gpu_memory_gb:.0f}GBæ˜¾å­˜çš„Batch Sizeå»ºè®®:")
    batch_suggestions = suggest_batch_size(gpu_memory_gb)
    
    print(f"  ä¿å®ˆè®¾ç½®: micro_batch={batch_suggestions['conservative']}, seq_len={batch_suggestions['max_seq_length']}")
    print(f"  æ¨èè®¾ç½®: micro_batch={batch_suggestions['recommended']}, seq_len={batch_suggestions['max_seq_length']}")
    print(f"  æ¿€è¿›è®¾ç½®: micro_batch={batch_suggestions['aggressive']}, seq_len={batch_suggestions['max_seq_length']}")
    
    # 5. å†…å­˜ä¼°ç®—
    print(f"\nğŸ§  7B Mambaæ¨¡å‹å†…å­˜ä¼°ç®— (micro_batch={batch_suggestions['recommended']}):")
    config = CONFIG_PRESETS['7b_mamba']
    memory_est = estimate_model_memory_single_gpu(config, batch_suggestions['recommended'])
    
    print(f"  æ¨¡å‹å‚æ•°: {memory_est['model_mb']/1024:.1f} GB")
    print(f"  æ¢¯åº¦: {memory_est['gradient_mb']/1024:.1f} GB")
    print(f"  ä¼˜åŒ–å™¨çŠ¶æ€: {memory_est['optimizer_mb']/1024:.1f} GB (CPUå¸è½½)")
    print(f"  æ¿€æ´»å€¼: {memory_est['activation_mb']/1024:.1f} GB")
    print(f"  æ€»GPUéœ€æ±‚: {memory_est['total_mb']/1024:.1f} GB")
    
    # 6. å¯è¡Œæ€§åˆ†æ
    print(f"\nğŸ” å¯è¡Œæ€§åˆ†æ:")
    required_memory = memory_est['total_mb'] / 1024
    available_memory = gpu['free_mb'] / 1024
    
    if required_memory < available_memory * 0.8:  # ç•™20%ä½™é‡
        print(f"  âœ… å†…å­˜å……è¶³! éœ€è¦{required_memory:.1f}GB, å¯ç”¨{available_memory:.1f}GB")
        print(f"  ğŸ’¡ å¯ä»¥å°è¯•æ›´å¤§çš„batch sizeæˆ–åºåˆ—é•¿åº¦")
    elif required_memory < available_memory:
        print(f"  âš ï¸  å†…å­˜ç´§å¼ ä½†å¯è¡Œã€‚éœ€è¦{required_memory:.1f}GB, å¯ç”¨{available_memory:.1f}GB")
        print(f"  ğŸ’¡ å»ºè®®ä½¿ç”¨ä¿å®ˆé…ç½®")
    else:
        print(f"  âŒ å†…å­˜ä¸è¶³! éœ€è¦{required_memory:.1f}GB, ä½†åªæœ‰{available_memory:.1f}GB")
        print(f"  ğŸ’¡ éœ€è¦é™ä½batch sizeæˆ–ä½¿ç”¨ZeRO-3")
    
    # 7. ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ’¡ A800-80GBä¼˜åŒ–å»ºè®®:")
    for recommendation in get_a800_recommendations():
        print(recommendation)
    
    # 8. å¯åŠ¨å‘½ä»¤
    print(f"\nğŸš€ æ¨èå¯åŠ¨å‘½ä»¤:")
    print("./launch_single_a800_80g.sh")
    print("")
    print("æˆ–æ‰‹åŠ¨:")
    print("python train_deepspeed.py --preset 7b_mamba --deepspeed_config deepspeed_single_a800_80g.json --max_seq_length 1024")

if __name__ == "__main__":
    main() 