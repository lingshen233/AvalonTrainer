#!/usr/bin/env python3
"""
è®­ç»ƒåŽå¿«é€Ÿæµ‹è¯•è„šæœ¬
ä¸“é—¨ç”¨äºŽè®­ç»ƒå®ŒæˆåŽå¿«é€ŸéªŒè¯æ¨¡åž‹æ€§èƒ½
"""

import os
import sys
import argparse
import torch
import time
from pathlib import Path

def quick_model_test(checkpoint_path, save_results=True):
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒçš„æ¨¡åž‹"""
    print(f"ðŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡åž‹: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from models import create_model
        from configs.base import ModelConfig
        from transformers import AutoTokenizer
        import json
        
        # åŠ è½½æ¨¡åž‹
        print("ðŸ“¥ åŠ è½½æ¨¡åž‹...")
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model_config = ModelConfig(**checkpoint['config'])
        
        model = create_model(model_config.model_type, model_config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # èŽ·å–è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡åž‹ç±»åž‹: {model_config.model_type}")
        print(f"   å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"   è®¾å¤‡: {device}")
        
        # å¿«é€Ÿæ–‡æœ¬ç”Ÿæˆæµ‹è¯•
        print("\nðŸŽ¯ å¿«é€Ÿç”Ÿæˆæµ‹è¯•...")
        test_prompts = [
            "äººå·¥æ™ºèƒ½",
            "The future of",
            "ç§‘æŠ€å‘å±•",
            "Machine learning"
        ]
        
        results = {
            'model_info': {
                'type': model_config.model_type,
                'params': total_params,
                'checkpoint': checkpoint_path
            },
            'generation_tests': []
        }
        
        for prompt in test_prompts:
            try:
                print(f"  æµ‹è¯•æç¤º: '{prompt}'")
                
                inputs = tokenizer(prompt, return_tensors='pt').to(device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs['input_ids'],
                        max_length=inputs['input_ids'].size(1) + 30,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                generated_text = generated_text[len(prompt):].strip()
                
                print(f"  ç”Ÿæˆç»“æžœ: {generated_text[:50]}...")
                
                results['generation_tests'].append({
                    'prompt': prompt,
                    'generated': generated_text
                })
                
            except Exception as e:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
                results['generation_tests'].append({
                    'prompt': prompt,
                    'error': str(e)
                })
        
        # ä¿å­˜ç»“æžœ
        if save_results:
            results_dir = Path('test_results')
            results_dir.mkdir(exist_ok=True)
            
            results_file = results_dir / f'quick_test_{int(time.time())}.json'
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"\nðŸ“ æµ‹è¯•ç»“æžœå·²ä¿å­˜: {results_file}")
        
        print("\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
        print("ðŸ’¡ å¦‚éœ€å®Œæ•´åŸºå‡†æµ‹è¯•ï¼Œè¯·è¿è¡Œ: python test_benchmark.py")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒåŽå¿«é€Ÿæµ‹è¯•")
    parser.add_argument("--checkpoint", type=str, default="checkpoints/final_model.pt", 
                       help="æ¨¡åž‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--no-save", action="store_true", help="ä¸ä¿å­˜æµ‹è¯•ç»“æžœ")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é»˜è®¤è·¯å¾„
    checkpoints_to_test = []
    
    if args.checkpoint != "checkpoints/final_model.pt":
        # ç”¨æˆ·æŒ‡å®šäº†ç‰¹å®šè·¯å¾„
        checkpoints_to_test.append(args.checkpoint)
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æ£€æŸ¥ç‚¹
        checkpoint_dir = Path("checkpoints")
        if checkpoint_dir.exists():
            # ä¼˜å…ˆçº§ï¼šfinal_model.pt > best_model.pt > æœ€æ–°çš„checkpoint
            if (checkpoint_dir / "final_model.pt").exists():
                checkpoints_to_test.append("checkpoints/final_model.pt")
            elif (checkpoint_dir / "best_model.pt").exists():
                checkpoints_to_test.append("checkpoints/best_model.pt")
            else:
                # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
                checkpoint_files = list(checkpoint_dir.glob("checkpoint_step_*.pt"))
                if checkpoint_files:
                    latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
                    checkpoints_to_test.append(str(latest_checkpoint))
    
    if not checkpoints_to_test:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡åž‹æ£€æŸ¥ç‚¹æ–‡ä»¶")
        print("è¯·ç¡®ä¿è®­ç»ƒå·²å®Œæˆå¹¶ç”Ÿæˆäº†æ¨¡åž‹æ–‡ä»¶")
        return 1
    
    success_count = 0
    for checkpoint in checkpoints_to_test:
        print(f"\n{'='*60}")
        if quick_model_test(checkpoint, not args.no_save):
            success_count += 1
    
    print(f"\nðŸŽ¯ æµ‹è¯•å®Œæˆ: {success_count}/{len(checkpoints_to_test)} æˆåŠŸ")
    return 0 if success_count > 0 else 1

if __name__ == "__main__":
    exit(main()) 