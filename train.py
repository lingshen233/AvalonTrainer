#!/usr/bin/env python3
"""
DeepSpeed ZeROä¼˜åŒ–è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆ
è§£å†³æ‰¹æ¬¡å¤§å°é…ç½®é—®é¢˜
"""

import os
import json
import yaml
import torch
import torch.nn as nn
import argparse
import shutil

# DeepSpeedå¯¼å…¥
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("âŒ DeepSpeedæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒ")

def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()

def load_config(config_path: str):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def create_deepspeed_config(model_config, training_config, num_gpus):
    """åˆ›å»ºDeepSpeedé…ç½®"""
    # è®¡ç®—æ­£ç¡®çš„æ‰¹æ¬¡å¤§å°å…³ç³»ï¼š
    # train_batch_size = micro_batch_per_gpu * gradient_accumulation_steps * world_size
    micro_batch_per_gpu = training_config.train_batch_size
    gradient_accumulation_steps = training_config.gradient_accumulation_steps
    train_batch_size = micro_batch_per_gpu * gradient_accumulation_steps * num_gpus
    
    ds_config = {
        "train_batch_size": train_batch_size,
        "train_micro_batch_size_per_gpu": micro_batch_per_gpu,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        
        # å¯ç”¨ZeRO-2ä¼˜åŒ–
        "zero_optimization": {
            "stage": 2,  # ZeRO-2: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦
            "contiguous_gradients": True,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 5e8,
            "allgather_bucket_size": 5e8,
            "offload_optimizer": {
                "device": "none"  # ä¸ä½¿ç”¨CPUå¸è½½ï¼ŒMambaæ¨¡å‹ä¸é€‚åˆ
            }
        },
        
        # æ··åˆç²¾åº¦
        "fp16": {
            "enabled": training_config.fp16,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        
        # ä¼˜åŒ–å™¨é…ç½®
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": training_config.learning_rate,
                "betas": [0.9, 0.95],
                "eps": 1e-8,
                "weight_decay": training_config.weight_decay
            }
        },
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": training_config.learning_rate,
                "warmup_num_steps": training_config.warmup_steps
            }
        },
        
        # æ¢¯åº¦è£å‰ª
        "gradient_clipping": training_config.max_grad_norm,
        
        # æ£€æŸ¥ç‚¹é…ç½®
        "steps_per_print": training_config.logging_steps,
        "wall_clock_breakdown": False,
        
        # å†…å­˜ä¼˜åŒ–
        "activation_checkpointing": {
            "partition_activations": True,
            "cpu_checkpointing": False,
            "contiguous_memory_optimization": True,
            "number_checkpoints": 4,
            "synchronize_checkpoint_boundary": True
        }
    }
    
    return ds_config

def use_prebuilt_config(num_gpus):
    """ä½¿ç”¨é¢„å…ˆæ„å»ºçš„æ­£ç¡®é…ç½®"""
    config_file = f"deepspeed_{num_gpus}gpu.json"
    
    if os.path.exists(config_file):
        print(f"âœ… ä½¿ç”¨é¢„æ„å»ºé…ç½®: {config_file}")
        
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        # éªŒè¯é…ç½®æ­£ç¡®æ€§
        train_batch = config.get('train_batch_size', 0)
        micro_batch = config.get('train_micro_batch_size_per_gpu', 0)
        grad_acc = config.get('gradient_accumulation_steps', 0)
        expected = micro_batch * grad_acc * num_gpus
        
        if train_batch == expected:
            print(f"âœ… é…ç½®éªŒè¯é€šè¿‡: {train_batch} = {micro_batch} Ã— {grad_acc} Ã— {num_gpus}")
            return config
        else:
            print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {train_batch} != {expected}")
            return None
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°é¢„æ„å»ºé…ç½®: {config_file}")
        return None

def create_configs_from_yaml(yaml_config):
    """ä»YAMLé…ç½®åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®"""
    from configs.base import ModelConfig, TrainingConfig
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_params = yaml_config.get('model', {})
    model_config = ModelConfig(
        model_type=yaml_config.get('model_type', 'mamba'),
        vocab_size=model_params.get('vocab_size', 50257),
        max_seq_length=model_params.get('max_seq_length', 4096),
        d_model=model_params.get('d_model', 4864),
        n_layers=model_params.get('n_layers', 45),
        n_heads=model_params.get('n_heads', 32),
        d_ff=model_params.get('d_ff', 16384),
        dropout=model_params.get('dropout', 0.1),
        # Mambaç‰¹æœ‰å‚æ•°
        d_state=model_params.get('d_state', 16),
        d_conv=model_params.get('d_conv', 4),
        expand=model_params.get('expand', 2)
    )
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    training_params = yaml_config.get('training', {})
    training_config = TrainingConfig(
        dataset_name=training_params.get('dataset', 'auto'),
        train_batch_size=training_params.get('batch_size', 1),
        eval_batch_size=training_params.get('eval_batch_size', 1),
        gradient_accumulation_steps=training_params.get('gradient_accumulation_steps', 8),
        max_length=training_params.get('max_length', 4096),
        learning_rate=training_params.get('learning_rate', 1e-4),
        weight_decay=training_params.get('weight_decay', 0.01),
        max_grad_norm=training_params.get('max_grad_norm', 1.0),
        max_steps=training_params.get('max_steps', 200000),
        warmup_steps=training_params.get('warmup_steps', 10000),
        eval_steps=training_params.get('eval_steps', 5000),
        save_steps=training_params.get('save_steps', 10000),
        logging_steps=training_params.get('logging_steps', 100),
        fp16=training_params.get('fp16', True),
        output_dir=training_params.get('output_dir', './outputs'),
        checkpoint_dir=training_params.get('checkpoint_dir', './checkpoints'),
        use_wandb=training_params.get('use_wandb', False),
        wandb_project=training_params.get('wandb_project', 'rag-transformer'),
        wandb_run_name=training_params.get('wandb_run_name', 'deepspeed_7b'),
        distributed=True,  # DeepSpeedéœ€è¦åˆ†å¸ƒå¼
        world_size=yaml_config.get('num_gpus', 4)
    )
    
    return model_config, training_config

class DeepSpeedTrainer:
    """DeepSpeedä¼˜åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, model_config, training_config, ds_config):
        self.model_config = model_config
        self.training_config = training_config
        self.ds_config = ds_config
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(training_config.output_dir, exist_ok=True)
        os.makedirs(training_config.checkpoint_dir, exist_ok=True)
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        if not torch.distributed.is_initialized():
            deepspeed.init_distributed()
        
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        print(f"ğŸ’¾ DeepSpeedè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   Local Rank: {self.local_rank}")
        print(f"   World Size: {self.world_size}")
        print(f"   è¾“å‡ºç›®å½•: {os.path.abspath(training_config.output_dir)}")
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        from models import create_model
        
        print("ğŸ“¦ åˆ›å»ºæ¨¡å‹...")
        model = create_model(self.model_config.model_type, self.model_config)
        
        # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°é‡è®¡ç®—å‡½æ•°
        from configs.model_presets import calculate_model_parameters
        estimated_params = calculate_model_parameters(self.model_config)
        
        # å®é™…å‚æ•°é‡ï¼ˆç”¨äºéªŒè¯ï¼‰
        actual_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        if self.local_rank == 0:
            print(f"æ¨¡å‹å‚æ•°: {actual_params:,} ({actual_params/1e9:.2f}B)")
            print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e9:.2f}B)")
        
        return model
    
    def create_dataloader(self):
        """åˆ›å»ºç®€åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        # è¿™é‡Œä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º
        class DummyDataset:
            def __init__(self, vocab_size, max_length, num_samples=10000):
                self.vocab_size = vocab_size
                self.max_length = max_length
                self.num_samples = num_samples
            
            def __len__(self):
                return self.num_samples
            
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºåºåˆ—
                input_ids = torch.randint(0, self.vocab_size, (self.max_length,))
                return {'input_ids': input_ids, 'labels': input_ids.clone()}
        
        dataset = DummyDataset(
            self.model_config.vocab_size,
            self.model_config.max_seq_length
        )
        
        # ä½¿ç”¨DeepSpeedçš„æ•°æ®é‡‡æ ·å™¨
        sampler = torch.utils.data.DistributedSampler(
            dataset,
            num_replicas=self.world_size,
            rank=self.local_rank,
            shuffle=True
        )
        
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.training_config.train_batch_size,
            sampler=sampler,
            num_workers=2,
            pin_memory=True
        )
        
        return dataloader
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = self.create_dataloader()
        
        # åˆå§‹åŒ–DeepSpeedå¼•æ“
        if self.local_rank == 0:
            print("ğŸš€ åˆå§‹åŒ–DeepSpeedå¼•æ“...")
        
        model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
            model=model,
            config=self.ds_config
        )
        
        # å¼€å§‹è®­ç»ƒ
        if self.local_rank == 0:
            print("ğŸš€ å¼€å§‹DeepSpeedè®­ç»ƒ...")
        
        model_engine.train()
        global_step = 0
        
        try:
            for epoch in range(1, 6):  # é™åˆ¶epochæ•°ç”¨äºæ¼”ç¤º
                dataloader.sampler.set_epoch(epoch)
                
                for step, batch in enumerate(dataloader):
                    if global_step >= self.training_config.max_steps:
                        break
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
                    input_ids = batch['input_ids'].to(model_engine.device)
                    labels = batch['labels'].to(model_engine.device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = model_engine(input_ids)
                    
                    # è®¡ç®—æŸå¤±
                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs
                    
                    # è¯­è¨€å»ºæ¨¡æŸå¤±
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    loss_fct = nn.CrossEntropyLoss()
                    loss = loss_fct(
                        shift_logits.view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )
                    
                    # åå‘ä¼ æ’­
                    model_engine.backward(loss)
                    model_engine.step()
                    
                    global_step += 1
                    
                    # è®°å½•æ—¥å¿—
                    if global_step % self.training_config.logging_steps == 0 and self.local_rank == 0:
                        current_lr = lr_scheduler.get_last_lr()[0] if lr_scheduler else self.training_config.learning_rate
                        
                        # GPUæ˜¾å­˜ä½¿ç”¨
                        allocated = torch.cuda.memory_allocated(self.local_rank) / 1e9
                        reserved = torch.cuda.memory_reserved(self.local_rank) / 1e9
                        
                        print(f"æ­¥éª¤ {global_step}: æŸå¤± {loss.item():.4f}, "
                              f"å­¦ä¹ ç‡ {current_lr:.2e}, "
                              f"æ˜¾å­˜ {allocated:.1f}/{reserved:.1f}GB")
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if global_step % self.training_config.save_steps == 0:
                        if self.local_rank == 0:
                            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹ (æ­¥éª¤ {global_step})...")
                        
                        checkpoint_dir = os.path.join(
                            self.training_config.checkpoint_dir,
                            f"step_{global_step}"
                        )
                        model_engine.save_checkpoint(checkpoint_dir)
                    
                    # æ¸…ç†æ˜¾å­˜
                    if global_step % 10 == 0:
                        torch.cuda.empty_cache()
                
                if global_step >= self.training_config.max_steps:
                    break
        
        except Exception as e:
            if self.local_rank == 0:
                print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        if self.local_rank == 0:
            print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹...")
        
        final_checkpoint_dir = os.path.join(
            self.training_config.checkpoint_dir,
            "final"
        )
        model_engine.save_checkpoint(final_checkpoint_dir)
        
        if self.local_rank == 0:
            print("âœ… DeepSpeedè®­ç»ƒå®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description="DeepSpeed ZeROä¼˜åŒ–è®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆ")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--config", type=str, default="config_7b_mamba.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--preset", type=str, help="ä½¿ç”¨é¢„è®¾é…ç½®")
    parser.add_argument("--num_gpus", type=int, default=4, help="GPUæ•°é‡")
    
    # DeepSpeedå‚æ•°
    parser.add_argument("--deepspeed_config", type=str, help="DeepSpeedé…ç½®æ–‡ä»¶")
    parser.add_argument("--local_rank", type=int, default=-1, help="æœ¬åœ°rank (DeepSpeedè‡ªåŠ¨è®¾ç½®)")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®")
    parser.add_argument("--check_memory", action="store_true", help="æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨")
    parser.add_argument("--fix_config", action="store_true", help="è‡ªåŠ¨ä¿®å¤é…ç½®é—®é¢˜")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥DeepSpeed
    if not DEEPSPEED_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£…DeepSpeed:")
        print("pip install deepspeed")
        return
    
    # æ˜¾å­˜æ£€æŸ¥
    if args.check_memory:
        print("ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total = props.total_memory / 1e9
                print(f"GPU {i}: {props.name} - {total:.1f}GB")
        return
    
    # è‡ªåŠ¨ä¿®å¤é…ç½®
    if args.fix_config:
        print("ğŸ”§ è¿è¡Œé…ç½®ä¿®å¤...")
        os.system(f"python fix_deepspeed_batch_size.py --num_gpus {args.num_gpus}")
    
    # ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„deepspeed_config
    ds_config = None
    
    if args.deepspeed_config and os.path.exists(args.deepspeed_config):
        print(f"âœ… ä½¿ç”¨æŒ‡å®šé…ç½®: {args.deepspeed_config}")
        with open(args.deepspeed_config, 'r') as f:
            ds_config = json.load(f)
        
        # éªŒè¯é…ç½®æ­£ç¡®æ€§
        train_batch = ds_config.get('train_batch_size', 0)
        micro_batch = ds_config.get('train_micro_batch_size_per_gpu', 0)
        grad_acc = ds_config.get('gradient_accumulation_steps', 0)
        expected = micro_batch * grad_acc * args.num_gpus
        
        if train_batch == expected:
            print(f"âœ… é…ç½®éªŒè¯é€šè¿‡: {train_batch} = {micro_batch} Ã— {grad_acc} Ã— {args.num_gpus}")
        else:
            print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {train_batch} != {expected}")
            print(f"ğŸ”§ å°†è‡ªåŠ¨ä¿®å¤æ‰¹æ¬¡å¤§å°ä¸ºå•GPUé…ç½®...")
            # ä¸ºå•GPUä¿®å¤é…ç½®
            ds_config['train_batch_size'] = micro_batch * grad_acc * 1
            print(f"âœ… ä¿®å¤å®Œæˆ: {ds_config['train_batch_size']} = {micro_batch} Ã— {grad_acc} Ã— 1")
    else:
        # å°è¯•ä½¿ç”¨é¢„æ„å»ºé…ç½®
        ds_config = use_prebuilt_config(args.num_gpus)
    
    if ds_config is None:
        print("ğŸ”„ fallbackåˆ°åŠ¨æ€ç”Ÿæˆé…ç½®...")
        
        # å¤„ç†é¢„è®¾é…ç½®
        if args.preset:
            from configs.model_presets import get_model_preset, get_training_config_for_model_size
            
            preset_config = get_model_preset(args.preset)
            model_config = preset_config['model']
            training_config = get_training_config_for_model_size(args.preset, args.num_gpus)
            
            print(f"âœ… ä½¿ç”¨é¢„è®¾: {preset_config['description']}")
            print(f"   å‚æ•°é‡: {preset_config['params']}")
            
            # åˆ›å»ºè™šæ‹Ÿyamlé…ç½®
            yaml_config = {
                'model_type': model_config.model_type,
                'num_gpus': args.num_gpus,
                'model': {
                    'vocab_size': model_config.vocab_size,
                    'max_seq_length': model_config.max_seq_length,
                    'd_model': model_config.d_model,
                    'n_layers': model_config.n_layers,
                    'd_state': getattr(model_config, 'd_state', 16),
                    'd_conv': getattr(model_config, 'd_conv', 4),
                    'expand': getattr(model_config, 'expand', 2),
                    'dropout': model_config.dropout
                },
                'training': {
                    'batch_size': training_config.train_batch_size,
                    'gradient_accumulation_steps': training_config.gradient_accumulation_steps,
                    'max_length': training_config.max_length,
                    'learning_rate': training_config.learning_rate,
                    'max_steps': training_config.max_steps,
                    'warmup_steps': training_config.warmup_steps,
                    'save_steps': training_config.save_steps,
                    'logging_steps': training_config.logging_steps,
                    'fp16': training_config.fp16,
                    'output_dir': training_config.output_dir,
                    'checkpoint_dir': training_config.checkpoint_dir
                }
            }
        else:
            # åŠ è½½é…ç½®æ–‡ä»¶
            if not os.path.exists(args.config):
                print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
                return
            
            yaml_config = load_config(args.config)
            yaml_config['num_gpus'] = args.num_gpus
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        model_config, training_config = create_configs_from_yaml(yaml_config)
        
        # åˆ›å»ºDeepSpeedé…ç½®
        ds_config = create_deepspeed_config(model_config, training_config, args.num_gpus)
    else:
        # ä½¿ç”¨é¢„æ„å»ºé…ç½®ï¼Œéœ€è¦åˆ›å»ºå¯¹åº”çš„model_config
        if args.preset:
            from configs.model_presets import get_model_preset, get_training_config_for_model_size
            preset_config = get_model_preset(args.preset)
            model_config = preset_config['model']
            training_config = get_training_config_for_model_size(args.preset, args.num_gpus)
        else:
            yaml_config = load_config(args.config)
            yaml_config['num_gpus'] = args.num_gpus
            model_config, training_config = create_configs_from_yaml(yaml_config)
    
    # ä¿å­˜DeepSpeedé…ç½®æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯ç”¨æˆ·æŒ‡å®šçš„ï¼‰
    if not args.deepspeed_config:
        ds_config_path = "deepspeed_config.json"
        with open(ds_config_path, 'w') as f:
            json.dump(ds_config, f, indent=2)
    else:
        ds_config_path = args.deepspeed_config
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    from configs.model_presets import calculate_model_parameters
    total_params = calculate_model_parameters(model_config)
    
    # è®¡ç®—æ‰¹æ¬¡å¤§å°è¯¦æƒ…
    micro_batch = ds_config['train_micro_batch_size_per_gpu']
    grad_acc = ds_config['gradient_accumulation_steps']
    world_size = args.num_gpus
    train_batch = ds_config['train_batch_size']
    
    print(f"\nğŸ“Š DeepSpeedè®­ç»ƒé…ç½®:")
    print(f"æ¨¡å‹ç±»å‹: {model_config.model_type}")
    print(f"ä¼°ç®—å‚æ•°: {total_params/1e9:.2f}B")
    print(f"GPUæ•°é‡: {args.num_gpus}")
    print(f"æ‰¹å¤§å°/GPU: {micro_batch}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {grad_acc}")
    print(f"æœ‰æ•ˆæ‰¹å¤§å°: {train_batch}")
    print(f"ZeROé˜¶æ®µ: {ds_config['zero_optimization']['stage']}")
    print(f"DeepSpeedé…ç½®: {ds_config_path}")
    
    # å¼ºåˆ¶éªŒè¯æ‰¹æ¬¡å¤§å°è®¡ç®—
    expected = micro_batch * grad_acc * world_size
    if train_batch != expected:
        print(f"\nâŒ è‡´å‘½é”™è¯¯: æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…")
        print(f"   train_batch_size: {train_batch}")
        print(f"   expected: {micro_batch} Ã— {grad_acc} Ã— {world_size} = {expected}")
        print(f"ğŸ”§ è¯·è¿è¡Œ: python fix_deepspeed_batch_size.py --num_gpus {args.num_gpus}")
        return
    else:
        print(f"\nâœ… æ‰¹æ¬¡å¤§å°éªŒè¯é€šè¿‡: {train_batch} = {micro_batch} Ã— {grad_acc} Ã— {world_size}")
    
    if args.dry_run:
        print("\nâœ… é…ç½®éªŒè¯å®Œæˆï¼ˆdry_runæ¨¡å¼ï¼‰")
        return
    
    # å¯åŠ¨è®­ç»ƒ
    try:
        trainer = DeepSpeedTrainer(model_config, training_config, ds_config)
        trainer.train()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # è‡ªåŠ¨è¿è¡Œä¿®å¤å»ºè®®
        print(f"\nğŸ”§ å»ºè®®è¿è¡Œä¿®å¤å‘½ä»¤:")
        print(f"python fix_deepspeed_batch_size.py --num_gpus {args.num_gpus}")

if __name__ == "__main__":
    main() 