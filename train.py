#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¤šGPUè®­ç»ƒè„šæœ¬
æ”¯æŒYAMLé…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°
"""

import os
import sys
import argparse
import yaml
import torch
import torch.multiprocessing as mp
from torch.distributed import init_process_group, destroy_process_group
from torch.nn.parallel import DistributedDataParallel as DDP
import time
import subprocess
import platform

from configs.presets import get_config, calculate_model_size, estimate_memory_usage, list_available_configs
from configs.base import ModelConfig, TrainingConfig
from models import create_model
from trainers.base import BaseTrainer
from data.processor import DataProcessor

def load_config(config_path: str):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config

def is_docker_container():
    """æ£€æµ‹æ˜¯å¦åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ"""
    try:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨Dockerç‰¹å¾æ–‡ä»¶
        return (os.path.exists('/.dockerenv') or 
                os.path.exists('/proc/1/cgroup') and 'docker' in open('/proc/1/cgroup').read())
    except:
        return False

def is_root_user():
    """æ£€æµ‹æ˜¯å¦ä¸ºrootç”¨æˆ·"""
    return os.geteuid() == 0

def auto_shutdown(delay_seconds: int = 60):
    """è‡ªåŠ¨å…³æœºåŠŸèƒ½"""
    print(f"\nğŸ”„ è®­ç»ƒå®Œæˆï¼å°†åœ¨ {delay_seconds} ç§’åè‡ªåŠ¨å…³æœº...")
    print("æŒ‰ Ctrl+C å–æ¶ˆè‡ªåŠ¨å…³æœº")
    
    try:
        for i in range(delay_seconds, 0, -1):
            print(f"\râ° å€’è®¡æ—¶: {i} ç§’", end="", flush=True)
            time.sleep(1)
        
        print(f"\nğŸ’¤ æ­£åœ¨å…³æœº...")
        
        # æ£€æµ‹ç¯å¢ƒå¹¶é€‰æ‹©åˆé€‚çš„å…³æœºå‘½ä»¤
        system = platform.system().lower()
        in_docker = is_docker_container()
        is_root = is_root_user()
        
        if in_docker:
            print("ğŸ³ æ£€æµ‹åˆ°Dockerå®¹å™¨ç¯å¢ƒ")
            # åœ¨Dockerå®¹å™¨ä¸­ï¼Œé€šå¸¸åªèƒ½åœæ­¢å®¹å™¨ï¼Œä¸èƒ½å…³æœº
            print("ğŸ’¡ å®¹å™¨ç¯å¢ƒæ— æ³•ç›´æ¥å…³æœºï¼Œå»ºè®®æ‰‹åŠ¨åœæ­¢å®¹å™¨")
            print("   å¯ä»¥ä½¿ç”¨: docker stop <container_id>")
            return
        
        if system == "windows":
            subprocess.run(["shutdown", "/s", "/t", "0"])
        elif system in ["linux", "darwin"]:  # Linuxæˆ–macOS
            if is_root:
                # rootç”¨æˆ·ç›´æ¥ä½¿ç”¨shutdown
                subprocess.run(["shutdown", "-h", "now"])
            else:
                # érootç”¨æˆ·ä½¿ç”¨sudo
                subprocess.run(["sudo", "shutdown", "-h", "now"])
        else:
            print("âŒ ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿï¼Œæ— æ³•è‡ªåŠ¨å…³æœº")
            
    except KeyboardInterrupt:
        print(f"\nâŒ è‡ªåŠ¨å…³æœºå·²å–æ¶ˆ")
    except FileNotFoundError as e:
        print(f"\nâŒ å…³æœºå‘½ä»¤æœªæ‰¾åˆ°: {e}")
        print("ğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        if is_docker_container():
            print("   - Dockerå®¹å™¨ç¯å¢ƒè¯·æ‰‹åŠ¨åœæ­¢å®¹å™¨")
        else:
            print("   - ç¡®ä¿ç³»ç»Ÿæ”¯æŒshutdownå‘½ä»¤")
            print("   - æ£€æŸ¥ç”¨æˆ·æƒé™è®¾ç½®")
    except Exception as e:
        print(f"\nâŒ è‡ªåŠ¨å…³æœºå¤±è´¥: {e}")

def create_configs_from_yaml(yaml_config):
    """ä»YAMLé…ç½®åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®"""
    
    # å¤„ç†è‡ªåŠ¨æ‰¹å¤§å°
    batch_size = yaml_config['training']['batch_size']
    if batch_size is None:
        # æ ¹æ®GPUæ•°é‡å’Œæ¨¡å‹ç±»å‹è‡ªåŠ¨è®¾ç½®
        if yaml_config['model_type'] == 'mamba':
            batch_size = 6 if yaml_config['num_gpus'] == 1 else 8
        else:
            batch_size = 4 if yaml_config['num_gpus'] == 1 else 6
    
    # æ¨¡å‹é…ç½®
    model_config = ModelConfig(
        model_type=yaml_config['model_type'],
        vocab_size=yaml_config['model']['vocab_size'],
        max_seq_length=yaml_config['model']['max_seq_length'],
        d_model=yaml_config['model']['d_model'],
        n_layers=yaml_config['model']['n_layers'],
        dropout=yaml_config['model']['dropout'],
        n_heads=yaml_config['model']['n_heads'],
        d_ff=yaml_config['model']['d_ff'],
        d_state=yaml_config['model']['d_state'],
        d_conv=yaml_config['model']['d_conv'],
        expand=yaml_config['model']['expand']
    )
    
    # è®­ç»ƒé…ç½®
    training_config = TrainingConfig(
        dataset_name=yaml_config['training']['dataset'],
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        gradient_accumulation_steps=yaml_config['training']['gradient_accumulation_steps'],
        max_length=yaml_config['training']['max_length'],
        learning_rate=yaml_config['training']['learning_rate'],
        weight_decay=yaml_config['training']['weight_decay'],
        max_grad_norm=yaml_config['training']['max_grad_norm'],
        max_steps=yaml_config['training']['max_steps'],
        warmup_steps=yaml_config['training']['warmup_steps'],
        eval_steps=yaml_config['training']['eval_steps'],
        save_steps=yaml_config['training']['save_steps'],
        logging_steps=yaml_config['training']['logging_steps'],
        fp16=yaml_config['training']['fp16'],
        output_dir=yaml_config['training']['output_dir'],
        checkpoint_dir=yaml_config['training']['checkpoint_dir'],
        use_wandb=yaml_config['training']['use_wandb'],
        wandb_project=yaml_config['training']['wandb_project'],
        wandb_run_name=yaml_config['training']['run_name'],
        distributed=(yaml_config['num_gpus'] > 1),
        world_size=yaml_config['num_gpus']
    )
    
    return model_config, training_config

def setup_ddp(rank: int, world_size: int):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    init_process_group(backend="nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    destroy_process_group()

def train_worker(rank: int, world_size: int, model_config: ModelConfig, training_config: TrainingConfig):
    """å¤šGPUè®­ç»ƒçš„å·¥ä½œè¿›ç¨‹"""
    
    # è®¾ç½®åˆ†å¸ƒå¼
    if world_size > 1:
        setup_ddp(rank, world_size)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(model_config.model_type, model_config)
    model = model.to(device)
    
    # åŒ…è£…ä¸ºDDP
    if world_size > 1:
        model = DDP(model, device_ids=[rank])
    
    # åˆ›å»ºç®€åŒ–çš„è®­ç»ƒå™¨
    class SimpleTrainer:
        def __init__(self, model, config, device, rank):
            self.model = model
            self.config = config
            self.device = device
            self.rank = rank
            self.data_processor = DataProcessor(config)
        
        def train(self):
            print(f"GPU {self.rank}: å¼€å§‹è®­ç»ƒ...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„è®­ç»ƒå¾ªç¯
            # ç›®å‰åªæ˜¯æ¼”ç¤ºæ¡†æ¶
            
            if self.rank == 0:
                total_params = sum(p.numel() for p in self.model.parameters())
                print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
                
                # åˆ›å»ºè¾“å‡ºç›®å½•
                os.makedirs(self.config.output_dir, exist_ok=True)
                os.makedirs(self.config.checkpoint_dir, exist_ok=True)
                
                # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰
                final_model_path = os.path.join(self.config.checkpoint_dir, "final_model.pt")
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'config': model_config.__dict__,
                    'total_params': total_params
                }, final_model_path)
                
                print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {os.path.abspath(final_model_path)}")
    
    trainer = SimpleTrainer(model, training_config, device, rank)
    trainer.train()
    
    if world_size > 1:
        cleanup_ddp()

def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–çš„å¤šGPUè®­ç»ƒè„šæœ¬")
    parser.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_type", type=str, choices=["transformer", "mamba"], help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--num_gpus", type=int, help="GPUæ•°é‡")
    parser.add_argument("--list_models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®")
    parser.add_argument("--no_shutdown", action="store_true", help="ç¦ç”¨è‡ªåŠ¨å…³æœº")
    
    args = parser.parse_args()
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    if args.list_models:
        print("\nğŸ¤– å¯ç”¨æ¨¡å‹:")
        for model_type, desc in list_available_configs().items():
            print(f"  {model_type}: {desc}")
        return
    
    # åŠ è½½é…ç½®
    if os.path.exists(args.config):
        yaml_config = load_config(args.config)
        print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    else:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.model_type:
        yaml_config['model_type'] = args.model_type
    if args.num_gpus:
        yaml_config['num_gpus'] = args.num_gpus
    if args.no_shutdown:
        yaml_config['system']['auto_shutdown'] = False
    
    # åˆ›å»ºé…ç½®
    model_config, training_config = create_configs_from_yaml(yaml_config)
    
    # è®¡ç®—èµ„æºéœ€æ±‚
    total_params = calculate_model_size(model_config)
    memory_info = estimate_memory_usage(model_config, training_config)
    
    # æ‰“å°ç¯å¢ƒä¿¡æ¯
    print(f"\nğŸŒ è¿è¡Œç¯å¢ƒ:")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()}")
    print(f"Dockerå®¹å™¨: {'æ˜¯' if is_docker_container() else 'å¦'}")
    print(f"Rootç”¨æˆ·: {'æ˜¯' if is_root_user() else 'å¦'}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"æ¨¡å‹ç±»å‹: {model_config.model_type}")
    print(f"å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    print(f"GPUæ•°é‡: {yaml_config['num_gpus']}")
    print(f"æ‰¹å¤§å°: {training_config.train_batch_size}")
    print(f"ä¼°ç®—æ˜¾å­˜: {memory_info['total_memory_gb']:.1f}GB/GPU")
    print(f"è¾“å‡ºç›®å½•: {os.path.abspath(training_config.output_dir)}")
    print(f"æ¨¡å‹ä¿å­˜: {os.path.abspath(training_config.checkpoint_dir)}")
    
    # æ˜¾ç¤ºè‡ªåŠ¨å…³æœºçŠ¶æ€
    auto_shutdown_enabled = yaml_config.get('system', {}).get('auto_shutdown', False)
    if auto_shutdown_enabled:
        shutdown_delay = yaml_config.get('system', {}).get('shutdown_delay', 60)
        print(f"ğŸ”„ è‡ªåŠ¨å…³æœº: å¯ç”¨ ({shutdown_delay}ç§’å»¶è¿Ÿ)")
        if is_docker_container():
            print(f"âš ï¸  Dockerç¯å¢ƒè­¦å‘Š: å°†æ˜¾ç¤ºå…³æœºæç¤ºä½†ä¸ä¼šå®é™…å…³æœº")
    else:
        print(f"ğŸ”„ è‡ªåŠ¨å…³æœº: ç¦ç”¨")
    
    if args.dry_run:
        print("\nâœ… é…ç½®éªŒè¯å®Œæˆï¼ˆdry_runæ¨¡å¼ï¼‰")
        return
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        yaml_config['num_gpus'] = 0
    elif yaml_config['num_gpus'] > torch.cuda.device_count():
        print(f"âš ï¸ è¯·æ±‚{yaml_config['num_gpus']}ä¸ªGPUï¼Œä½†åªæœ‰{torch.cuda.device_count()}ä¸ªå¯ç”¨")
        yaml_config['num_gpus'] = torch.cuda.device_count()
    
    # å¼€å§‹è®­ç»ƒ
    world_size = yaml_config['num_gpus']
    
    try:
        if world_size <= 1:
            # å•GPUè®­ç»ƒ
            print("ğŸš€ å¯åŠ¨å•GPUè®­ç»ƒ...")
            train_worker(0, 1, model_config, training_config)
        else:
            # å¤šGPUè®­ç»ƒ
            print(f"ğŸš€ å¯åŠ¨{world_size}GPUè®­ç»ƒ...")
            mp.spawn(
                train_worker,
                args=(world_size, model_config, training_config),
                nprocs=world_size,
                join=True
            )
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # è‡ªåŠ¨å…³æœºåŠŸèƒ½
        if auto_shutdown_enabled and not args.no_shutdown:
            shutdown_delay = yaml_config.get('system', {}).get('shutdown_delay', 60)
            auto_shutdown(shutdown_delay)
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")

if __name__ == "__main__":
    main() 