#!/usr/bin/env python3
"""
æ˜¾å­˜ä¼˜åŒ–ç‰ˆå¤šGPUè®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹å¤§æ¨¡å‹å’Œæœ‰é™æ˜¾å­˜çš„æƒ…å†µ
"""

import os
import sys
import argparse
import yaml
import torch
import torch.multiprocessing as mp
from torch.distributed import init_process_group, destroy_process_group
from torch.nn.parallel import DistributedDataParallel as DDP
import time
import subprocess
import platform
import gc

def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def setup_ddp(rank: int, world_size: int):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12356'  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
    init_process_group(backend="nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    destroy_process_group()

def check_available_memory(device):
    """æ£€æŸ¥å¯ç”¨æ˜¾å­˜"""
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(device).total_memory / 1e9
        allocated = torch.cuda.memory_allocated(device) / 1e9
        reserved = torch.cuda.memory_reserved(device) / 1e9
        free = total - reserved
        return free, total
    return 0, 0

def train_worker_optimized(rank: int, world_size: int, config_path: str):
    """æ˜¾å­˜ä¼˜åŒ–çš„è®­ç»ƒå·¥ä½œè¿›ç¨‹"""
    
    # è®¾ç½®åˆ†å¸ƒå¼
    if world_size > 1:
        setup_ddp(rank, world_size)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')
    
    # åœ¨æ¯ä¸ªè¿›ç¨‹å¼€å§‹å‰æ¸…ç†æ˜¾å­˜
    clear_gpu_memory()
    
    # æ£€æŸ¥å¯ç”¨æ˜¾å­˜
    free_mem, total_mem = check_available_memory(rank)
    if rank == 0:
        print(f"GPU {rank}: å¯ç”¨æ˜¾å­˜ {free_mem:.2f}GB / æ€»æ˜¾å­˜ {total_mem:.2f}GB")
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from configs.base import ModelConfig
    from models import create_model
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ModelConfig(
        model_type=config['model_type'],
        vocab_size=config['model']['vocab_size'],
        max_seq_length=config['model']['max_seq_length'],
        d_model=config['model']['d_model'],
        n_layers=config['model']['n_layers'],
        n_heads=config['model']['n_heads'],
        d_ff=config['model']['d_ff'],
        dropout=config['model']['dropout']
    )
    
    # åˆ›å»ºæ¨¡å‹
    print(f"GPU {rank}: åˆ›å»ºæ¨¡å‹...")
    model = create_model(model_config.model_type, model_config)
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœé…ç½®å¯ç”¨ï¼‰
    if config.get('optimization', {}).get('gradient_checkpointing', False):
        print(f"GPU {rank}: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
    
    # æ£€æŸ¥æ¨¡å‹åˆ›å»ºåçš„æ˜¾å­˜
    model_mem_before = torch.cuda.memory_allocated(rank) / 1e9 if torch.cuda.is_available() else 0
    if rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"æ¨¡å‹åˆ›å»ºåæ˜¾å­˜ä½¿ç”¨: {model_mem_before:.2f}GB")
    
    # ç§»åŠ¨æ¨¡å‹åˆ°GPUï¼ˆåˆ†æ‰¹è¿›è¡Œä»¥å‡å°‘æ˜¾å­˜å³°å€¼ï¼‰
    print(f"GPU {rank}: ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡...")
    try:
        # é€å±‚ç§»åŠ¨æ¨¡å‹ä»¥å‡å°‘æ˜¾å­˜å³°å€¼
        if hasattr(model, 'transformer'):
            # Transformeræ¨¡å‹
            model.transformer.wte = model.transformer.wte.to(device)
            model.transformer.wpe = model.transformer.wpe.to(device)
            
            for i, layer in enumerate(model.transformer.h):
                layer = layer.to(device)
                if i % 4 == 0:  # æ¯4å±‚æ¸…ç†ä¸€æ¬¡ç¼“å­˜
                    clear_gpu_memory()
            
            model.transformer.ln_f = model.transformer.ln_f.to(device)
            model.lm_head = model.lm_head.to(device)
        else:
            # å…¶ä»–æ¨¡å‹ç±»å‹ç›´æ¥ç§»åŠ¨
            model = model.to(device)
            
    except torch.cuda.OutOfMemoryError as e:
        print(f"GPU {rank}: æ˜¾å­˜ä¸è¶³ - {e}")
        # å°è¯•æ¸…ç†å¹¶é‡è¯•
        clear_gpu_memory()
        print(f"GPU {rank}: æ¸…ç†ç¼“å­˜åé‡è¯•...")
        try:
            model = model.to(device)
        except torch.cuda.OutOfMemoryError:
            print(f"GPU {rank}: æ˜¾å­˜ä»ç„¶ä¸è¶³ï¼Œè¯·å‡å°æ¨¡å‹è§„æ¨¡æˆ–æ‰¹å¤§å°")
            return
    
    # æ£€æŸ¥æ¨¡å‹ç§»åŠ¨åçš„æ˜¾å­˜
    model_mem_after = torch.cuda.memory_allocated(rank) / 1e9 if torch.cuda.is_available() else 0
    if rank == 0:
        print(f"æ¨¡å‹ç§»åŠ¨åæ˜¾å­˜ä½¿ç”¨: {model_mem_after:.2f}GB")
    
    # åŒ…è£…ä¸ºDDP
    if world_size > 1:
        model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    
    # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼ˆæ¼”ç¤ºï¼‰
    print(f"GPU {rank}: å¼€å§‹è®­ç»ƒ...")
    
    # åˆ›å»ºè™šæ‹Ÿæ‰¹æ¬¡æ•°æ®è¿›è¡Œæµ‹è¯•
    batch_size = config['training']['batch_size']
    seq_length = config['training']['max_length']
    
    try:
        # æµ‹è¯•å‰å‘ä¼ æ’­
        dummy_input = torch.randint(0, model_config.vocab_size, (batch_size, seq_length)).to(device)
        
        with torch.cuda.amp.autocast(enabled=config['training']['fp16']):
            output = model(dummy_input)
        
        if rank == 0:
            forward_mem = torch.cuda.memory_allocated(rank) / 1e9
            print(f"å‰å‘ä¼ æ’­åæ˜¾å­˜: {forward_mem:.2f}GB")
            print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.logits.shape if hasattr(output, 'logits') else output.shape}")
            
            # ä¿å­˜ç®€å•çš„æ¨¡å‹æ£€æŸ¥ç‚¹
            os.makedirs(config['training']['checkpoint_dir'], exist_ok=True)
            checkpoint_path = os.path.join(config['training']['checkpoint_dir'], "test_model.pt")
            torch.save({
                'model_state_dict': model.module.state_dict() if world_size > 1 else model.state_dict(),
                'config': model_config.__dict__,
                'rank': rank
            }, checkpoint_path)
            print(f"âœ… æµ‹è¯•æ¨¡å‹å·²ä¿å­˜è‡³: {checkpoint_path}")
            
    except torch.cuda.OutOfMemoryError as e:
        print(f"GPU {rank}: å‰å‘ä¼ æ’­æ˜¾å­˜ä¸è¶³ - {e}")
        print(f"å»ºè®®: å‡å°batch_sizeæˆ–max_length")
    
    # æ¸…ç†
    if world_size > 1:
        cleanup_ddp()
    clear_gpu_memory()

def main():
    parser = argparse.ArgumentParser(description="æ˜¾å­˜ä¼˜åŒ–çš„å¤šGPUè®­ç»ƒ")
    parser.add_argument("--config", type=str, default="config_7b_transformer_fixed.yaml", help="é…ç½®æ–‡ä»¶")
    parser.add_argument("--check_memory", action="store_true", help="åªæ£€æŸ¥æ˜¾å­˜ä¸è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¾å­˜æ¨¡å¼
    if args.check_memory:
        if torch.cuda.is_available():
            print("ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total = props.total_memory / 1e9
                allocated = torch.cuda.memory_allocated(i) / 1e9
                reserved = torch.cuda.memory_reserved(i) / 1e9
                free = total - reserved
                
                print(f"GPU {i}: {props.name}")
                print(f"  æ€»æ˜¾å­˜: {total:.2f}GB")
                print(f"  å·²åˆ†é…: {allocated:.2f}GB") 
                print(f"  å·²ä¿ç•™: {reserved:.2f}GB")
                print(f"  å¯ç”¨: {free:.2f}GB")
                
                if free < 15:  # 7Bæ¨¡å‹è‡³å°‘éœ€è¦15GBå¯ç”¨æ˜¾å­˜
                    print(f"  âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³ï¼ˆéœ€è¦çº¦15-20GBï¼‰")
                else:
                    print(f"  âœ… æ˜¾å­˜å……è¶³")
        return
    
    # åŠ è½½é…ç½®
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    world_size = config['num_gpus']
    
    print(f"ğŸš€ å¯åŠ¨æ˜¾å­˜ä¼˜åŒ–è®­ç»ƒ...")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"GPUæ•°é‡: {world_size}")
    print(f"æ‰¹å¤§å°: {config['training']['batch_size']}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {config['training']['gradient_accumulation_steps']}")
    print(f"æ··åˆç²¾åº¦: {config['training']['fp16']}")
    
    # æ¸…ç†æ‰€æœ‰GPUçš„ç¼“å­˜
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
    
    try:
        if world_size <= 1:
            train_worker_optimized(0, 1, args.config)
        else:
            mp.spawn(
                train_worker_optimized,
                args=(world_size, args.config),
                nprocs=world_size,
                join=True
            )
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥GPUæ˜¾å­˜: python train_memory_optimized.py --check_memory")
        print("2. å‡å°æ‰¹å¤§å°æˆ–åºåˆ—é•¿åº¦")
        print("3. æ¸…ç†å…¶ä»–GPUè¿›ç¨‹")

if __name__ == "__main__":
    main() 