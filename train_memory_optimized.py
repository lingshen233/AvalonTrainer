#!/usr/bin/env python3
"""
æ˜¾å­˜ä¼˜åŒ–ç‰ˆå•æœºå¤šGPUè®­ç»ƒè„šæœ¬
å–æ¶ˆåˆ†å¸ƒå¼è®­ç»ƒï¼Œä½¿ç”¨DataParallelï¼ŒèŠ‚çœæ˜¾å­˜
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
from torch.nn import DataParallel
import time
import subprocess
import platform
import gc
from pathlib import Path

def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
        gc.collect()
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰GPUç¼“å­˜")

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def is_docker_container():
    """æ£€æŸ¥æ˜¯å¦åœ¨Dockerå®¹å™¨ä¸­"""
    try:
        with open('/proc/1/cgroup', 'r') as f:
            return 'docker' in f.read()
    except:
        return False

def is_root_user():
    """æ£€æŸ¥æ˜¯å¦ä¸ºrootç”¨æˆ·"""
    return os.getuid() == 0 if hasattr(os, 'getuid') else False

def auto_shutdown(delay_seconds: int = 60):
    """è‡ªåŠ¨å…³æœºåŠŸèƒ½"""
    is_docker = is_docker_container()
    is_root = is_root_user()
    
    if not is_docker and not is_root:
        print(f"âš ï¸ éDockerç¯å¢ƒä¸”érootç”¨æˆ·ï¼Œæ— æ³•æ‰§è¡Œå…³æœºå‘½ä»¤")
        return
    
    print(f"ğŸ”„ {delay_seconds}ç§’åè‡ªåŠ¨å…³æœº...")
    
    for remaining in range(delay_seconds, 0, -1):
        print(f"â³ å€’è®¡æ—¶: {remaining}ç§’ (Ctrl+Cå–æ¶ˆ)")
        try:
            time.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸš« ç”¨æˆ·å–æ¶ˆè‡ªåŠ¨å…³æœº")
            return
    
    print("ğŸ”Œ æ‰§è¡Œå…³æœºå‘½ä»¤...")
    try:
        if is_docker:
            subprocess.run(['halt'], check=True)
        else:
            subprocess.run(['shutdown', '-h', 'now'], check=True)
    except subprocess.CalledProcessError as e:
        print(f"âŒ å…³æœºå‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
    except FileNotFoundError:
        print("âŒ å…³æœºå‘½ä»¤æœªæ‰¾åˆ°")

def create_configs_from_yaml(yaml_config):
    """ä»YAMLé…ç½®åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®"""
    from configs.base import ModelConfig, TrainingConfig
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_params = yaml_config.get('model', {})
    model_config = ModelConfig(
        model_type=yaml_config.get('model_type', 'transformer'),
        vocab_size=model_params.get('vocab_size', 50257),
        max_seq_length=model_params.get('max_seq_length', 2048),
        d_model=model_params.get('d_model', 768),
        n_layers=model_params.get('n_layers', 12),
        n_heads=model_params.get('n_heads', 12),
        d_ff=model_params.get('d_ff', 3072),
        dropout=model_params.get('dropout', 0.1),
        # Mambaç‰¹æœ‰å‚æ•°
        d_state=model_params.get('d_state', 16),
        d_conv=model_params.get('d_conv', 4),
        expand=model_params.get('expand', 2)
    )
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    training_params = yaml_config.get('training', {})
    training_config = TrainingConfig(
        dataset_name=training_params.get('dataset', 'auto'),
        train_batch_size=training_params.get('batch_size', 4),
        eval_batch_size=training_params.get('eval_batch_size', 4),
        gradient_accumulation_steps=training_params.get('gradient_accumulation_steps', 1),
        max_length=training_params.get('max_length', 512),
        learning_rate=training_params.get('learning_rate', 5e-5),
        weight_decay=training_params.get('weight_decay', 0.01),
        max_grad_norm=training_params.get('max_grad_norm', 1.0),
        max_steps=training_params.get('max_steps', 10000),
        warmup_steps=training_params.get('warmup_steps', 1000),
        eval_steps=training_params.get('eval_steps', 500),
        save_steps=training_params.get('save_steps', 1000),
        logging_steps=training_params.get('logging_steps', 100),
        fp16=training_params.get('fp16', True),
        output_dir=training_params.get('output_dir', './outputs'),
        checkpoint_dir=training_params.get('checkpoint_dir', './checkpoints'),
        use_wandb=training_params.get('use_wandb', False),
        wandb_project=training_params.get('wandb_project', 'rag-transformer'),
        wandb_run_name=training_params.get('wandb_run_name', 'run'),
        distributed=False,  # å•æœºå¤šå¡ä¸ä½¿ç”¨åˆ†å¸ƒå¼
        world_size=1
    )
    
    return model_config, training_config

def calculate_model_size(model_config):
    """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
    if model_config.model_type == 'transformer':
        # Transformerå‚æ•°ä¼°ç®—
        embedding_params = model_config.vocab_size * model_config.d_model
        attention_params = 4 * model_config.d_model * model_config.d_model
        ffn_params = 2 * model_config.d_model * model_config.d_ff
        layer_params = attention_params + ffn_params
        total_params = embedding_params + model_config.n_layers * layer_params
    elif model_config.model_type == 'mamba':
        # Mambaå‚æ•°ä¼°ç®—ï¼ˆæ›´å‡†ç¡®ï¼‰
        embedding_params = model_config.vocab_size * model_config.d_model
        
        # æ¯å±‚Mambaå—çš„å‚æ•°
        d_inner = model_config.d_model * model_config.expand  # inner dimension
        
        # è¾“å…¥æŠ•å½±å±‚
        in_proj_params = model_config.d_model * (d_inner * 2)  # x and z projections
        
        # å·ç§¯å±‚
        conv_params = d_inner * model_config.d_conv
        
        # çŠ¶æ€ç©ºé—´å‚æ•°
        ss_params = d_inner * model_config.d_state  # A and B matrices
        dt_params = d_inner  # dt projection
        
        # è¾“å‡ºæŠ•å½±
        out_proj_params = d_inner * model_config.d_model
        
        # å±‚å½’ä¸€åŒ–
        norm_params = model_config.d_model
        
        # æ¯å±‚æ€»å‚æ•°
        layer_params = in_proj_params + conv_params + ss_params + dt_params + out_proj_params + norm_params
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–å’Œè¯­è¨€æ¨¡å‹å¤´
        final_norm_params = model_config.d_model
        lm_head_params = model_config.vocab_size * model_config.d_model
        
        total_params = embedding_params + (model_config.n_layers * layer_params) + final_norm_params + lm_head_params
    else:
        total_params = 0
    
    return total_params

def estimate_memory_usage(model_config, training_config):
    """ä¼°ç®—æ˜¾å­˜ä½¿ç”¨"""
    params = calculate_model_size(model_config)
    
    # åŸºç¡€æ¨¡å‹æ˜¾å­˜ (FP16)
    model_memory = params * 2 / 1e9  # 2 bytes per parameter
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ (Adam: 8 bytes per parameter)
    optimizer_memory = params * 8 / 1e9
    
    # æ¢¯åº¦ (FP16)
    gradient_memory = params * 2 / 1e9
    
    # æ¿€æ´»å€¼ä¼°ç®— (batch_size * seq_length * d_model * layers)
    activation_memory = (training_config.train_batch_size * 
                        training_config.max_length * 
                        model_config.d_model * 
                        model_config.n_layers * 2) / 1e9
    
    total_memory = model_memory + optimizer_memory + gradient_memory + activation_memory
    
    return {
        'model_memory_gb': model_memory,
        'optimizer_memory_gb': optimizer_memory,
        'gradient_memory_gb': gradient_memory,
        'activation_memory_gb': activation_memory,
        'total_memory_gb': total_memory
    }

def list_available_configs():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹é…ç½®"""
    return {
        'transformer': 'Transformeræ¨¡å‹ - æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶',
        'mamba': 'Mambaæ¨¡å‹ - çŠ¶æ€ç©ºé—´æ¨¡å‹'
    }

class OptimizedTrainer:
    """ä¼˜åŒ–çš„å•æœºå¤šGPUè®­ç»ƒå™¨"""
    
    def __init__(self, model, config, device_count):
        self.model = model
        self.config = config
        self.device_count = device_count
        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
        os.makedirs(self.config.checkpoint_dir, exist_ok=True)
        
        print(f"ğŸ’¾ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"   GPUæ•°é‡: {self.device_count}")
        print(f"   è¾“å‡ºç›®å½•: {os.path.abspath(self.config.output_dir)}")
        print(f"   æ£€æŸ¥ç‚¹ç›®å½•: {os.path.abspath(self.config.checkpoint_dir)}")
    
    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯ï¼ˆæ¼”ç¤ºï¼‰
        for step in range(1, min(self.config.max_steps + 1, 100)):  # é™åˆ¶æ­¥æ•°ç”¨äºæµ‹è¯•
            try:
                # åˆ›å»ºè™šæ‹Ÿæ‰¹æ¬¡æ•°æ®
                batch_size = self.config.train_batch_size
                seq_length = min(self.config.max_length, 1024)  # é™åˆ¶åºåˆ—é•¿åº¦
                
                # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®
                input_ids = torch.randint(0, 50257, (batch_size, seq_length))
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = input_ids.to(self.device)
                
                # å‰å‘ä¼ æ’­
                with torch.amp.autocast('cuda', enabled=self.config.fp16):
                    outputs = self.model(input_ids)
                    
                    # è®¡ç®—æŸå¤±
                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs
                    
                    # ç®€å•çš„è¯­è¨€å»ºæ¨¡æŸå¤±
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = input_ids[..., 1:].contiguous()
                    
                    loss_fct = nn.CrossEntropyLoss()
                    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                                  shift_labels.view(-1))
                
                # è®°å½•
                if step % self.config.logging_steps == 0:
                    current_mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
                    print(f"æ­¥éª¤ {step}: æŸå¤± {loss.item():.4f}, æ˜¾å­˜ {current_mem:.2f}GB")
                
                # æ¸…ç†
                del input_ids, outputs, logits, loss
                if step % 5 == 0:  # æ¯5æ­¥æ¸…ç†ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
                    clear_gpu_memory()
                
            except torch.cuda.OutOfMemoryError as e:
                print(f"âŒ æ­¥éª¤ {step} æ˜¾å­˜ä¸è¶³: {e}")
                print(f"ğŸ”§ æ‰¹å¤§å° {batch_size}, åºåˆ—é•¿åº¦ {seq_length} ä»ç„¶è¿‡å¤§")
                
                # å°è¯•æ›´æ¿€è¿›çš„å‡å°‘
                if batch_size > 1:
                    self.config.train_batch_size = max(1, batch_size // 2)
                    print(f"ğŸ”§ å‡å°æ‰¹å¤§å°è‡³ {self.config.train_batch_size}")
                elif seq_length > 256:
                    seq_length = max(256, seq_length // 2)
                    print(f"ğŸ”§ å‡å°åºåˆ—é•¿åº¦è‡³ {seq_length}")
                else:
                    print("âŒ æ— æ³•è¿›ä¸€æ­¥å‡å°å‚æ•°ï¼Œæ¨¡å‹å¯èƒ½å¤ªå¤§")
                    break
                    
                # æ¸…ç†åé‡è¯•
                clear_gpu_memory()
                continue
                
            except Exception as e:
                print(f"âŒ æ­¥éª¤ {step} è®­ç»ƒé”™è¯¯: {e}")
                break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(self.config.checkpoint_dir, "final_model.pt")
        
        # å¦‚æœä½¿ç”¨äº†DataParallelï¼Œéœ€è¦ä¿å­˜module
        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model
        
        # æ„å»ºå®Œæ•´çš„é…ç½®å­—å…¸
        config_dict = {
            'model_type': getattr(model_to_save, 'config', {}).get('model_type') or 
                         getattr(model_to_save, 'model_type', 'mamba'),
            'vocab_size': getattr(model_to_save.config, 'vocab_size', 50257) if hasattr(model_to_save, 'config') else 50257,
            'd_model': getattr(model_to_save.config, 'd_model', 4096) if hasattr(model_to_save, 'config') else 4096,
            'n_layers': getattr(model_to_save.config, 'n_layers', 32) if hasattr(model_to_save, 'config') else 32,
            'max_seq_length': getattr(model_to_save.config, 'max_seq_length', 4096) if hasattr(model_to_save, 'config') else 4096,
        }
        
        # æ·»åŠ Mambaç‰¹æœ‰å‚æ•°
        if 'mamba' in config_dict['model_type']:
            config_dict.update({
                'd_state': getattr(model_to_save.config, 'd_state', 16) if hasattr(model_to_save, 'config') else 16,
                'd_conv': getattr(model_to_save.config, 'd_conv', 4) if hasattr(model_to_save, 'config') else 4,
                'expand': getattr(model_to_save.config, 'expand', 2) if hasattr(model_to_save, 'config') else 2,
            })
        else:
            config_dict.update({
                'n_heads': getattr(model_to_save.config, 'n_heads', 32) if hasattr(model_to_save, 'config') else 32,
                'd_ff': getattr(model_to_save.config, 'd_ff', 16384) if hasattr(model_to_save, 'config') else 16384,
            })
        
        torch.save({
            'model_state_dict': model_to_save.state_dict(),
            'config': config_dict,
            'total_params': sum(p.numel() for p in model_to_save.parameters())
        }, final_model_path)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {os.path.abspath(final_model_path)}")

def main():
    parser = argparse.ArgumentParser(description="æ˜¾å­˜ä¼˜åŒ–çš„å•æœºå¤šGPUè®­ç»ƒè„šæœ¬")
    
    # é…ç½®å‚æ•°
    parser.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_type", type=str, choices=["transformer", "mamba"], help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--num_gpus", type=int, help="GPUæ•°é‡")
    
    # é¢„è®¾é…ç½®
    parser.add_argument("--preset", type=str, help="ä½¿ç”¨é¢„è®¾é…ç½® (å¦‚: 1b_transformer, 7b_mamba)")
    parser.add_argument("--list_models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--list_presets", action="store_true", help="åˆ—å‡ºå¯ç”¨é¢„è®¾é…ç½®")
    parser.add_argument("--list_datasets", action="store_true", help="åˆ—å‡ºå¯ç”¨æ•°æ®é›†")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, help="æ‰¹å¤§å°")
    parser.add_argument("--max_length", type=int, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--learning_rate", type=float, help="å­¦ä¹ ç‡")
    parser.add_argument("--max_steps", type=int, help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®")
    parser.add_argument("--no_shutdown", action="store_true", help="ç¦ç”¨è‡ªåŠ¨å…³æœº")
    parser.add_argument("--check_memory", action="store_true", help="æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨")
    parser.add_argument("--clear_cache", action="store_true", help="æ¸…ç†GPUç¼“å­˜")
    
    args = parser.parse_args()
    
    # æ¸…ç†ç¼“å­˜
    if args.clear_cache:
        clear_gpu_memory()
        return
    
    # æ£€æŸ¥æ˜¾å­˜
    if args.check_memory:
        if torch.cuda.is_available():
            print("ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
            total_free = 0
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total = props.total_memory / 1e9
                allocated = torch.cuda.memory_allocated(i) / 1e9
                reserved = torch.cuda.memory_reserved(i) / 1e9
                free = total - reserved
                total_free += free
                
                print(f"GPU {i}: {props.name}")
                print(f"  æ€»æ˜¾å­˜: {total:.2f}GB")
                print(f"  å·²åˆ†é…: {allocated:.2f}GB") 
                print(f"  å·²ä¿ç•™: {reserved:.2f}GB")
                print(f"  å¯ç”¨: {free:.2f}GB")
                
                if free < 10:
                    print(f"  âš ï¸  æ˜¾å­˜è¾ƒå°‘")
                else:
                    print(f"  âœ… æ˜¾å­˜å……è¶³")
            
            print(f"\næ€»å¯ç”¨æ˜¾å­˜: {total_free:.2f}GB")
        return
    
    # åˆ—å‡ºä¿¡æ¯
    if args.list_models:
        print("\nğŸ¤– å¯ç”¨æ¨¡å‹:")
        for model_type, desc in list_available_configs().items():
            print(f"  {model_type}: {desc}")
        return
    
    if args.list_presets:
        from configs.model_presets import list_model_presets
        list_model_presets()
        return
    
    if args.list_datasets:
        from data.dataset_manager import DatasetManager
        manager = DatasetManager()
        manager.list_datasets()
        return
    
    # å¤„ç†é¢„è®¾é…ç½®
    if args.preset:
        from configs.model_presets import MODEL_PRESETS, get_model_preset, get_training_config_for_model_size
        
        if args.preset not in MODEL_PRESETS:
            print(f"âŒ æœªçŸ¥é¢„è®¾é…ç½®: {args.preset}")
            print("å¯ç”¨é¢„è®¾:")
            for preset_id in MODEL_PRESETS.keys():
                print(f"  {preset_id}")
            return
        
        preset_config = get_model_preset(args.preset)
        model_config = preset_config['model']
        
        # ç”Ÿæˆè®­ç»ƒé…ç½®
        training_config = get_training_config_for_model_size(
            args.preset, 
            args.num_gpus or 1
        )
        
        print(f"âœ… ä½¿ç”¨é¢„è®¾é…ç½®: {preset_config['description']}")
        print(f"   å‚æ•°é‡: {preset_config['params']}")
        print(f"   æ˜¾å­˜éœ€æ±‚: {preset_config['memory_estimate']}")
        
        # åˆ›å»ºè™šæ‹Ÿyamlé…ç½®
        yaml_config = {
            'model_type': model_config.model_type,
            'num_gpus': args.num_gpus or 1,
            'system': {'auto_shutdown': False, 'shutdown_delay': 60}
        }
        
    else:
        # åŠ è½½é…ç½®æ–‡ä»¶
        if os.path.exists(args.config):
            yaml_config = load_config(args.config)
            print(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        else:
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        if args.model_type:
            yaml_config['model_type'] = args.model_type
        if args.num_gpus:
            yaml_config['num_gpus'] = args.num_gpus
        if args.no_shutdown:
            yaml_config['system']['auto_shutdown'] = False
        
        # è®­ç»ƒå‚æ•°è¦†ç›–
        if args.batch_size:
            yaml_config.setdefault('training', {})['batch_size'] = args.batch_size
        if args.max_length:
            yaml_config.setdefault('training', {})['max_length'] = args.max_length
        if args.learning_rate:
            yaml_config.setdefault('training', {})['learning_rate'] = args.learning_rate
        if args.max_steps:
            yaml_config.setdefault('training', {})['max_steps'] = args.max_steps
        
        # åˆ›å»ºé…ç½®
        model_config, training_config = create_configs_from_yaml(yaml_config)
    
    # è®¡ç®—èµ„æºéœ€æ±‚
    total_params = calculate_model_size(model_config)
    memory_info = estimate_memory_usage(model_config, training_config)
    
    # æ‰“å°ç¯å¢ƒä¿¡æ¯
    print(f"\nğŸŒ è¿è¡Œç¯å¢ƒ:")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()}")
    print(f"Dockerå®¹å™¨: {'æ˜¯' if is_docker_container() else 'å¦'}")
    print(f"Rootç”¨æˆ·: {'æ˜¯' if is_root_user() else 'å¦'}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"\nğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"æ¨¡å‹ç±»å‹: {model_config.model_type}")
    print(f"å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
    print(f"GPUæ•°é‡: {yaml_config['num_gpus']}")
    print(f"æ‰¹å¤§å°: {training_config.train_batch_size}")
    print(f"ä¼°ç®—æ˜¾å­˜: {memory_info['total_memory_gb']:.1f}GB/GPU")
    print(f"è¾“å‡ºç›®å½•: {os.path.abspath(training_config.output_dir)}")
    print(f"æ¨¡å‹ä¿å­˜: {os.path.abspath(training_config.checkpoint_dir)}")
    
    # æ˜¾ç¤ºè‡ªåŠ¨å…³æœºçŠ¶æ€
    auto_shutdown_enabled = yaml_config.get('system', {}).get('auto_shutdown', False)
    if auto_shutdown_enabled:
        shutdown_delay = yaml_config.get('system', {}).get('shutdown_delay', 60)
        print(f"ğŸ”„ è‡ªåŠ¨å…³æœº: å¯ç”¨ ({shutdown_delay}ç§’å»¶è¿Ÿ)")
    else:
        print(f"ğŸ”„ è‡ªåŠ¨å…³æœº: ç¦ç”¨")
    
    if args.dry_run:
        print("\nâœ… é…ç½®éªŒè¯å®Œæˆï¼ˆdry_runæ¨¡å¼ï¼‰")
        return
    
    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        device_count = 0
    else:
        device_count = min(yaml_config['num_gpus'], torch.cuda.device_count())
        if yaml_config['num_gpus'] > torch.cuda.device_count():
            print(f"âš ï¸ è¯·æ±‚{yaml_config['num_gpus']}ä¸ªGPUï¼Œä½†åªæœ‰{torch.cuda.device_count()}ä¸ªå¯ç”¨")
    
    print(f"ğŸš€ å¯åŠ¨å•æœº{device_count}GPUè®­ç»ƒï¼ˆä½¿ç”¨DataParallelï¼‰...")
    
    # æ¸…ç†GPUç¼“å­˜
    clear_gpu_memory()
    
    try:
        # å¯¼å…¥æ¨¡å‹
        from models import create_model
        
        # åˆ›å»ºæ¨¡å‹
        print("ğŸ“¦ åˆ›å»ºæ¨¡å‹...")
        model = create_model(model_config.model_type, model_config)
        
        # ç§»åŠ¨åˆ°ä¸»GPU
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        # ä½¿ç”¨DataParallelåŒ…è£…å¤šGPU
        if device_count > 1:
            print(f"ğŸ”— ä½¿ç”¨DataParallelåŒ…è£…{device_count}ä¸ªGPU...")
            gpu_ids = list(range(device_count))
            model = DataParallel(model, device_ids=gpu_ids)
            
            # è°ƒæ•´æ‰¹å¤§å°
            total_batch_size = training_config.train_batch_size * device_count
            training_config.train_batch_size = training_config.train_batch_size // device_count
            print(f"ğŸ“ è°ƒæ•´æ‰¹å¤§å°: æ¯GPU {training_config.train_batch_size}, æ€»è®¡ {total_batch_size}")
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = OptimizedTrainer(model, training_config, device_count)
        trainer.train()
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # è‡ªåŠ¨æµ‹è¯•è®­ç»ƒçš„æ¨¡å‹
        final_model_path = os.path.join(training_config.checkpoint_dir, "final_model.pt")
        if os.path.exists(final_model_path):
            print("\nğŸ§ª å¼€å§‹å¿«é€Ÿæµ‹è¯•è®­ç»ƒçš„æ¨¡å‹...")
            try:
                import subprocess
                result = subprocess.run([
                    sys.executable, 'test_after_training.py', 
                    '--checkpoint', final_model_path
                ], capture_output=True, text=True, timeout=120)
                
                if result.returncode == 0:
                    print("âœ… æ¨¡å‹å¿«é€Ÿæµ‹è¯•å®Œæˆ")
                    print("ğŸ’¡ å¦‚éœ€å®Œæ•´åŸºå‡†æµ‹è¯•ï¼Œè¯·è¿è¡Œ: python test_benchmark.py")
                else:
                    print(f"âš ï¸ æ¨¡å‹æµ‹è¯•é‡åˆ°é—®é¢˜: {result.stderr}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è¿è¡Œè‡ªåŠ¨æµ‹è¯•: {e}")
                print("ğŸ’¡ å¯æ‰‹åŠ¨è¿è¡Œ: python test_after_training.py")
        
        # è‡ªåŠ¨å…³æœºåŠŸèƒ½
        if auto_shutdown_enabled and not args.no_shutdown:
            shutdown_delay = yaml_config.get('system', {}).get('shutdown_delay', 60)
            auto_shutdown(shutdown_delay)
            
    except torch.cuda.OutOfMemoryError as e:
        print(f"âŒ æ˜¾å­˜ä¸è¶³: {e}")
        print("ğŸ’¡ å»ºè®®:")
        print("1. å‡å°æ‰¹å¤§å°: --batch_size 2")
        print("2. å‡å°åºåˆ—é•¿åº¦: --max_length 1024")
        print("3. æ¸…ç†GPUè¿›ç¨‹æˆ–ç¼“å­˜")
        print("4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹é¢„è®¾")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")

if __name__ == "__main__":
    main() 