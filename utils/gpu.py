"""
GPUç›¸å…³å·¥å…·å‡½æ•°
"""

import torch
from typing import Dict
from configs.base import ModelConfig, TrainingConfig
from configs.presets import calculate_model_size, estimate_memory_usage

def check_gpu_info() -> Dict:
    """æ£€æŸ¥GPUä¿¡æ¯"""
    if not torch.cuda.is_available():
        return {
            'available': False,
            'count': 0,
            'memory_gb': 0,
            'devices': []
        }
    
    gpu_count = torch.cuda.device_count()
    devices = []
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        memory_gb = props.total_memory / 1024**3
        devices.append({
            'id': i,
            'name': props.name,
            'memory_gb': memory_gb,
            'compute_capability': f"{props.major}.{props.minor}"
        })
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPUçš„ä¿¡æ¯ä½œä¸ºä»£è¡¨
    primary_gpu = devices[0] if devices else None
    
    info = {
        'available': True,
        'count': gpu_count,
        'memory_gb': primary_gpu['memory_gb'] if primary_gpu else 0,
        'devices': devices
    }
    
    # æ‰“å°GPUä¿¡æ¯
    print(f"ğŸ–¥ï¸  GPUä¿¡æ¯:")
    print(f"  æ•°é‡: {gpu_count}")
    for device in devices:
        print(f"  GPU {device['id']}: {device['name']} ({device['memory_gb']:.1f}GB)")
        
        # ç‰¹æ®Šæ ‡æ³¨
        if "3090" in device['name']:
            print(f"    ğŸ¯ RTX 3090æ£€æµ‹åˆ°ï¼æ¨èç”¨äº1Bæ¨¡å‹è®­ç»ƒ")
        elif "4090" in device['name']:
            print(f"    ğŸš€ RTX 4090æ£€æµ‹åˆ°ï¼å¯è®­ç»ƒæ›´å¤§æ¨¡å‹")
        elif "V100" in device['name'] or "A100" in device['name']:
            print(f"    âš¡ æ•°æ®ä¸­å¿ƒGPUæ£€æµ‹åˆ°ï¼é€‚åˆå¤§è§„æ¨¡è®­ç»ƒ")
    
    return info

def get_optimal_batch_size(model_config: ModelConfig, training_config: TrainingConfig, gpu_memory_gb: float) -> int:
    """æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ‰¹å¤§å°"""
    
    # é¢„è®¾çš„æ‰¹å¤§å°å€™é€‰
    batch_size_candidates = [1, 2, 4, 6, 8, 12, 16, 20, 24, 32]
    
    for batch_size in sorted(batch_size_candidates, reverse=True):
        # ä¸´æ—¶è®¾ç½®æ‰¹å¤§å°
        temp_config = TrainingConfig(**training_config.to_dict())
        temp_config.train_batch_size = batch_size
        
        # ä¼°ç®—æ˜¾å­˜ä½¿ç”¨
        memory_info = estimate_memory_usage(model_config, temp_config)
        
        # ç•™å‡º10%çš„å®‰å…¨ä½™é‡
        if memory_info['total_memory_gb'] <= gpu_memory_gb * 0.9:
            return batch_size
    
    # å¦‚æœæ‰€æœ‰å€™é€‰éƒ½ä¸è¡Œï¼Œè¿”å›æœ€å°å€¼
    return 1

def estimate_training_time(model_config: ModelConfig, training_config: TrainingConfig, gpu_info: Dict) -> Dict:
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å’ŒGPUç±»å‹ä¼°ç®—é€Ÿåº¦
    if gpu_info['available']:
        device_name = gpu_info['devices'][0]['name']
        
        # tokens/ç§’çš„ä¼°ç®—ï¼ˆåŸºäºç»éªŒï¼‰
        if "3090" in device_name:
            if model_config.model_type == "mamba":
                tokens_per_sec = 3000  # Mambaæ›´é«˜æ•ˆ
            else:
                tokens_per_sec = 2000  # Transformer
        elif "4090" in device_name:
            if model_config.model_type == "mamba":
                tokens_per_sec = 4500
            else:
                tokens_per_sec = 3000
        elif "V100" in device_name:
            if model_config.model_type == "mamba":
                tokens_per_sec = 2500
            else:
                tokens_per_sec = 1800
        elif "A100" in device_name:
            if model_config.model_type == "mamba":
                tokens_per_sec = 6000
            else:
                tokens_per_sec = 4500
        else:
            # å…¶ä»–GPU
            if model_config.model_type == "mamba":
                tokens_per_sec = 1500
            else:
                tokens_per_sec = 1000
    else:
        # CPUè®­ç»ƒï¼ˆå¾ˆæ…¢ï¼‰
        tokens_per_sec = 50
    
    # è®¡ç®—æ€»tokens
    total_tokens = (
        training_config.max_steps * 
        training_config.effective_batch_size * 
        training_config.max_length
    )
    
    # ä¼°ç®—æ—¶é—´
    total_seconds = total_tokens / tokens_per_sec
    hours = total_seconds / 3600
    days = hours / 24
    
    return {
        'total_tokens': total_tokens,
        'tokens_per_sec': tokens_per_sec,
        'total_seconds': total_seconds,
        'hours': hours,
        'days': days
    }

def get_gpu_recommendations(model_config: ModelConfig) -> Dict:
    """æ ¹æ®æ¨¡å‹è·å–GPUæ¨è"""
    
    total_params = calculate_model_size(model_config)
    
    if total_params < 500e6:  # <500M
        tier = "å°å‹"
        gpus = ["GTX 1080", "RTX 2080", "RTX 3060"]
    elif total_params < 1.5e9:  # <1.5B
        tier = "ä¸­å‹"
        gpus = ["RTX 3080", "RTX 3090", "RTX 4080"]
    elif total_params < 7e9:  # <7B
        tier = "å¤§å‹"
        gpus = ["RTX 4090", "A100-40GB", "V100-32GB"]
    else:  # >=7B
        tier = "è¶…å¤§å‹"
        gpus = ["A100-80GB", "H100", "å¤šå¡è®­ç»ƒ"]
    
    return {
        'tier': tier,
        'recommended_gpus': gpus,
        'min_memory_gb': 8 if total_params < 500e6 else 16 if total_params < 1.5e9 else 24,
        'recommended_memory_gb': 12 if total_params < 500e6 else 24 if total_params < 1.5e9 else 40
    } 