#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬
- ä¸‹è½½åŸºå‡†æ•°æ®é›†
- ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
- æ‰§è¡Œæ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°
"""

import os
import sys
import argparse
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import requests
import json
from pathlib import Path
import time

def download_benchmark_datasets():
    """ä¸‹è½½åŸºå‡†æµ‹è¯•æ•°æ®é›†"""
    print("ğŸ“¥ ä¸‹è½½åŸºå‡†æ•°æ®é›†...")
    
    datasets_info = {
        'wikitext': {
            'name': 'wikitext',
            'config': 'wikitext-2-raw-v1',
            'description': 'WikiText-2 è¯­è¨€å»ºæ¨¡æ•°æ®é›†'
        },
        'lambada': {
            'name': 'lambada',
            'config': None,
            'description': 'LAMBADA é˜…è¯»ç†è§£æ•°æ®é›†'
        },
        'hellaswag': {
            'name': 'hellaswag',
            'config': None,
            'description': 'HellaSwag å¸¸è¯†æ¨ç†æ•°æ®é›†'
        }
    }
    
    downloaded_datasets = {}
    
    for dataset_key, info in datasets_info.items():
        try:
            print(f"  æ­£åœ¨ä¸‹è½½ {info['description']}...")
            if info['config']:
                dataset = load_dataset(info['name'], info['config'])
            else:
                dataset = load_dataset(info['name'])
            
            downloaded_datasets[dataset_key] = dataset
            print(f"  âœ… {dataset_key} ä¸‹è½½å®Œæˆ")
            
        except Exception as e:
            print(f"  âŒ {dataset_key} ä¸‹è½½å¤±è´¥: {e}")
            downloaded_datasets[dataset_key] = None
    
    return downloaded_datasets

def download_pretrained_models():
    """ä¸‹è½½é¢„è®­ç»ƒçš„1Bæ¨¡å‹"""
    print("\nğŸ¤– ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹...")
    
    models_info = {
        'gpt2-xl': {
            'name': 'gpt2-xl',
            'description': 'GPT-2 XL (1.5Bå‚æ•°)',
            'size': '1.5B'
        },
        'EleutherAI/gpt-neo-1.3B': {
            'name': 'EleutherAI/gpt-neo-1.3B',
            'description': 'GPT-Neo 1.3B (1.3Bå‚æ•°)',
            'size': '1.3B'
        },
        'microsoft/DialoGPT-large': {
            'name': 'microsoft/DialoGPT-large',
            'description': 'DialoGPT Large (774Må‚æ•°)',
            'size': '774M'
        },
        'gpt2-medium': {
            'name': 'gpt2-medium',
            'description': 'GPT-2 Medium (355Må‚æ•°)',
            'size': '355M'
        },
        'distilgpt2': {
            'name': 'distilgpt2', 
            'description': 'DistilGPT-2 (82Må‚æ•°) - å¿«é€Ÿæµ‹è¯•',
            'size': '82M'
        }
    }
    
    downloaded_models = {}
    
    for model_key, info in models_info.items():
        try:
            print(f"  æ­£åœ¨ä¸‹è½½ {info['description']}...")
            
            # ä¸‹è½½tokenizerå’Œæ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(info['name'])
            model = AutoModelForCausalLM.from_pretrained(info['name'])
            
            downloaded_models[model_key] = {
                'tokenizer': tokenizer,
                'model': model,
                'info': info
            }
            
            print(f"  âœ… {model_key} ä¸‹è½½å®Œæˆ ({info['size']})")
            
        except Exception as e:
            print(f"  âŒ {model_key} ä¸‹è½½å¤±è´¥: {e}")
            downloaded_models[model_key] = None
    
    return downloaded_models

def evaluate_perplexity(model, tokenizer, dataset, max_samples=100):
    """è®¡ç®—å›°æƒ‘åº¦"""
    print("ğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
    
    model.eval()
    device = next(model.parameters()).device
    
    total_loss = 0
    total_tokens = 0
    
    # ä½¿ç”¨æµ‹è¯•é›†çš„ä¸€å°éƒ¨åˆ†
    test_texts = dataset['test']['text'][:max_samples] if 'test' in dataset else dataset['validation']['text'][:max_samples]
    
    with torch.no_grad():
        for i, text in enumerate(test_texts):
            if len(text.strip()) < 10:  # è·³è¿‡å¤ªçŸ­çš„æ–‡æœ¬
                continue
                
            try:
                # ç¼–ç æ–‡æœ¬
                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # è®¡ç®—æŸå¤±
                outputs = model(**inputs, labels=inputs['input_ids'])
                loss = outputs.loss
                
                total_loss += loss.item() * inputs['input_ids'].size(1)
                total_tokens += inputs['input_ids'].size(1)
                
                if (i + 1) % 10 == 0:
                    print(f"  å·²å¤„ç† {i+1}/{len(test_texts)} ä¸ªæ ·æœ¬...")
                    
            except Exception as e:
                print(f"  è·³è¿‡æ ·æœ¬ {i}: {e}")
                continue
    
    if total_tokens > 0:
        avg_loss = total_loss / total_tokens
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        return perplexity
    else:
        return float('inf')

def test_text_generation(model, tokenizer, prompts=None):
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
    print("âœï¸ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    
    if prompts is None:
        prompts = [
            "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•",
            "Once upon a time",
            "The meaning of life is", 
            "ç§‘æŠ€æ”¹å˜äº†æˆ‘ä»¬çš„ç”Ÿæ´»"
        ]
    
    model.eval()
    device = next(model.parameters()).device
    
    results = []
    
    for prompt in prompts:
        try:
            print(f"\n  æç¤ºè¯: '{prompt}'")
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors='pt').to(device)
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    max_length=inputs['input_ids'].size(1) + 50,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = generated_text[len(prompt):].strip()
            
            print(f"  ç”Ÿæˆ: {generated_text}")
            results.append({
                'prompt': prompt,
                'generated': generated_text
            })
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
            results.append({
                'prompt': prompt,
                'generated': None,
                'error': str(e)
            })
    
    return results

def test_trained_model(checkpoint_path='checkpoints/final_model.pt'):
    """æµ‹è¯•æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒçš„æ¨¡å‹: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"  âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    try:
        # å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹ç³»ç»Ÿ
        from models import create_model
        from configs.base import ModelConfig
        from transformers import AutoTokenizer
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model_config = ModelConfig(**checkpoint['config'])
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(model_config.model_type, model_config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # ä½¿ç”¨GPT-2çš„tokenizerï¼ˆå…¼å®¹æ€§è¾ƒå¥½ï¼‰
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  æ¨¡å‹ç±»å‹: {model_config.model_type}")
        print(f"  å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        
        return {
            'model': model,
            'tokenizer': tokenizer,
            'config': model_config,
            'checkpoint_path': checkpoint_path
        }
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ç»¼åˆæµ‹è¯•...")
    
    # æ£€æŸ¥GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"GPUä¿¡æ¯: {torch.cuda.get_device_name()}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # ä¸‹è½½æ•°æ®é›†
    datasets = download_benchmark_datasets()
    
    # ä¸‹è½½æ¨¡å‹
    models = download_pretrained_models()
    
    # æµ‹è¯•æˆ‘ä»¬è®­ç»ƒçš„æ¨¡å‹
    trained_model = test_trained_model()
    if trained_model is not None:
        models['our_trained_model'] = {
            'model': trained_model['model'],
            'tokenizer': trained_model['tokenizer'],
            'info': {
                'description': f"æˆ‘ä»¬è®­ç»ƒçš„{trained_model['config'].model_type}æ¨¡å‹",
                'size': f"{sum(p.numel() for p in trained_model['model'].parameters())/1e6:.1f}M"
            }
        }
    
    # æµ‹è¯•ç»“æœ
    test_results = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'device': str(device),
        'models': {}
    }
    
    print("\n" + "="*60)
    print("å¼€å§‹æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    for model_name, model_data in models.items():
        if model_data is None:
            continue
            
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        print("-" * 40)
        
        try:
            model = model_data['model'].to(device)
            tokenizer = model_data['tokenizer']
            
            # ç¡®ä¿tokenizeræœ‰pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_results = {
                'model_info': model_data['info'],
                'perplexity': {},
                'generation_test': None
            }
            
            # æµ‹è¯•å›°æƒ‘åº¦
            for dataset_name, dataset in datasets.items():
                if dataset is not None:
                    try:
                        ppl = evaluate_perplexity(model, tokenizer, dataset, max_samples=50)
                        model_results['perplexity'][dataset_name] = ppl
                        print(f"  {dataset_name} å›°æƒ‘åº¦: {ppl:.2f}")
                    except Exception as e:
                        print(f"  âŒ {dataset_name} å›°æƒ‘åº¦æµ‹è¯•å¤±è´¥: {e}")
                        model_results['perplexity'][dataset_name] = None
            
            # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
            try:
                generation_results = test_text_generation(model, tokenizer)
                model_results['generation_test'] = generation_results
            except Exception as e:
                print(f"  âŒ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
                model_results['generation_test'] = None
            
            test_results['models'][model_name] = model_results
            
            # æ¸…ç†GPUå†…å­˜
            del model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
            test_results['models'][model_name] = {'error': str(e)}
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    results_dir = Path('test_results')
    results_dir.mkdir(exist_ok=True)
    
    results_file = results_dir / f'benchmark_results_{int(time.time())}.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    for model_name, results in test_results['models'].items():
        if 'error' in results:
            print(f"âŒ {model_name}: æµ‹è¯•å¤±è´¥")
        else:
            print(f"âœ… {model_name}: æµ‹è¯•å®Œæˆ")
            if results.get('perplexity'):
                for dataset, ppl in results['perplexity'].items():
                    if ppl is not None:
                        print(f"   {dataset}: å›°æƒ‘åº¦ {ppl:.2f}")

def main():
    parser = argparse.ArgumentParser(description="è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•è„šæœ¬")
    parser.add_argument("--datasets-only", action="store_true", help="åªä¸‹è½½æ•°æ®é›†")
    parser.add_argument("--models-only", action="store_true", help="åªä¸‹è½½æ¨¡å‹")
    parser.add_argument("--quick-test", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--test-trained", type=str, help="æµ‹è¯•æŒ‡å®šçš„è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--skip-download", action="store_true", help="è·³è¿‡ä¸‹è½½ï¼Œåªæµ‹è¯•è®­ç»ƒæ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.datasets_only:
        download_benchmark_datasets()
    elif args.models_only:
        download_pretrained_models()
    elif args.test_trained:
        # åªæµ‹è¯•æŒ‡å®šçš„è®­ç»ƒæ¨¡å‹
        test_trained_model(args.test_trained)
    elif args.skip_download:
        # åªæµ‹è¯•æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹ï¼Œä¸ä¸‹è½½å…¶ä»–æ¨¡å‹
        trained_model = test_trained_model()
        if trained_model is not None:
            datasets = download_benchmark_datasets()
            # ç®€åŒ–æµ‹è¯•æµç¨‹...
            print("ğŸ§ª ç®€åŒ–æµ‹è¯•å®Œæˆ")
    else:
        run_comprehensive_test()

if __name__ == "__main__":
    main() 