#!/usr/bin/env python3
"""
ç»¼åˆåŸºå‡†æµ‹è¯•è„šæœ¬
ä¸‹è½½æ ‡å‡†æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°
"""

import os
import sys
import argparse
import torch
import time
import json
from pathlib import Path
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

def remove_module_prefix(state_dict):
    """ç§»é™¤DDPæ¨¡å‹çš„module.å‰ç¼€"""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v  # ç§»é™¤'module.'å‰ç¼€
        else:
            new_state_dict[k] = v
    return new_state_dict

def download_benchmarks():
    """ä¸‹è½½åŸºå‡†æµ‹è¯•æ•°æ®é›†"""
    print("ğŸ“¥ ä¸‹è½½åŸºå‡†æ•°æ®é›†...")
    datasets = {}
    
    # WikiText-2 è¯­è¨€å»ºæ¨¡
    try:
        print("  æ­£åœ¨ä¸‹è½½ WikiText-2 è¯­è¨€å»ºæ¨¡æ•°æ®é›†...")
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
        datasets['wikitext'] = dataset
        print("  âœ… wikitext ä¸‹è½½å®Œæˆ")
    except Exception as e:
        print(f"  âŒ wikitext ä¸‹è½½å¤±è´¥: {e}")
    
    # LAMBADA é˜…è¯»ç†è§£
    try:
        print("  æ­£åœ¨ä¸‹è½½ LAMBADA é˜…è¯»ç†è§£æ•°æ®é›†...")
        dataset = load_dataset("lambada", split="test")
        datasets['lambada'] = dataset
        print("  âœ… lambada ä¸‹è½½å®Œæˆ")
    except Exception as e:
        print(f"  âŒ lambada ä¸‹è½½å¤±è´¥: {e}")
    
    # HellaSwag å¸¸è¯†æ¨ç†
    try:
        print("  æ­£åœ¨ä¸‹è½½ HellaSwag å¸¸è¯†æ¨ç†æ•°æ®é›†...")
        dataset = load_dataset("hellaswag", split="validation", trust_remote_code=True)
        datasets['hellaswag'] = dataset
        print("  âœ… hellaswag ä¸‹è½½å®Œæˆ")
    except Exception as e:
        print(f"  âŒ hellaswag ä¸‹è½½å¤±è´¥: {e}")
    
    return datasets

def download_baseline_models():
    """ä¸‹è½½åŸºå‡†å¯¹æ¯”æ¨¡å‹"""
    print("\nğŸ¤– ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹...")
    models = {}
    
    baseline_models = [
        ("gpt2-xl", "GPT-2 XL (1.5Bå‚æ•°)"),
        ("EleutherAI/gpt-neo-1.3B", "GPT-Neo 1.3B (1.3Bå‚æ•°)"),
        ("microsoft/DialoGPT-large", "DialoGPT Large (774Må‚æ•°)"),
        ("gpt2-medium", "GPT-2 Medium (355Må‚æ•°)"),
        ("distilgpt2", "DistilGPT-2 (82Må‚æ•°) - å¿«é€Ÿæµ‹è¯•")
    ]
    
    for model_id, description in baseline_models:
        try:
            print(f"  æ­£åœ¨ä¸‹è½½ {description}...")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            param_count = sum(p.numel() for p in model.parameters())
            models[model_id] = {
                'model': model,
                'tokenizer': tokenizer,
                'params': param_count
            }
            
            print(f"  âœ… {model_id} ä¸‹è½½å®Œæˆ ({param_count/1e9:.1f}B)")
            
        except Exception as e:
            print(f"  âŒ {model_id} ä¸‹è½½å¤±è´¥: {e}")
    
    return models

def test_trained_model(checkpoint_path, datasets):
    """æµ‹è¯•è®­ç»ƒçš„æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•è®­ç»ƒçš„æ¨¡å‹: {checkpoint_path}")
    
    if not os.path.exists(checkpoint_path):
        print(f"  âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    try:
        from models import create_model
        from configs.base import ModelConfig
        
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        model_config = ModelConfig(**checkpoint['config'])
        
        model = create_model(model_config.model_type, model_config)
        
        # å¤„ç†DDPæ¨¡å‹çš„state_dict
        state_dict = checkpoint['model_state_dict']
        has_module_prefix = any(k.startswith('module.') for k in state_dict.keys())
        if has_module_prefix:
            print("  ğŸ”„ æ£€æµ‹åˆ°DDPæ¨¡å‹ï¼Œç§»é™¤module.å‰ç¼€...")
            state_dict = remove_module_prefix(state_dict)
        
        model.load_state_dict(state_dict, strict=True)
        model.eval()
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        
        # ä½¿ç”¨GPT-2 tokenizerï¼ˆå…¼å®¹æ€§æœ€å¥½ï¼‰
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"     ç±»å‹: {model_config.model_type}")
        print(f"     å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        
        # æµ‹è¯•å›°æƒ‘åº¦
        results = {}
        for dataset_name, dataset in datasets.items():
            if dataset is None:
                continue
                
            try:
                print(f"  ğŸ“Š åœ¨{dataset_name}ä¸Šè®¡ç®—å›°æƒ‘åº¦...")
                perplexity = calculate_perplexity_trained_model(model, tokenizer, dataset, device)
                results[dataset_name] = {'perplexity': perplexity}
                print(f"     {dataset_name} å›°æƒ‘åº¦: {perplexity:.2f}")
            except Exception as e:
                print(f"     âŒ {dataset_name} æµ‹è¯•å¤±è´¥: {e}")
                results[dataset_name] = {'error': str(e)}
        
        return {
            'model_type': model_config.model_type,
            'params': total_params,
            'results': results
        }
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def calculate_perplexity_trained_model(model, tokenizer, dataset, device, max_samples=50):
    """è®¡ç®—è®­ç»ƒæ¨¡å‹çš„å›°æƒ‘åº¦"""
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for i, example in enumerate(dataset):
            if i >= max_samples:
                break
                
            if i % 10 == 0 and i > 0:
                print(f"    å·²å¤„ç† {i}/{max_samples} ä¸ªæ ·æœ¬...")
            
            # è·å–æ–‡æœ¬
            text = example.get('text', '') or example.get('ending', '') or str(example)
            if not text or len(text.strip()) < 10:
                continue
            
            # Tokenize
            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            input_ids = inputs['input_ids'].to(device)
            
            if input_ids.size(1) < 2:
                continue
            
            # å‰å‘ä¼ æ’­
            outputs = model(input_ids)
            
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å‡º
            if isinstance(outputs, dict):
                if 'logits' in outputs:
                    logits = outputs['logits']
                else:
                    # è·å–ç¬¬ä¸€ä¸ªå¼ é‡è¾“å‡º
                    tensor_outputs = {k: v for k, v in outputs.items() if torch.is_tensor(v)}
                    logits = next(iter(tensor_outputs.values()))
            elif hasattr(outputs, 'logits'):
                logits = outputs.logits
            else:
                logits = outputs
            
            # è®¡ç®—æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = input_ids[..., 1:].contiguous()
            
            loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), 
                           shift_labels.view(-1))
            
            total_loss += loss.item()
            total_tokens += shift_labels.numel()
    
    if total_tokens == 0:
        return float('inf')
    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    return perplexity

def calculate_perplexity(model, tokenizer, dataset, device, max_samples=50):
    """è®¡ç®—å›°æƒ‘åº¦"""
    total_loss = 0
    total_tokens = 0
    
    model.eval()
    with torch.no_grad():
        for i, example in enumerate(dataset):
            if i >= max_samples:
                break
                
            if i % 10 == 0 and i > 0:
                print(f"  å·²å¤„ç† {i}/{max_samples} ä¸ªæ ·æœ¬...")
            
            # è·å–æ–‡æœ¬
            text = example.get('text', '') or example.get('ending', '') or str(example)
            if not text or len(text.strip()) < 10:
                continue
            
            # Tokenize
            try:
                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
                input_ids = inputs['input_ids'].to(device)
                
                if input_ids.size(1) < 2:
                    continue
                
                # å‰å‘ä¼ æ’­
                outputs = model(input_ids, labels=input_ids)
                loss = outputs.loss
                
                total_loss += loss.item() * input_ids.size(1)
                total_tokens += input_ids.size(1)
                
            except Exception as e:
                continue
    
    if total_tokens == 0:
        return float('inf')
    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    return perplexity

def test_text_generation(model, tokenizer, device):
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
    print("âœï¸ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    
    test_prompts = [
        "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•",
        "Once upon a time",
        "The meaning of life is",
        "ç§‘æŠ€æ”¹å˜äº†æˆ‘ä»¬çš„ç”Ÿæ´»"
    ]
    
    results = []
    
    for prompt in test_prompts:
        try:
            inputs = tokenizer(prompt, return_tensors='pt').to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    max_length=inputs['input_ids'].size(1) + 50,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = generated_text[len(prompt):].strip()
            
            print(f"\n  æç¤ºè¯: '{prompt}'")
            print(f"  ç”Ÿæˆ: {generated_text[:100]}...")
            
            results.append({
                'prompt': prompt,
                'generated': generated_text
            })
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆ'{prompt}'å¤±è´¥: {e}")
            results.append({
                'prompt': prompt,
                'error': str(e)
            })
    
    return results

def main():
    parser = argparse.ArgumentParser(description="ç»¼åˆåŸºå‡†æµ‹è¯•")
    parser.add_argument("--datasets-only", action="store_true", help="åªä¸‹è½½æ•°æ®é›†")
    parser.add_argument("--models-only", action="store_true", help="åªä¸‹è½½æ¨¡å‹")
    parser.add_argument("--trained-model", type=str, default="checkpoints/final_model.pt", 
                       help="è®­ç»ƒçš„æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹ç»¼åˆæµ‹è¯•...")
    
    # è®¾å¤‡ä¿¡æ¯
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"GPUä¿¡æ¯: {gpu_name}")
        print(f"æ˜¾å­˜: {gpu_memory:.1f}GB")
    
    results = {
        'device': str(device),
        'timestamp': time.time(),
        'models': {},
        'trained_model': None
    }
    
    # ä¸‹è½½æ•°æ®é›†
    if not args.models_only:
        datasets = download_benchmarks()
    else:
        datasets = {}
    
    # æµ‹è¯•è®­ç»ƒçš„æ¨¡å‹
    if not args.models_only and datasets:
        trained_results = test_trained_model(args.trained_model, datasets)
        if trained_results:
            results['trained_model'] = trained_results
    
    # ä¸‹è½½å’Œæµ‹è¯•åŸºå‡†æ¨¡å‹
    if not args.datasets_only:
        baseline_models = download_baseline_models()
        
        print("\n" + "="*60)
        print("å¼€å§‹æ¨¡å‹æµ‹è¯•")
        print("="*60)
        
        for model_id, model_info in baseline_models.items():
            print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_id}")
            print("-" * 40)
            
            model = model_info['model']
            tokenizer = model_info['tokenizer']
            
            model_results = {
                'params': model_info['params'],
                'perplexity': {},
                'generation': []
            }
            
            # å›°æƒ‘åº¦æµ‹è¯•
            for dataset_name, dataset in datasets.items():
                if dataset is None:
                    continue
                    
                try:
                    print(f"ğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
                    perplexity = calculate_perplexity(model, tokenizer, dataset, device)
                    model_results['perplexity'][dataset_name] = perplexity
                    print(f"  {dataset_name} å›°æƒ‘åº¦: {perplexity:.2f}")
                except Exception as e:
                    print(f"  âŒ {dataset_name} å›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {e}")
                    model_results['perplexity'][dataset_name] = None
            
            # æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
            try:
                generation_results = test_text_generation(model, tokenizer, device)
                model_results['generation'] = generation_results
            except Exception as e:
                print(f"âŒ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
            
            results['models'][model_id] = model_results
    
    # ä¿å­˜ç»“æœ
    results_dir = Path('test_results')
    results_dir.mkdir(exist_ok=True)
    
    results_file = results_dir / f'benchmark_results_{int(time.time())}.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {results_file}")
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    if results['trained_model']:
        tm = results['trained_model']
        print(f"ğŸ¤– è®­ç»ƒçš„æ¨¡å‹ ({tm['model_type']}):")
        for dataset, result in tm['results'].items():
            if 'perplexity' in result:
                print(f"   {dataset}: å›°æƒ‘åº¦ {result['perplexity']:.2f}")
        print()
    
    for model_id, model_result in results['models'].items():
        print(f"âœ… {model_id}: æµ‹è¯•å®Œæˆ")
        for dataset, perplexity in model_result['perplexity'].items():
            if perplexity is not None:
                print(f"   {dataset}: å›°æƒ‘åº¦ {perplexity:.2f}")

if __name__ == "__main__":
    main() 