#!/bin/bash

echo "ğŸš€ æç«¯å®‰å…¨æ¨¡å¼ï¼š6GPUè®­ç»ƒå¯åŠ¨è„šæœ¬"
echo "================================================"

# è®¾ç½®æœ€å¤§å†…å­˜ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,roundup_power2_divisions:16"
export CUDA_LAUNCH_BLOCKING=0
export TORCH_USE_CUDA_DSA=1

# è®¾ç½®CPUä½¿ç”¨
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# æ¸…ç†GPUç¼“å­˜
python -c "import torch; torch.cuda.empty_cache(); print('âœ… GPUç¼“å­˜å·²æ¸…ç†')"

echo "ğŸ“Š æ£€æŸ¥GPUçŠ¶æ€..."
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv,noheader,nounits

echo ""
echo "ğŸ”§ ä½¿ç”¨æç«¯ä¼˜åŒ–é…ç½®..."
echo "  - Batch Size: 6 (æ¯GPU 1ä¸ª)"
echo "  - ZeRO-3 + å…¨CPUå¸è½½"
echo "  - 32ä¸ªæ¿€æ´»æ£€æŸ¥ç‚¹"
echo "  - FP16 åˆå§‹scale: 8"
echo ""

# å¯åŠ¨è®­ç»ƒ
deepspeed --num_gpus=6 train_deepspeed.py \
    --preset 7b_mamba \
    --deepspeed_config deepspeed_6gpu_extreme.json \
    --max_seq_length 512 \
    --save_steps 100 \
    --eval_steps 50 \
    --logging_steps 10

echo "âœ… è®­ç»ƒå®Œæˆï¼" 