#!/usr/bin/env python3
"""
DeepSpeed ZeROä¼˜åŒ–ç‰ˆè®­ç»ƒè„šæœ¬ - æ”¯æŒçœŸæ­£çš„7Bæ¨¡å‹
ä½¿ç”¨ZeRO-2ä¼˜åŒ–ï¼Œå°†å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¤šä¸ªGPU
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import time
import subprocess
import platform
import gc
import json
from pathlib import Path

# DeepSpeedç›¸å…³å¯¼å…¥
try:
    import deepspeed
    from deepspeed.ops.adam import FusedAdam
    DEEPSPEED_AVAILABLE = True
    print("âœ… DeepSpeedå·²å®‰è£…")
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("âŒ DeepSpeedæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ ‡å‡†è®­ç»ƒ")

def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
        gc.collect()
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰GPUç¼“å­˜")

def load_config(config_path: str):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def create_deepspeed_config(model_config, training_config, num_gpus):
    """åˆ›å»ºDeepSpeedé…ç½®"""
    ds_config = {
        "train_batch_size": training_config.train_batch_size * num_gpus,
        "train_micro_batch_size_per_gpu": training_config.train_batch_size,
        "gradient_accumulation_steps": training_config.gradient_accumulation_steps,
        
        # å¯ç”¨ZeRO-2ä¼˜åŒ–
        "zero_optimization": {
            "stage": 2,  # ZeRO-2: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦
            "contiguous_gradients": True,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 5e8,
            "allgather_bucket_size": 5e8,
            "cpu_offload": False  # Mambaæ¨¡å‹ä¸é€‚åˆCPUå¸è½½
        },
        
        # æ··åˆç²¾åº¦
        "fp16": {
            "enabled": training_config.fp16,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        
        # ä¼˜åŒ–å™¨é…ç½®
        "optimizer": {
            "type": "Adam",
            "params": {
                "lr": training_config.learning_rate,
                "betas": [0.9, 0.95],
                "eps": 1e-8,
                "weight_decay": training_config.weight_decay
            }
        },
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": training_config.learning_rate,
                "warmup_num_steps": training_config.warmup_steps
            }
        },
        
        # æ¢¯åº¦è£å‰ª
        "gradient_clipping": training_config.max_grad_norm,
        
        # æ£€æŸ¥ç‚¹é…ç½®
        "steps_per_print": training_config.logging_steps,
        "wall_clock_breakdown": False,
        
        # å†…å­˜ä¼˜åŒ–
        "activation_checkpointing": {
            "partition_activations": True,
            "cpu_checkpointing": False,
            "contiguous_memory_optimization": True,
            "number_checkpoints": 4,
            "synchronize_checkpoint_boundary": True
        }
    }
    
    return ds_config

def create_configs_from_yaml(yaml_config):
    """ä»YAMLé…ç½®åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒé…ç½®"""
    from configs.base import ModelConfig, TrainingConfig
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_params = yaml_config.get('model', {})
    model_config = ModelConfig(
        model_type=yaml_config.get('model_type', 'mamba'),
        vocab_size=model_params.get('vocab_size', 50257),
        max_seq_length=model_params.get('max_seq_length', 4096),
        d_model=model_params.get('d_model', 4864),
        n_layers=model_params.get('n_layers', 45),
        n_heads=model_params.get('n_heads', 32),
        d_ff=model_params.get('d_ff', 16384),
        dropout=model_params.get('dropout', 0.1),
        # Mambaç‰¹æœ‰å‚æ•°
        d_state=model_params.get('d_state', 16),
        d_conv=model_params.get('d_conv', 4),
        expand=model_params.get('expand', 2)
    )
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    training_params = yaml_config.get('training', {})
    training_config = TrainingConfig(
        dataset_name=training_params.get('dataset', 'auto'),
        train_batch_size=training_params.get('batch_size', 1),
        eval_batch_size=training_params.get('eval_batch_size', 1),
        gradient_accumulation_steps=training_params.get('gradient_accumulation_steps', 8),
        max_length=training_params.get('max_length', 4096),
        learning_rate=training_params.get('learning_rate', 1e-4),
        weight_decay=training_params.get('weight_decay', 0.01),
        max_grad_norm=training_params.get('max_grad_norm', 1.0),
        max_steps=training_params.get('max_steps', 200000),
        warmup_steps=training_params.get('warmup_steps', 10000),
        eval_steps=training_params.get('eval_steps', 5000),
        save_steps=training_params.get('save_steps', 10000),
        logging_steps=training_params.get('logging_steps', 100),
        fp16=training_params.get('fp16', True),
        output_dir=training_params.get('output_dir', './outputs'),
        checkpoint_dir=training_params.get('checkpoint_dir', './checkpoints'),
        use_wandb=training_params.get('use_wandb', False),
        wandb_project=training_params.get('wandb_project', 'rag-transformer'),
        wandb_run_name=training_params.get('wandb_run_name', 'deepspeed_7b'),
        distributed=True,  # DeepSpeedéœ€è¦åˆ†å¸ƒå¼
        world_size=yaml_config.get('num_gpus', 4)
    )
    
    return model_config, training_config

class DeepSpeedTrainer:
    """DeepSpeedä¼˜åŒ–è®­ç»ƒå™¨"""
    
    def __init__(self, model_config, training_config, ds_config):
        self.model_config = model_config
        self.training_config = training_config
        self.ds_config = ds_config
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(training_config.output_dir, exist_ok=True)
        os.makedirs(training_config.checkpoint_dir, exist_ok=True)
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        if not torch.distributed.is_initialized():
            deepspeed.init_distributed()
        
        self.local_rank = int(os.environ.get('LOCAL_RANK', 0))
        self.world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        print(f"ğŸ’¾ DeepSpeedè®­ç»ƒå™¨åˆå§‹åŒ–")
        print(f"   Local Rank: {self.local_rank}")
        print(f"   World Size: {self.world_size}")
        print(f"   è¾“å‡ºç›®å½•: {os.path.abspath(training_config.output_dir)}")
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        from models import create_model
        
        print("ğŸ“¦ åˆ›å»ºæ¨¡å‹...")
        model = create_model(self.model_config.model_type, self.model_config)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        if self.local_rank == 0:
            print(f"æ¨¡å‹å‚æ•°: {total_params:,} ({total_params/1e9:.2f}B)")
            print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e9:.2f}B)")
        
        return model
    
    def create_dataloader(self):
        """åˆ›å»ºç®€åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        # è¿™é‡Œä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º
        class DummyDataset:
            def __init__(self, vocab_size, max_length, num_samples=10000):
                self.vocab_size = vocab_size
                self.max_length = max_length
                self.num_samples = num_samples
            
            def __len__(self):
                return self.num_samples
            
            def __getitem__(self, idx):
                # ç”Ÿæˆéšæœºåºåˆ—
                input_ids = torch.randint(0, self.vocab_size, (self.max_length,))
                return {'input_ids': input_ids, 'labels': input_ids.clone()}
        
        dataset = DummyDataset(
            self.model_config.vocab_size,
            self.model_config.max_seq_length
        )
        
        # ä½¿ç”¨DeepSpeedçš„æ•°æ®é‡‡æ ·å™¨
        sampler = torch.utils.data.DistributedSampler(
            dataset,
            num_replicas=self.world_size,
            rank=self.local_rank,
            shuffle=True
        )
        
        dataloader = torch.utils.data.DataLoader(
            dataset,
            batch_size=self.training_config.train_batch_size,
            sampler=sampler,
            num_workers=2,
            pin_memory=True
        )
        
        return dataloader
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = self.create_dataloader()
        
        # åˆå§‹åŒ–DeepSpeedå¼•æ“
        if self.local_rank == 0:
            print("ğŸš€ åˆå§‹åŒ–DeepSpeedå¼•æ“...")
        
        model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
            model=model,
            config=self.ds_config
        )
        
        # å¼€å§‹è®­ç»ƒ
        if self.local_rank == 0:
            print("ğŸš€ å¼€å§‹DeepSpeedè®­ç»ƒ...")
        
        model_engine.train()
        global_step = 0
        
        try:
            for epoch in range(1, 6):  # é™åˆ¶epochæ•°ç”¨äºæ¼”ç¤º
                dataloader.sampler.set_epoch(epoch)
                
                for step, batch in enumerate(dataloader):
                    if global_step >= self.training_config.max_steps:
                        break
                    
                    # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
                    input_ids = batch['input_ids'].to(model_engine.device)
                    labels = batch['labels'].to(model_engine.device)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = model_engine(input_ids)
                    
                    # è®¡ç®—æŸå¤±
                    if hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs
                    
                    # è¯­è¨€å»ºæ¨¡æŸå¤±
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    loss_fct = nn.CrossEntropyLoss()
                    loss = loss_fct(
                        shift_logits.view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )
                    
                    # åå‘ä¼ æ’­
                    model_engine.backward(loss)
                    model_engine.step()
                    
                    global_step += 1
                    
                    # è®°å½•æ—¥å¿—
                    if global_step % self.training_config.logging_steps == 0 and self.local_rank == 0:
                        current_lr = lr_scheduler.get_last_lr()[0] if lr_scheduler else self.training_config.learning_rate
                        
                        # GPUæ˜¾å­˜ä½¿ç”¨
                        allocated = torch.cuda.memory_allocated(self.local_rank) / 1e9
                        reserved = torch.cuda.memory_reserved(self.local_rank) / 1e9
                        
                        print(f"æ­¥éª¤ {global_step}: æŸå¤± {loss.item():.4f}, "
                              f"å­¦ä¹ ç‡ {current_lr:.2e}, "
                              f"æ˜¾å­˜ {allocated:.1f}/{reserved:.1f}GB")
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if global_step % self.training_config.save_steps == 0:
                        if self.local_rank == 0:
                            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹ (æ­¥éª¤ {global_step})...")
                        
                        checkpoint_dir = os.path.join(
                            self.training_config.checkpoint_dir,
                            f"step_{global_step}"
                        )
                        model_engine.save_checkpoint(checkpoint_dir)
                    
                    # æ¸…ç†æ˜¾å­˜
                    if global_step % 10 == 0:
                        torch.cuda.empty_cache()
                
                if global_step >= self.training_config.max_steps:
                    break
        
        except Exception as e:
            if self.local_rank == 0:
                print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        if self.local_rank == 0:
            print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹...")
        
        final_checkpoint_dir = os.path.join(
            self.training_config.checkpoint_dir,
            "final"
        )
        model_engine.save_checkpoint(final_checkpoint_dir)
        
        if self.local_rank == 0:
            print("âœ… DeepSpeedè®­ç»ƒå®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description="DeepSpeed ZeROä¼˜åŒ–è®­ç»ƒè„šæœ¬")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--config", type=str, default="config_7b_mamba.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--preset", type=str, help="ä½¿ç”¨é¢„è®¾é…ç½®")
    parser.add_argument("--num_gpus", type=int, default=4, help="GPUæ•°é‡")
    
    # DeepSpeedå‚æ•°
    parser.add_argument("--deepspeed_config", type=str, help="DeepSpeedé…ç½®æ–‡ä»¶")
    parser.add_argument("--local_rank", type=int, default=-1, help="æœ¬åœ°rank (DeepSpeedè‡ªåŠ¨è®¾ç½®)")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--dry_run", action="store_true", help="åªéªŒè¯é…ç½®")
    parser.add_argument("--check_memory", action="store_true", help="æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥DeepSpeed
    if not DEEPSPEED_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£…DeepSpeed:")
        print("pip install deepspeed")
        return
    
    # æ˜¾å­˜æ£€æŸ¥
    if args.check_memory:
        print("ğŸ” GPUæ˜¾å­˜æ£€æŸ¥:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total = props.total_memory / 1e9
                print(f"GPU {i}: {props.name} - {total:.1f}GB")
        return
    
    # å¤„ç†é¢„è®¾é…ç½®
    if args.preset:
        from configs.model_presets import get_model_preset, get_training_config_for_model_size
        
        preset_config = get_model_preset(args.preset)
        model_config = preset_config['model']
        training_config = get_training_config_for_model_size(args.preset, args.num_gpus)
        
        print(f"âœ… ä½¿ç”¨é¢„è®¾: {preset_config['description']}")
        print(f"   å‚æ•°é‡: {preset_config['params']}")
        
        # åˆ›å»ºè™šæ‹Ÿyamlé…ç½®
        yaml_config = {
            'model_type': model_config.model_type,
            'num_gpus': args.num_gpus,
            'model': {
                'vocab_size': model_config.vocab_size,
                'max_seq_length': model_config.max_seq_length,
                'd_model': model_config.d_model,
                'n_layers': model_config.n_layers,
                'd_state': getattr(model_config, 'd_state', 16),
                'd_conv': getattr(model_config, 'd_conv', 4),
                'expand': getattr(model_config, 'expand', 2),
                'dropout': model_config.dropout
            },
            'training': {
                'batch_size': training_config.train_batch_size,
                'gradient_accumulation_steps': training_config.gradient_accumulation_steps,
                'max_length': training_config.max_length,
                'learning_rate': training_config.learning_rate,
                'max_steps': training_config.max_steps,
                'warmup_steps': training_config.warmup_steps,
                'save_steps': training_config.save_steps,
                'logging_steps': training_config.logging_steps,
                'fp16': training_config.fp16,
                'output_dir': training_config.output_dir,
                'checkpoint_dir': training_config.checkpoint_dir
            }
        }
    else:
        # åŠ è½½é…ç½®æ–‡ä»¶
        if not os.path.exists(args.config):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return
        
        yaml_config = load_config(args.config)
        yaml_config['num_gpus'] = args.num_gpus
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    model_config, training_config = create_configs_from_yaml(yaml_config)
    
    # åˆ›å»ºDeepSpeedé…ç½®
    ds_config = create_deepspeed_config(model_config, training_config, args.num_gpus)
    
    # ä¿å­˜DeepSpeedé…ç½®æ–‡ä»¶
    ds_config_path = "deepspeed_config.json"
    with open(ds_config_path, 'w') as f:
        json.dump(ds_config, f, indent=2)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    total_params = (model_config.vocab_size * model_config.d_model + 
                   model_config.n_layers * model_config.d_model * model_config.d_model * 4)
    
    print(f"\nğŸ“Š DeepSpeedè®­ç»ƒé…ç½®:")
    print(f"æ¨¡å‹ç±»å‹: {model_config.model_type}")
    print(f"ä¼°ç®—å‚æ•°: {total_params/1e9:.2f}B")
    print(f"GPUæ•°é‡: {args.num_gpus}")
    print(f"æ‰¹å¤§å°/GPU: {training_config.train_batch_size}")
    print(f"æ¢¯åº¦ç´¯ç§¯: {training_config.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹å¤§å°: {training_config.train_batch_size * args.num_gpus * training_config.gradient_accumulation_steps}")
    print(f"ZeROé˜¶æ®µ: {ds_config['zero_optimization']['stage']}")
    print(f"DeepSpeedé…ç½®: {ds_config_path}")
    
    if args.dry_run:
        print("\nâœ… é…ç½®éªŒè¯å®Œæˆï¼ˆdry_runæ¨¡å¼ï¼‰")
        return
    
    # å¯åŠ¨è®­ç»ƒ
    try:
        trainer = DeepSpeedTrainer(model_config, training_config, ds_config)
        trainer.train()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 