# ğŸ”§ æœåŠ¡å™¨ç«¯A800è®­ç»ƒä¿®å¤æŒ‡å—

ç”±äºç½‘ç»œé—®é¢˜æ— æ³•git pullï¼Œè¯·æ‰‹åŠ¨åº”ç”¨ä»¥ä¸‹ä¿®å¤ï¼š

## ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šæ‰¹æ¬¡å¤§å°é”™è¯¯

**å½“å‰é”™è¯¯**: `train_batch_size is not equal to micro_batch_per_gpu * gradient_acc_step * world_size 32 != 1 * 8 * 1`

**ç«‹å³è§£å†³æ–¹æ¡ˆ**:

```bash
cd ~/autodl-tmp/AvalonTrainer

# è¿è¡Œæ‰¹æ¬¡å¤§å°ä¿®å¤å·¥å…·
python fix_deepspeed_batch_size.py --num_gpus 1 --config deepspeed_single_a800_80g.json

# æˆ–è€…æ‰‹åŠ¨ä¿®å¤é…ç½®æ–‡ä»¶
nano deepspeed_single_a800_80g.json
```

**ç¡®ä¿deepspeed_single_a800_80g.jsonå†…å®¹ä¸º**:
```json
{
    "train_batch_size": 16,
    "train_micro_batch_size_per_gpu": 4,
    "gradient_accumulation_steps": 4,
    ...
}
```

**éªŒè¯ä¿®å¤**: `4 Ã— 4 Ã— 1 = 16` âœ…

## 1. ä¿®å¤ configs/config_presets.py

**é—®é¢˜**: SyntaxError: illegal target for annotation

**è§£å†³æ–¹æ¡ˆ**: å®Œå…¨æ›¿æ¢æ–‡ä»¶å†…å®¹ä¸ºï¼š

```python
"""
é…ç½®é¢„è®¾
"""

from .base import ModelConfig

CONFIG_PRESETS = {
    '7b_mamba': ModelConfig(
        # åŸºç¡€å‚æ•°
        vocab_size=50257,
        d_model=4096,
        n_layers=32,
        max_seq_length=1024,
        
        # Mambaç‰¹å®šå‚æ•°
        d_state=16,
        d_conv=4,
        expand=2,
        
        # è®­ç»ƒå‚æ•°
        learning_rate=5e-4,
        batch_size=32,
        train_micro_batch_size_per_gpu=4,
        gradient_accumulation_steps=8,
        weight_decay=0.01,
        warmup_steps=2000,
        max_steps=100000,
        
        # æ­£åˆ™åŒ–
        dropout=0.1,
        
        # ä¼˜åŒ–å™¨
        optimizer_type='adamw',
        adam_beta1=0.9,
        adam_beta2=0.95,
        adam_eps=1e-8,
        
        # ä¿å­˜å’Œæ—¥å¿—
        save_steps=1000,
        eval_steps=500,
        logging_steps=10,
        
        # å…¶ä»–
        fp16=True,
        gradient_checkpointing=True
    ),
    
    '3b_mamba_lite': ModelConfig(
        # åŸºç¡€å‚æ•°ï¼ˆå†…å­˜å‹å¥½ç‰ˆæœ¬ï¼‰
        vocab_size=50257,
        d_model=2560,          # å‡å°‘åˆ°2560
        n_layers=24,           # å‡å°‘åˆ°24å±‚
        max_seq_length=1024,
        
        # Mambaç‰¹å®šå‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        d_state=8,             # ä»16å‡å°‘åˆ°8
        d_conv=4,
        expand=1.5,            # ä»2å‡å°‘åˆ°1.5
        
        # è®­ç»ƒå‚æ•°
        learning_rate=3e-4,
        batch_size=32,
        train_micro_batch_size_per_gpu=1,
        gradient_accumulation_steps=8,
        weight_decay=0.01,
        warmup_steps=1000,
        max_steps=100000,
        
        # æ­£åˆ™åŒ–
        dropout=0.1,
        
        # ä¼˜åŒ–å™¨
        optimizer_type='adamw',
        adam_beta1=0.9,
        adam_beta2=0.95,
        adam_eps=1e-8,
        
        # ä¿å­˜å’Œæ—¥å¿—
        save_steps=1000,
        eval_steps=500,
        logging_steps=10,
        
        # å…¶ä»–
        fp16=True,
        gradient_checkpointing=True
    ),
}
```

## 2. ä¿®å¤ launch_single_a800_80g.sh

**é—®é¢˜**: unrecognized arguments

**è§£å†³æ–¹æ¡ˆ**: å°†å¯åŠ¨å‘½ä»¤æ”¹ä¸ºï¼š

```bash
# åŸæ¥çš„é”™è¯¯å‘½ä»¤
python train_deepspeed.py \
    --preset 7b_mamba \
    --deepspeed_config deepspeed_single_a800_80g.json \
    --max_seq_length 1024 \
    --save_steps 500 \
    --eval_steps 250 \
    --logging_steps 50

# ä¿®æ”¹ä¸ºæ­£ç¡®çš„å‘½ä»¤
deepspeed --num_gpus=1 train_deepspeed.py \
    --preset 7b_mamba \
    --deepspeed_config deepspeed_single_a800_80g.json
```

## 3. åº”ç”¨ä¿®å¤çš„å®Œæ•´æ­¥éª¤

```bash
cd ~/autodl-tmp/AvalonTrainer

# 1. ä¿®å¤æ‰¹æ¬¡å¤§å°é—®é¢˜ (æœ€é‡è¦!)
python fix_deepspeed_batch_size.py --num_gpus 1

# 2. ä¿®å¤é…ç½®æ–‡ä»¶
nano configs/config_presets.py
# å°†ä¸Šé¢çš„Pythonä»£ç å®Œå…¨æ›¿æ¢åŸæ–‡ä»¶å†…å®¹

# 3. ä¿®å¤å¯åŠ¨è„šæœ¬
nano launch_single_a800_80g.sh
# æ‰¾åˆ°æœ€åçš„å¯åŠ¨å‘½ä»¤ï¼ŒæŒ‰ç…§ä¸Šé¢çš„æ–¹å¼ä¿®æ”¹

# 4. è®¾ç½®æƒé™
chmod +x launch_single_a800_80g.sh

# 5. æµ‹è¯•è¯Šæ–­å·¥å…·
python diagnose_a800_80g.py

# 6. å¯åŠ¨è®­ç»ƒ
./launch_single_a800_80g.sh
```

## 4. éªŒè¯ä¿®å¤æˆåŠŸ

ä¿®å¤ååº”è¯¥çœ‹åˆ°ï¼š
- âœ… æ‰¹æ¬¡å¤§å°éªŒè¯é€šè¿‡: `16 = 4 Ã— 4 Ã— 1` 
- âœ… configs/config_presets.py æ— è¯­æ³•é”™è¯¯
- âœ… diagnose_a800_80g.py æ­£å¸¸è¿è¡Œ
- âœ… launch_single_a800_80g.sh æ— å‚æ•°é”™è¯¯
- âœ… è®­ç»ƒæ­£å¸¸å¼€å§‹

## 5. å¿«é€Ÿæµ‹è¯•å‘½ä»¤

```bash
# æµ‹è¯•é…ç½®æ–‡ä»¶
python -c "from configs.config_presets import CONFIG_PRESETS; print('âœ… é…ç½®æ–‡ä»¶æ­£å¸¸')"

# ä¿®å¤æ‰¹æ¬¡å¤§å°
python fix_deepspeed_batch_size.py --num_gpus 1

# æµ‹è¯•A800è¯Šæ–­
python diagnose_a800_80g.py

# ç›´æ¥å¯åŠ¨è®­ç»ƒ
deepspeed --num_gpus=1 train_deepspeed.py --preset 7b_mamba --deepspeed_config deepspeed_single_a800_80g.json
```

## 6. å¦‚æœè¿˜æœ‰é—®é¢˜

å¤‡ç”¨æ–¹æ¡ˆï¼š
```bash
# ä½¿ç”¨æ ‡å‡†1GPUé…ç½®
deepspeed --num_gpus=1 train_deepspeed.py --preset 7b_mamba --deepspeed_config deepspeed_1gpu.json
```

## 7. æ‰‹åŠ¨æ‰¹æ¬¡å¤§å°ä¿®å¤

å¦‚æœä¿®å¤å·¥å…·ä¸èµ·ä½œç”¨ï¼Œæ‰‹åŠ¨ç¼–è¾‘ `deepspeed_single_a800_80g.json`:

```json
{
    "train_batch_size": 16,              // ç¡®ä¿è¿™ä¸ªç­‰äºä¸‹é¢çš„ä¹˜ç§¯
    "train_micro_batch_size_per_gpu": 4, // A800å¯ä»¥æ‰¿å—4
    "gradient_accumulation_steps": 4,    // 4 Ã— 4 Ã— 1 = 16
    ...
}
```

ç°åœ¨æ‚¨çš„A800åº”è¯¥å¯ä»¥æ­£å¸¸å¼€å§‹è®­ç»ƒäº†ï¼ 