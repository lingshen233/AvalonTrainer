#!/usr/bin/env python3
"""
ç»¼åˆæ”¹è¿›ç‰ˆæµ‹è¯•è„šæœ¬
æ”¯æŒDDPæ¨¡å‹åŠ è½½ã€å®Œæ•´åŸºå‡†æµ‹è¯•ã€æ€§èƒ½åˆ†æ
"""

import os
import sys
import argparse
import torch
import time
import json
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def remove_module_prefix(state_dict):
    """ç§»é™¤DDPæ¨¡å‹çš„module.å‰ç¼€"""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('module.'):
            new_state_dict[k[7:]] = v
        else:
            new_state_dict[k] = v
    return new_state_dict

def load_trained_model(checkpoint_path):
    """åŠ è½½è®­ç»ƒçš„æ¨¡å‹ï¼ˆæ”¯æŒDDPï¼‰"""
    from models import create_model
    from configs.base import ModelConfig
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    config_dict = checkpoint['config']
    model_config = ModelConfig(**config_dict)
    
    model = create_model(model_config.model_type, model_config)
    
    # å¤„ç†DDPæ¨¡å‹
    state_dict = checkpoint['model_state_dict']
    has_module_prefix = any(k.startswith('module.') for k in state_dict.keys())
    if has_module_prefix:
        print("ğŸ”„ æ£€æµ‹åˆ°DDPæ¨¡å‹ï¼Œç§»é™¤module.å‰ç¼€...")
        state_dict = remove_module_prefix(state_dict)
    
    model.load_state_dict(state_dict, strict=True)
    model.eval()
    
    return model, model_config

def comprehensive_model_test(model, model_config, device):
    """ç»¼åˆæ¨¡å‹æµ‹è¯•"""
    results = {
        'model_type': model_config.model_type,
        'params': sum(p.numel() for p in model.parameters()),
        'tests': {}
    }
    
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   ç±»å‹: {model_config.model_type}")
    print(f"   å‚æ•°é‡: {results['params']:,} ({results['params']/1e6:.1f}M)")
    
    # 1. åŸºç¡€å‰å‘ä¼ æ’­æµ‹è¯•
    print("\nğŸ§ª åŸºç¡€å‰å‘ä¼ æ’­æµ‹è¯•...")
    try:
        batch_sizes = [1, 2, 4]
        seq_lengths = [128, 256, 512, 1024]
        
        forward_results = {}
        for batch_size in batch_sizes:
            for seq_length in seq_lengths:
                if seq_length > model_config.max_seq_length:
                    continue
                    
                test_input = torch.randint(0, model_config.vocab_size, (batch_size, seq_length)).to(device)
                
                # æµ‹é‡æ—¶é—´å’Œæ˜¾å­˜
                torch.cuda.synchronize()
                start_time = time.time()
                start_memory = torch.cuda.memory_allocated() / 1e9
                
                with torch.no_grad():
                    outputs = model(test_input)
                
                torch.cuda.synchronize()
                end_time = time.time()
                end_memory = torch.cuda.memory_allocated() / 1e9
                
                forward_results[f'batch{batch_size}_seq{seq_length}'] = {
                    'time': end_time - start_time,
                    'memory_delta': end_memory - start_memory,
                    'tokens_per_sec': (batch_size * seq_length) / (end_time - start_time)
                }
                
                print(f"   æ‰¹å¤§å°{batch_size}, åºåˆ—{seq_length}: {forward_results[f'batch{batch_size}_seq{seq_length}']['tokens_per_sec']:.0f} tokens/sec")
        
        results['tests']['forward_pass'] = forward_results
        
    except Exception as e:
        print(f"   âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        results['tests']['forward_pass'] = {'error': str(e)}
    
    # 2. ç”Ÿæˆæµ‹è¯•
    print("\nğŸ“ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•...")
    try:
        generation_tests = []
        test_prompts = [
            "äººå·¥æ™ºèƒ½çš„å‘å±•",
            "The future of technology",
            "æœºå™¨å­¦ä¹ ç®—æ³•",
            "Once upon a time in"
        ]
        
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors='pt').to(device)
            
            torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                # ç”Ÿæˆ10ä¸ªtoken
                current_input = inputs['input_ids']
                for _ in range(10):
                    outputs = model(current_input)
                    
                    # å¤„ç†ä¸åŒè¾“å‡ºæ ¼å¼
                    if isinstance(outputs, dict):
                        if 'logits' in outputs:
                            logits = outputs['logits']
                        else:
                            tensor_outputs = {k: v for k, v in outputs.items() if torch.is_tensor(v)}
                            logits = next(iter(tensor_outputs.values()))
                    elif hasattr(outputs, 'logits'):
                        logits = outputs.logits
                    else:
                        logits = outputs
                    
                    next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
                    current_input = torch.cat([current_input, next_token], dim=1)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            generated = tokenizer.decode(current_input[0], skip_special_tokens=True)
            generated_text = generated[len(prompt):].strip()
            
            generation_tests.append({
                'prompt': prompt,
                'generated': generated_text,
                'time': end_time - start_time,
                'tokens_per_sec': 10 / (end_time - start_time)
            })
            
            print(f"   '{prompt}' -> '{generated_text[:30]}...' ({generation_tests[-1]['tokens_per_sec']:.1f} t/s)")
        
        results['tests']['generation'] = generation_tests
        
    except Exception as e:
        print(f"   âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        results['tests']['generation'] = {'error': str(e)}
    
    # 3. æ˜¾å­˜ä½¿ç”¨åˆ†æ
    print("\nğŸ’¾ æ˜¾å­˜ä½¿ç”¨åˆ†æ...")
    try:
        torch.cuda.empty_cache()
        baseline_memory = torch.cuda.memory_allocated() / 1e9
        
        # æµ‹è¯•ä¸åŒæ‰¹å¤§å°çš„æ˜¾å­˜ä½¿ç”¨
        memory_usage = {}
        for batch_size in [1, 2, 4, 8]:
            try:
                test_input = torch.randint(0, model_config.vocab_size, (batch_size, 512)).to(device)
                
                with torch.no_grad():
                    outputs = model(test_input)
                
                current_memory = torch.cuda.memory_allocated() / 1e9
                memory_usage[f'batch_{batch_size}'] = current_memory - baseline_memory
                
                print(f"   æ‰¹å¤§å°{batch_size}: {memory_usage[f'batch_{batch_size}']:.2f}GB")
                
                # æ¸…ç†
                del test_input, outputs
                torch.cuda.empty_cache()
                
            except torch.cuda.OutOfMemoryError:
                print(f"   æ‰¹å¤§å°{batch_size}: OOM")
                memory_usage[f'batch_{batch_size}'] = None
                break
        
        results['tests']['memory_usage'] = memory_usage
        
    except Exception as e:
        print(f"   âŒ æ˜¾å­˜åˆ†æå¤±è´¥: {e}")
        results['tests']['memory_usage'] = {'error': str(e)}
    
    return results

def benchmark_comparison(model, model_config, device):
    """ä¸åŸºå‡†æ¨¡å‹å¯¹æ¯”"""
    print("\nğŸ“Š åŸºå‡†å¯¹æ¯”æµ‹è¯•...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # é€‰æ‹©åˆé€‚çš„åŸºå‡†æ¨¡å‹
        if model_config.model_type == 'mamba':
            baseline_models = ['distilgpt2', 'gpt2']  # è½»é‡çº§å¯¹æ¯”
        else:
            baseline_models = ['distilgpt2', 'gpt2', 'gpt2-medium']
        
        comparison_results = {}
        
        for baseline_id in baseline_models:
            try:
                print(f"   åŠ è½½åŸºå‡†æ¨¡å‹: {baseline_id}")
                
                tokenizer = AutoTokenizer.from_pretrained(baseline_id)
                baseline_model = AutoModelForCausalLM.from_pretrained(
                    baseline_id,
                    torch_dtype=torch.float16,
                    device_map="auto"
                )
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # ç®€å•æ€§èƒ½å¯¹æ¯”
                test_input = torch.randint(0, min(model_config.vocab_size, tokenizer.vocab_size), (2, 256)).to(device)
                
                # æµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹
                torch.cuda.synchronize()
                start_time = time.time()
                with torch.no_grad():
                    our_outputs = model(test_input)
                torch.cuda.synchronize()
                our_time = time.time() - start_time
                
                # æµ‹è¯•åŸºå‡†æ¨¡å‹
                torch.cuda.synchronize()
                start_time = time.time()
                with torch.no_grad():
                    baseline_outputs = baseline_model(test_input)
                torch.cuda.synchronize()
                baseline_time = time.time() - start_time
                
                comparison_results[baseline_id] = {
                    'our_time': our_time,
                    'baseline_time': baseline_time,
                    'speedup': baseline_time / our_time,
                    'params_baseline': sum(p.numel() for p in baseline_model.parameters())
                }
                
                print(f"   {baseline_id}: é€Ÿåº¦æ¯” {comparison_results[baseline_id]['speedup']:.2f}x")
                
                # æ¸…ç†
                del baseline_model, tokenizer
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"   âŒ {baseline_id} å¯¹æ¯”å¤±è´¥: {e}")
                comparison_results[baseline_id] = {'error': str(e)}
        
        return comparison_results
        
    except Exception as e:
        print(f"   âŒ åŸºå‡†å¯¹æ¯”å¤±è´¥: {e}")
        return {'error': str(e)}

def save_test_report(results, output_file):
    """ä¿å­˜è¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
    
    # JSONæŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    try:
        create_visualization_report(results, output_file.replace('.json', '_visual.png'))
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

def create_visualization_report(results, output_file):
    """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'æ¨¡å‹æ€§èƒ½æµ‹è¯•æŠ¥å‘Š - {results["model_type"]}', fontsize=16)
    
    # 1. å‰å‘ä¼ æ’­æ€§èƒ½
    if 'forward_pass' in results['tests'] and 'error' not in results['tests']['forward_pass']:
        forward_data = results['tests']['forward_pass']
        batch_sizes = []
        tokens_per_sec = []
        
        for key, data in forward_data.items():
            if 'batch1' in key:  # åªæ˜¾ç¤ºbatch_size=1çš„ç»“æœ
                seq_len = int(key.split('_seq')[1])
                batch_sizes.append(seq_len)
                tokens_per_sec.append(data['tokens_per_sec'])
        
        if batch_sizes:
            axes[0, 0].plot(batch_sizes, tokens_per_sec, marker='o')
            axes[0, 0].set_title('å‰å‘ä¼ æ’­æ€§èƒ½')
            axes[0, 0].set_xlabel('åºåˆ—é•¿åº¦')
            axes[0, 0].set_ylabel('Tokens/ç§’')
            axes[0, 0].grid(True)
    
    # 2. æ˜¾å­˜ä½¿ç”¨
    if 'memory_usage' in results['tests'] and 'error' not in results['tests']['memory_usage']:
        memory_data = results['tests']['memory_usage']
        batch_sizes = []
        memory_usage = []
        
        for key, usage in memory_data.items():
            if usage is not None:
                batch_size = int(key.split('_')[1])
                batch_sizes.append(batch_size)
                memory_usage.append(usage)
        
        if batch_sizes:
            axes[0, 1].bar(range(len(batch_sizes)), memory_usage)
            axes[0, 1].set_title('æ˜¾å­˜ä½¿ç”¨')
            axes[0, 1].set_xlabel('æ‰¹å¤§å°')
            axes[0, 1].set_ylabel('æ˜¾å­˜ä½¿ç”¨ (GB)')
            axes[0, 1].set_xticks(range(len(batch_sizes)))
            axes[0, 1].set_xticklabels(batch_sizes)
    
    # 3. ç”Ÿæˆæ€§èƒ½
    if 'generation' in results['tests'] and 'error' not in results['tests']['generation']:
        gen_data = results['tests']['generation']
        prompts = [item['prompt'][:10] + '...' for item in gen_data]
        gen_speeds = [item['tokens_per_sec'] for item in gen_data]
        
        axes[1, 0].bar(range(len(prompts)), gen_speeds)
        axes[1, 0].set_title('æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦')
        axes[1, 0].set_xlabel('æç¤ºè¯')
        axes[1, 0].set_ylabel('Tokens/ç§’')
        axes[1, 0].set_xticks(range(len(prompts)))
        axes[1, 0].set_xticklabels(prompts, rotation=45)
    
    # 4. æ¨¡å‹å‚æ•°åˆ†å¸ƒï¼ˆå¦‚æœæœ‰å¯¹æ¯”æ•°æ®ï¼‰
    if 'benchmark_comparison' in results:
        comp_data = results['benchmark_comparison']
        models = list(comp_data.keys())
        speedups = [comp_data[model].get('speedup', 0) for model in models]
        
        axes[1, 1].bar(range(len(models)), speedups)
        axes[1, 1].set_title('ç›¸å¯¹åŸºå‡†æ¨¡å‹é€Ÿåº¦')
        axes[1, 1].set_xlabel('åŸºå‡†æ¨¡å‹')
        axes[1, 1].set_ylabel('é€Ÿåº¦å€æ•°')
        axes[1, 1].set_xticks(range(len(models)))
        axes[1, 1].set_xticklabels(models, rotation=45)
        axes[1, 1].axhline(y=1, color='r', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="ç»¼åˆæ”¹è¿›ç‰ˆæ¨¡å‹æµ‹è¯•")
    parser.add_argument("--checkpoint", type=str, default="checkpoints/final_model.pt",
                       help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output", type=str, default="test_results",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--benchmark", action="store_true",
                       help="æ‰§è¡ŒåŸºå‡†å¯¹æ¯”æµ‹è¯•")
    parser.add_argument("--quick", action="store_true",
                       help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    print("ğŸš€ ç»¼åˆæ¨¡å‹æµ‹è¯•å¼€å§‹...")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    try:
        model, model_config = load_trained_model(args.checkpoint)
        model = model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # æ‰§è¡Œæµ‹è¯•
    results = {
        'timestamp': time.time(),
        'model_checkpoint': args.checkpoint,
        'device': str(device),
        'model_type': model_config.model_type,
        'params': sum(p.numel() for p in model.parameters())
    }
    
    # ç»¼åˆæµ‹è¯•
    print("\n" + "=" * 60)
    print("å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    test_results = comprehensive_model_test(model, model_config, device)
    results.update(test_results)
    
    # åŸºå‡†å¯¹æ¯”
    if args.benchmark and not args.quick:
        print("\n" + "=" * 60)
        print("åŸºå‡†æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        benchmark_results = benchmark_comparison(model, model_config, device)
        results['benchmark_comparison'] = benchmark_results
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = int(time.time())
    output_file = output_dir / f'comprehensive_test_{timestamp}.json'
    save_test_report(results, str(output_file))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    print(f"âœ… æµ‹è¯•å®Œæˆ")
    print(f"   æ¨¡å‹ç±»å‹: {results['model_type']}")
    print(f"   å‚æ•°é‡: {results['params']:,} ({results['params']/1e6:.1f}M)")
    print(f"   è¯¦ç»†æŠ¥å‘Š: {output_file}")
    
    if 'tests' in results:
        if 'forward_pass' in results['tests'] and 'error' not in results['tests']['forward_pass']:
            # æ˜¾ç¤ºæœ€ä½³æ€§èƒ½
            forward_data = results['tests']['forward_pass']
            best_perf = max(data['tokens_per_sec'] for data in forward_data.values() if isinstance(data, dict))
            print(f"   æœ€ä½³ååé‡: {best_perf:.0f} tokens/sec")
        
        if 'generation' in results['tests'] and 'error' not in results['tests']['generation']:
            gen_data = results['tests']['generation']
            avg_speed = np.mean([item['tokens_per_sec'] for item in gen_data])
            print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {avg_speed:.1f} tokens/sec")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"   1. æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: cat {output_file}")
    print(f"   2. å¦‚æœæ€§èƒ½æ»¡æ„ï¼Œå¯å°è¯•æ›´å¤§æ¨¡å‹")
    print(f"   3. è¿è¡ŒåŸºå‡†æµ‹è¯•: --benchmark")
    
    return 0

if __name__ == "__main__":
    exit(main()) 