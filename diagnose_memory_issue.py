#!/usr/bin/env python3
"""
GPUæ˜¾å­˜é—®é¢˜è¯Šæ–­è„šæœ¬
åˆ†ææ˜¾å­˜ä½¿ç”¨æƒ…å†µå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import torch
import json
import os
import argparse

def check_gpu_memory():
    """æ£€æŸ¥GPUæ˜¾å­˜çŠ¶æ€"""
    print("ğŸ” GPUæ˜¾å­˜æ£€æŸ¥")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return
    
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ¯ æ£€æµ‹åˆ° {gpu_count} å¼ GPU")
    
    total_memory = 0
    available_memory = 0
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        total = props.total_memory / (1024**3)  # GB
        
        torch.cuda.set_device(i)
        torch.cuda.empty_cache()
        
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        free = total - allocated
        
        total_memory += total
        available_memory += free
        
        print(f"GPU {i}: {props.name}")
        print(f"  æ€»æ˜¾å­˜: {total:.1f}GB")
        print(f"  å·²åˆ†é…: {allocated:.1f}GB")
        print(f"  å·²ä¿ç•™: {reserved:.1f}GB") 
        print(f"  å¯ç”¨: {free:.1f}GB")
        print()
    
    print(f"ğŸ“Š æ€»è®¡:")
    print(f"  æ€»æ˜¾å­˜: {total_memory:.1f}GB")
    print(f"  å¯ç”¨æ˜¾å­˜: {available_memory:.1f}GB")
    
    return gpu_count, total_memory, available_memory

def estimate_model_memory(model_size_b=7):
    """ä¼°ç®—æ¨¡å‹æ˜¾å­˜éœ€æ±‚"""
    print(f"\nğŸ§® ä¼°ç®— {model_size_b}B å‚æ•°æ¨¡å‹æ˜¾å­˜éœ€æ±‚")
    print("=" * 50)
    
    # åŸºæœ¬è®¡ç®—ï¼ˆFP16ï¼‰
    model_params = model_size_b * 1e9
    model_memory_fp16 = model_params * 2 / (1024**3)  # 2 bytes per param
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamW: momentum + varianceï¼‰
    optimizer_memory = model_params * 2 * 4 / (1024**3)  # 8 bytes per param (FP32)
    
    # æ¢¯åº¦
    gradient_memory = model_params * 2 / (1024**3)  # FP16 gradients
    
    # æ¿€æ´»å€¼ï¼ˆä¼°ç®—ï¼‰
    activation_memory_per_batch = 2.0  # GB per batch
    
    print(f"æ¨¡å‹å‚æ•° (FP16): {model_memory_fp16:.1f}GB")
    print(f"ä¼˜åŒ–å™¨çŠ¶æ€ (FP32): {optimizer_memory:.1f}GB") 
    print(f"æ¢¯åº¦ (FP16): {gradient_memory:.1f}GB")
    print(f"æ¿€æ´»å€¼ (æ¯batch): {activation_memory_per_batch:.1f}GB")
    
    total_per_gpu_no_zero = model_memory_fp16 + optimizer_memory + gradient_memory
    print(f"\nğŸš« æ— ZeROä¼˜åŒ– - æ¯GPUéœ€æ±‚: {total_per_gpu_no_zero:.1f}GB")
    
    # ZeRO-2ä¼˜åŒ–
    zero2_optimizer = optimizer_memory  # ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡
    zero2_per_gpu = model_memory_fp16 + gradient_memory + zero2_optimizer / 2
    print(f"ğŸ”„ ZeRO-2ä¼˜åŒ– - æ¯GPUéœ€æ±‚: {zero2_per_gpu:.1f}GB")
    
    # ZeRO-3ä¼˜åŒ–
    zero3_per_gpu = model_memory_fp16 / 4 + gradient_memory / 4  # å‚æ•°å’Œæ¢¯åº¦åˆ†ç‰‡
    print(f"âš¡ ZeRO-3ä¼˜åŒ– - æ¯GPUéœ€æ±‚: {zero3_per_gpu:.1f}GB")
    
    return {
        'model_fp16': model_memory_fp16,
        'optimizer': optimizer_memory, 
        'gradient': gradient_memory,
        'activation_per_batch': activation_memory_per_batch,
        'no_zero': total_per_gpu_no_zero,
        'zero2': zero2_per_gpu,
        'zero3': zero3_per_gpu
    }

def analyze_config(config_file):
    """åˆ†æDeepSpeedé…ç½®"""
    print(f"\nğŸ“‹ åˆ†æé…ç½®æ–‡ä»¶: {config_file}")
    print("=" * 50)
    
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return None
    
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    # åŸºæœ¬é…ç½®
    train_batch = config.get('train_batch_size', 'NOT SET')
    micro_batch = config.get('train_micro_batch_size_per_gpu', 'NOT SET')
    grad_acc = config.get('gradient_accumulation_steps', 'NOT SET')
    
    print(f"æ‰¹æ¬¡é…ç½®:")
    print(f"  train_batch_size: {train_batch}")
    print(f"  micro_batch_per_gpu: {micro_batch}")
    print(f"  gradient_accumulation_steps: {grad_acc}")
    
    # ZeROé…ç½®
    zero_config = config.get('zero_optimization', {})
    zero_stage = zero_config.get('stage', 'NOT SET')
    offload_optimizer = zero_config.get('offload_optimizer', {}).get('device', 'none')
    offload_param = zero_config.get('offload_param', {}).get('device', 'none')
    
    print(f"\nZeROä¼˜åŒ–:")
    print(f"  é˜¶æ®µ: {zero_stage}")
    print(f"  ä¼˜åŒ–å™¨å¸è½½: {offload_optimizer}")
    print(f"  å‚æ•°å¸è½½: {offload_param}")
    
    # æ¿€æ´»æ£€æŸ¥ç‚¹
    activation_checkpointing = config.get('activation_checkpointing', {})
    cpu_checkpointing = activation_checkpointing.get('cpu_checkpointing', False)
    num_checkpoints = activation_checkpointing.get('number_checkpoints', 'NOT SET')
    
    print(f"\næ¿€æ´»æ£€æŸ¥ç‚¹:")
    print(f"  CPUæ£€æŸ¥ç‚¹: {cpu_checkpointing}")
    print(f"  æ£€æŸ¥ç‚¹æ•°é‡: {num_checkpoints}")
    
    return {
        'train_batch_size': train_batch,
        'micro_batch_per_gpu': micro_batch,
        'gradient_accumulation_steps': grad_acc,
        'zero_stage': zero_stage,
        'offload_optimizer': offload_optimizer,
        'offload_param': offload_param,
        'cpu_checkpointing': cpu_checkpointing
    }

def provide_recommendations(gpu_count, available_memory, memory_estimates, config_analysis):
    """æä¾›ä¼˜åŒ–å»ºè®®"""
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®")
    print("=" * 50)
    
    if config_analysis is None:
        print("âŒ æ— æ³•åˆ†æé…ç½®ï¼Œè·³è¿‡å»ºè®®")
        return
    
    # æ£€æŸ¥micro_batch_size
    micro_batch = config_analysis.get('micro_batch_per_gpu', 0)
    if isinstance(micro_batch, int) and micro_batch > 1:
        print(f"âš ï¸ micro_batch_per_gpu = {micro_batch} å¤ªå¤§")
        print(f"   å»ºè®®: è®¾ç½®ä¸º 1")
    
    # æ£€æŸ¥ZeROé˜¶æ®µ
    zero_stage = config_analysis.get('zero_stage')
    if zero_stage != 3 and gpu_count >= 4:
        print(f"âš ï¸ å½“å‰ZeROé˜¶æ®µ: {zero_stage}")
        print(f"   å»ºè®®: å¯¹äº {gpu_count} å¼ GPUï¼Œä½¿ç”¨ZeRO-3")
    
    # æ£€æŸ¥CPUå¸è½½
    offload_optimizer = config_analysis.get('offload_optimizer', 'none')
    offload_param = config_analysis.get('offload_param', 'none')
    
    if offload_optimizer == 'none' or offload_param == 'none':
        print(f"âš ï¸ CPUå¸è½½æœªå¯ç”¨")
        print(f"   ä¼˜åŒ–å™¨å¸è½½: {offload_optimizer}")
        print(f"   å‚æ•°å¸è½½: {offload_param}")
        print(f"   å»ºè®®: å¯ç”¨CPUå¸è½½ä»¥èŠ‚çœGPUæ˜¾å­˜")
    
    # æ£€æŸ¥æ¿€æ´»æ£€æŸ¥ç‚¹
    cpu_checkpointing = config_analysis.get('cpu_checkpointing', False)
    if not cpu_checkpointing:
        print(f"âš ï¸ CPUæ¿€æ´»æ£€æŸ¥ç‚¹æœªå¯ç”¨")
        print(f"   å»ºè®®: å¯ç”¨CPUæ¿€æ´»æ£€æŸ¥ç‚¹")
    
    # è®¡ç®—æ¨èé…ç½®
    available_per_gpu = available_memory / gpu_count
    recommended_zero_stage = 3 if available_per_gpu < 10 else 2
    
    print(f"\nğŸ“‹ æ¨èé…ç½®:")
    print(f"  ZeROé˜¶æ®µ: {recommended_zero_stage}")
    print(f"  micro_batch_per_gpu: 1")
    print(f"  å¯ç”¨CPUå¸è½½: æ˜¯")
    print(f"  å¯ç”¨CPUæ¿€æ´»æ£€æŸ¥ç‚¹: æ˜¯")
    
    # ç”Ÿæˆå…·ä½“çš„é…ç½®æ–‡ä»¶å»ºè®®
    if available_per_gpu < 8:
        print(f"\nğŸš¨ æ˜¾å­˜ä¸¥é‡ä¸è¶³ (æ¯GPUå¯ç”¨: {available_per_gpu:.1f}GB)")
        print(f"   å»ºè®®ä½¿ç”¨: deepspeed_extreme_memory_safe.json")
    elif available_per_gpu < 12:
        print(f"\nâš ï¸ æ˜¾å­˜ç´§å¼  (æ¯GPUå¯ç”¨: {available_per_gpu:.1f}GB)")
        print(f"   å»ºè®®ä½¿ç”¨: deepspeed_{gpu_count}gpu.json (å¦‚æœå­˜åœ¨)")

def main():
    parser = argparse.ArgumentParser(description="GPUæ˜¾å­˜é—®é¢˜è¯Šæ–­")
    parser.add_argument("--config", type=str, help="è¦åˆ†æçš„é…ç½®æ–‡ä»¶")
    parser.add_argument("--model_size", type=int, default=7, help="æ¨¡å‹å¤§å°(B)")
    parser.add_argument("--fix", action="store_true", help="è‡ªåŠ¨ç”Ÿæˆä¿®å¤é…ç½®")
    
    args = parser.parse_args()
    
    print("ğŸ” GPUæ˜¾å­˜é—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # 1. æ£€æŸ¥GPUæ˜¾å­˜
    gpu_count, total_memory, available_memory = check_gpu_memory()
    
    # 2. ä¼°ç®—æ¨¡å‹æ˜¾å­˜éœ€æ±‚
    memory_estimates = estimate_model_memory(args.model_size)
    
    # 3. åˆ†æé…ç½®æ–‡ä»¶
    config_analysis = None
    if args.config:
        config_analysis = analyze_config(args.config)
    
    # 4. æä¾›å»ºè®®
    provide_recommendations(gpu_count, available_memory, memory_estimates, config_analysis)
    
    # 5. è‡ªåŠ¨ä¿®å¤
    if args.fix:
        print(f"\nğŸ”§ è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–é…ç½®...")
        os.system(f"python fix_deepspeed_batch_size.py --num_gpus {gpu_count} --generate")

if __name__ == "__main__":
    main() 