#!/usr/bin/env python3
"""
ä¿®å¤FP16æ··åˆç²¾åº¦è®­ç»ƒä¸­çš„æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜
"""

import torch
import torch.nn as nn
import os
import argparse

def fix_model_dtype_consistency(model):
    """ä¿®å¤æ¨¡å‹ä¸­çš„æ•°æ®ç±»å‹ä¸€è‡´æ€§é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§...")
    
    def ensure_dtype_consistency(module):
        """ç¡®ä¿æ¨¡å—å†…çš„æ•°æ®ç±»å‹ä¸€è‡´æ€§"""
        if hasattr(module, 'weight') and module.weight is not None:
            target_dtype = module.weight.dtype
            
            # ä¿®å¤biasçš„æ•°æ®ç±»å‹
            if hasattr(module, 'bias') and module.bias is not None:
                if module.bias.dtype != target_dtype:
                    module.bias.data = module.bias.data.to(target_dtype)
            
            # ä¿®å¤å…¶ä»–å‚æ•°çš„æ•°æ®ç±»å‹
            for name, param in module.named_parameters(recurse=False):
                if param is not None and param.dtype != target_dtype:
                    param.data = param.data.to(target_dtype)
    
    # å¯¹æ‰€æœ‰æ¨¡å—åº”ç”¨æ•°æ®ç±»å‹ä¿®å¤
    for module in model.modules():
        ensure_dtype_consistency(module)
    
    print("âœ… æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§ä¿®å¤å®Œæˆ")

class DTypeFriendlyLinear(nn.Linear):
    """æ”¯æŒæ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢çš„Linearå±‚"""
    
    def forward(self, input):
        # ç¡®ä¿inputå’Œweightçš„æ•°æ®ç±»å‹ä¸€è‡´
        if input.dtype != self.weight.dtype:
            input = input.to(self.weight.dtype)
        return super().forward(input)

class DTypeFriendlyEmbedding(nn.Embedding):
    """æ”¯æŒæ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢çš„Embeddingå±‚"""
    
    def forward(self, input):
        # Embeddingçš„è¾“å…¥é€šå¸¸æ˜¯longç±»å‹ï¼Œè¾“å‡ºéœ€è¦åŒ¹é…æƒé‡ç±»å‹
        result = super().forward(input)
        return result.to(self.weight.dtype)

def replace_modules_with_dtype_friendly(model):
    """æ›¿æ¢æ¨¡å‹ä¸­çš„æ¨¡å—ä¸ºæ•°æ®ç±»å‹å‹å¥½ç‰ˆæœ¬"""
    print("ğŸ”„ æ›¿æ¢æ¨¡å—ä¸ºæ•°æ®ç±»å‹å‹å¥½ç‰ˆæœ¬...")
    
    def replace_linear_modules(module, parent_name=""):
        for name, child in module.named_children():
            full_name = f"{parent_name}.{name}" if parent_name else name
            
            if isinstance(child, nn.Linear):
                # æ›¿æ¢ä¸ºæ•°æ®ç±»å‹å‹å¥½çš„Linear
                new_linear = DTypeFriendlyLinear(
                    child.in_features, 
                    child.out_features, 
                    bias=child.bias is not None
                )
                new_linear.weight.data = child.weight.data.clone()
                if child.bias is not None:
                    new_linear.bias.data = child.bias.data.clone()
                
                setattr(module, name, new_linear)
                print(f"  æ›¿æ¢: {full_name} -> DTypeFriendlyLinear")
                
            elif isinstance(child, nn.Embedding):
                # æ›¿æ¢ä¸ºæ•°æ®ç±»å‹å‹å¥½çš„Embedding
                new_embedding = DTypeFriendlyEmbedding(
                    child.num_embeddings,
                    child.embedding_dim,
                    padding_idx=child.padding_idx,
                    max_norm=child.max_norm,
                    norm_type=child.norm_type,
                    scale_grad_by_freq=child.scale_grad_by_freq,
                    sparse=child.sparse
                )
                new_embedding.weight.data = child.weight.data.clone()
                
                setattr(module, name, new_embedding)
                print(f"  æ›¿æ¢: {full_name} -> DTypeFriendlyEmbedding")
            else:
                # é€’å½’å¤„ç†å­æ¨¡å—
                replace_linear_modules(child, full_name)
    
    replace_linear_modules(model)
    print("âœ… æ¨¡å—æ›¿æ¢å®Œæˆ")

def add_dtype_hooks(model):
    """æ·»åŠ æ•°æ®ç±»å‹æ£€æŸ¥hook"""
    print("ğŸ”— æ·»åŠ æ•°æ®ç±»å‹æ£€æŸ¥hook...")
    
    def dtype_check_hook(module, input, output):
        """æ£€æŸ¥è¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹ä¸€è‡´æ€§"""
        if hasattr(module, 'weight') and module.weight is not None:
            target_dtype = module.weight.dtype
            
            # æ£€æŸ¥è¾“å…¥
            if isinstance(input, (tuple, list)):
                for i, inp in enumerate(input):
                    if torch.is_tensor(inp) and inp.dtype != target_dtype and inp.dtype.is_floating_point:
                        print(f"âš ï¸ {module.__class__.__name__}: è¾“å…¥{i} dtypeä¸åŒ¹é… {inp.dtype} != {target_dtype}")
            
            # æ£€æŸ¥è¾“å‡º
            if torch.is_tensor(output) and output.dtype != target_dtype:
                print(f"âš ï¸ {module.__class__.__name__}: è¾“å‡º dtypeä¸åŒ¹é… {output.dtype} != {target_dtype}")
    
    # ä¸ºç‰¹å®šæ¨¡å—ç±»å‹æ·»åŠ hook
    hook_modules = [nn.Linear, nn.Embedding, nn.Conv1d]
    
    for module in model.modules():
        if any(isinstance(module, cls) for cls in hook_modules):
            module.register_forward_hook(dtype_check_hook)
    
    print("âœ… æ•°æ®ç±»å‹æ£€æŸ¥hookæ·»åŠ å®Œæˆ")

def create_fp16_safe_config():
    """åˆ›å»ºFP16å®‰å…¨çš„DeepSpeedé…ç½®"""
    print("ğŸ“ åˆ›å»ºFP16å®‰å…¨é…ç½®...")
    
    config = {
        "train_batch_size": 48,
        "train_micro_batch_size_per_gpu": 1,
        "gradient_accumulation_steps": 8,
        
        "zero_optimization": {
            "stage": 3,
            "offload_optimizer": {
                "device": "cpu",
                "pin_memory": True,
                "buffer_count": 4,
                "fast_init": False
            },
            "offload_param": {
                "device": "cpu",
                "pin_memory": True,
                "buffer_count": 5,
                "buffer_size": 1e8,
                "max_in_cpu": 1e9
            },
            "overlap_comm": True,
            "contiguous_gradients": True,
            "reduce_bucket_size": 5e7,
            "stage3_prefetch_bucket_size": 5e7,
            "stage3_param_persistence_threshold": 1e6,
            "sub_group_size": 1e9,
            "stage3_max_live_parameters": 1e9,
            "stage3_max_reuse_distance": 1e9,
            "stage3_gather_16bit_weights_on_model_save": True
        },
        
        # æ›´å®‰å…¨çš„FP16é…ç½®
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "initial_scale_power": 12,  # é™ä½åˆå§‹loss scale
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        
        # æ›´ä¿å®ˆçš„ä¼˜åŒ–å™¨è®¾ç½®
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": 3e-5,  # é™ä½å­¦ä¹ ç‡
                "betas": [0.9, 0.95],
                "eps": 1e-8,
                "weight_decay": 0.01
            }
        },
        
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": 3e-5,
                "warmup_num_steps": 2000  # å¢åŠ warmupæ­¥æ•°
            }
        },
        
        "gradient_clipping": 1.0,
        "steps_per_print": 10,
        "wall_clock_breakdown": False,
        
        # æ›´æ¿€è¿›çš„æ¿€æ´»æ£€æŸ¥ç‚¹
        "activation_checkpointing": {
            "partition_activations": True,
            "cpu_checkpointing": True,
            "contiguous_memory_optimization": True,
            "number_checkpoints": 16,
            "synchronize_checkpoint_boundary": True,
            "profile": False
        },
        
        "comms_logger": {"enabled": False},
        "memory_breakdown": False,
        "flops_profiler": {"enabled": False}
    }
    
    # ä¿å­˜é…ç½®
    import json
    config_file = "deepspeed_6gpu_fp16_safe.json"
    with open(config_file, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"âœ… FP16å®‰å…¨é…ç½®å·²ä¿å­˜: {config_file}")
    return config_file

def main():
    parser = argparse.ArgumentParser(description="ä¿®å¤FP16æ•°æ®ç±»å‹ä¸åŒ¹é…é—®é¢˜")
    parser.add_argument("--create_safe_config", action="store_true", help="åˆ›å»ºFP16å®‰å…¨é…ç½®")
    parser.add_argument("--test_model", action="store_true", help="æµ‹è¯•æ¨¡å‹æ•°æ®ç±»å‹")
    
    args = parser.parse_args()
    
    print("ğŸ”§ FP16æ•°æ®ç±»å‹ä¸åŒ¹é…ä¿®å¤å·¥å…·")
    print("=" * 50)
    
    if args.create_safe_config:
        create_fp16_safe_config()
    
    if args.test_model:
        print("\nğŸ§ª æµ‹è¯•æ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´æ€§...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹æµ‹è¯•ä»£ç 
        
    print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
    print("1. ä½¿ç”¨æ›´å®‰å…¨çš„FP16é…ç½® (é™ä½initial_scale_power)")
    print("2. åœ¨æ¨¡å‹ä¸­æ·»åŠ æ˜¾å¼çš„æ•°æ®ç±»å‹è½¬æ¢")
    print("3. ä½¿ç”¨æ•°æ®ç±»å‹å‹å¥½çš„æ¨¡å—æ›¿æ¢")
    print("4. æ·»åŠ æ•°æ®ç±»å‹æ£€æŸ¥hookè¿›è¡Œè°ƒè¯•")
    
    print("\nğŸš€ ä½¿ç”¨æ–¹æ³•:")
    print("# åˆ›å»ºå®‰å…¨é…ç½®")
    print("python fix_dtype_mismatch.py --create_safe_config")
    print("")
    print("# ä½¿ç”¨å®‰å…¨é…ç½®è®­ç»ƒ")
    print("deepspeed --num_gpus=6 train_deepspeed.py --preset 7b_mamba --deepspeed_config deepspeed_6gpu_fp16_safe.json")

if __name__ == "__main__":
    main() 