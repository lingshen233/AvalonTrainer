# 4GPU Transformer训练配置
model_type: "transformer"
num_gpus: 4

# 模型配置
model:
  vocab_size: 50257
  max_seq_length: 2048
  d_model: 1536
  n_layers: 24
  dropout: 0.1
  n_heads: 16
  d_ff: 6144
  
  # Mamba参数（未使用）
  d_state: 16
  d_conv: 4
  expand: 2

# 训练配置
training:
  dataset: "wikitext"
  batch_size: 8          # 每个GPU的批大小
  gradient_accumulation_steps: 2
  max_length: 2048
  
  learning_rate: 5e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  max_steps: 100000
  warmup_steps: 5000
  
  eval_steps: 1000
  save_steps: 5000
  logging_steps: 100
  
  fp16: true
  
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  
  use_wandb: true
  wandb_project: "transformer-4gpu"
  run_name: null

# GPU配置
gpu:
  auto_batch_size: false
  memory_fraction: 0.9

# 系统配置
system:
  auto_shutdown: false     # 训练完成后自动关机
  shutdown_delay: 60       # 关机前等待时间（秒） 