#!/usr/bin/env python3
"""
7B Mambaæ¨¡å‹å†…å­˜è¯Šæ–­å·¥å…·
ä¸“é—¨åˆ†æ6Ã—32GB GPUç¯å¢ƒä¸‹çš„å†…å­˜ä½¿ç”¨
"""

import torch
import psutil
import subprocess
import json
from configs.config_presets import CONFIG_PRESETS

def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')
        
        gpus = []
        for line in lines:
            if line.strip():
                index, name, used, total = line.split(', ')
                gpus.append({
                    'index': int(index),
                    'name': name,
                    'used_mb': int(used),
                    'total_mb': int(total),
                    'free_mb': int(total) - int(used),
                    'usage_pct': (int(used) / int(total)) * 100
                })
        return gpus
    except Exception as e:
        print(f"âŒ æ— æ³•è·å–GPUä¿¡æ¯: {e}")
        return []

def estimate_model_memory(config):
    """ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚"""
    # 7Bæ¨¡å‹å‚æ•°é‡
    param_count = 7e9
    
    # FP16æ¯ä¸ªå‚æ•°2å­—èŠ‚
    model_memory_mb = (param_count * 2) / (1024**2)
    
    # æ¢¯åº¦å†…å­˜ï¼ˆFP16ï¼‰
    gradient_memory_mb = model_memory_mb
    
    # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamWéœ€è¦2å€å‚æ•°ï¼‰
    optimizer_memory_mb = model_memory_mb * 2
    
    # æ¿€æ´»å€¼å†…å­˜ï¼ˆä¼°ç®—ï¼‰
    batch_size = config.get('train_micro_batch_size_per_gpu', 1)
    seq_length = config.get('max_seq_length', 1024)
    d_model = 4096
    n_layers = 32
    
    # ä¼°ç®—æ¿€æ´»å€¼å†…å­˜
    activation_memory_mb = (batch_size * seq_length * d_model * n_layers * 2) / (1024**2)
    
    return {
        'model_mb': model_memory_mb,
        'gradient_mb': gradient_memory_mb,
        'optimizer_mb': optimizer_memory_mb,
        'activation_mb': activation_memory_mb,
        'total_mb': model_memory_mb + gradient_memory_mb + optimizer_memory_mb + activation_memory_mb
    }

def analyze_deepspeed_config(config_file):
    """åˆ†æDeepSpeedé…ç½®çš„å†…å­˜ä¼˜åŒ–ç¨‹åº¦"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        score = 0
        suggestions = []
        
        # æ£€æŸ¥ZeRO stage
        zero_stage = config.get('zero_optimization', {}).get('stage', 0)
        if zero_stage == 3:
            score += 30
        elif zero_stage == 2:
            score += 20
            suggestions.append("å»ºè®®ä½¿ç”¨ZeRO-3ä»¥è·å¾—æ›´å¥½çš„å†…å­˜èŠ‚çœ")
        else:
            suggestions.append("å¼ºçƒˆå»ºè®®å¯ç”¨ZeRO-3")
        
        # æ£€æŸ¥CPUå¸è½½
        if config.get('zero_optimization', {}).get('offload_optimizer'):
            score += 25
        else:
            suggestions.append("å»ºè®®å¯ç”¨ä¼˜åŒ–å™¨CPUå¸è½½")
            
        if config.get('zero_optimization', {}).get('offload_param'):
            score += 25
        else:
            suggestions.append("å»ºè®®å¯ç”¨å‚æ•°CPUå¸è½½")
        
        # æ£€æŸ¥æ¿€æ´»æ£€æŸ¥ç‚¹
        if config.get('activation_checkpointing', {}).get('partition_activations'):
            score += 15
        else:
            suggestions.append("å»ºè®®å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹")
        
        # æ£€æŸ¥batch size
        micro_batch = config.get('train_micro_batch_size_per_gpu', 4)
        if micro_batch <= 1:
            score += 5
        else:
            suggestions.append(f"micro_batch_sizeè¿‡å¤§({micro_batch})ï¼Œå»ºè®®è®¾ä¸º1")
        
        return score, suggestions
        
    except Exception as e:
        return 0, [f"æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}"]

def get_optimization_recommendations():
    """è·å–ä¼˜åŒ–å»ºè®®"""
    return [
        "ğŸ”§ ç«‹å³å¯è¡Œçš„ä¼˜åŒ–:",
        "  1. ä½¿ç”¨æç«¯é…ç½®: deepspeed_6gpu_extreme.json",
        "  2. å‡å°‘åºåˆ—é•¿åº¦: --max_seq_length 512",
        "  3. è®¾ç½®å†…å­˜ç¯å¢ƒå˜é‡: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
        "",
        "ğŸ”¬ æ¨¡å‹çº§ä¼˜åŒ–:",
        "  1. å·²å®ç°ï¼šåˆ†å—å¼selective_scanè®¡ç®—",
        "  2. å‡å°‘d_stateå‚æ•°ï¼ˆä»16åˆ°8ï¼‰",
        "  3. å‡å°‘expandå‚æ•°ï¼ˆä»2åˆ°1.5ï¼‰",
        "",
        "âš¡ ç¡¬ä»¶çº§ä¼˜åŒ–:",
        "  1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆ3Bæˆ–1Bï¼‰",
        "  2. è€ƒè™‘ä½¿ç”¨8bité‡åŒ–",
        "  3. å¢åŠ ç³»ç»Ÿå†…å­˜ç”¨äºCPUå¸è½½",
        "",
        "ğŸ¯ ç»ˆææ–¹æ¡ˆ:",
        "  å¦‚æœä»ç„¶OOMï¼Œå»ºè®®ï¼š",
        "  - ä½¿ç”¨3Bå‚æ•°æ¨¡å‹",
        "  - æˆ–è€…å‡å°‘åˆ°4å¼ GPUè®­ç»ƒ",
        "  - æˆ–è€…ä½¿ç”¨gradient checkpointing + CPUå¸è½½"
    ]

def main():
    print("ğŸ” 7B Mambaæ¨¡å‹å†…å­˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # 1. ç³»ç»Ÿä¿¡æ¯
    print("\nğŸ“± ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUå†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024**3):.1f} GB")
    
    # 2. GPUä¿¡æ¯
    print("\nğŸ® GPUä¿¡æ¯:")
    gpus = get_gpu_memory_info()
    total_gpu_memory = 0
    for gpu in gpus:
        print(f"  GPU {gpu['index']}: {gpu['name']}")
        print(f"    æ€»å†…å­˜: {gpu['total_mb']/1024:.1f} GB")
        print(f"    å·²ä½¿ç”¨: {gpu['used_mb']/1024:.1f} GB ({gpu['usage_pct']:.1f}%)")
        print(f"    å¯ç”¨: {gpu['free_mb']/1024:.1f} GB")
        total_gpu_memory += gpu['total_mb']
    
    print(f"\n  æ€»GPUå†…å­˜: {total_gpu_memory/1024:.1f} GB")
    
    # 3. æ¨¡å‹å†…å­˜ä¼°ç®—
    print("\nğŸ§  7B Mambaæ¨¡å‹å†…å­˜ä¼°ç®—:")
    config = CONFIG_PRESETS['7b_mamba']
    memory_est = estimate_model_memory(config)
    
    print(f"  æ¨¡å‹å‚æ•°: {memory_est['model_mb']/1024:.1f} GB")
    print(f"  æ¢¯åº¦: {memory_est['gradient_mb']/1024:.1f} GB")
    print(f"  ä¼˜åŒ–å™¨çŠ¶æ€: {memory_est['optimizer_mb']/1024:.1f} GB")
    print(f"  æ¿€æ´»å€¼: {memory_est['activation_mb']/1024:.1f} GB")
    print(f"  æ€»è®¡: {memory_est['total_mb']/1024:.1f} GB")
    
    # 4. é…ç½®åˆ†æ
    config_files = [
        'deepspeed_6gpu.json',
        'deepspeed_6gpu_extreme.json',
        'deepspeed_6gpu_fp16_safe.json'
    ]
    
    print("\nâš™ï¸ é…ç½®æ–‡ä»¶åˆ†æ:")
    for config_file in config_files:
        try:
            score, suggestions = analyze_deepspeed_config(config_file)
            print(f"\n  {config_file}:")
            print(f"    å†…å­˜ä¼˜åŒ–è¯„åˆ†: {score}/100")
            if suggestions:
                for suggestion in suggestions[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå»ºè®®
                    print(f"    - {suggestion}")
        except:
            print(f"  {config_file}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # 5. é—®é¢˜è¯Šæ–­
    print("\nğŸ” é—®é¢˜è¯Šæ–­:")
    single_gpu_memory = total_gpu_memory / len(gpus) / 1024  # GB
    required_memory = memory_est['total_mb'] / 1024  # GB
    
    if required_memory > single_gpu_memory * 6:  # å³ä½¿åˆ†å¸ƒå¼ä¹Ÿä¸å¤Ÿ
        print("  âŒ ä¸¥é‡é—®é¢˜ï¼šå³ä½¿ä½¿ç”¨6GPU + ZeRO-3ä¹Ÿå¯èƒ½å†…å­˜ä¸è¶³")
        print(f"     éœ€è¦: {required_memory:.1f} GB, å¯ç”¨: {single_gpu_memory*6:.1f} GB")
    elif required_memory > single_gpu_memory:
        print("  âš ï¸  éœ€è¦åˆ†å¸ƒå¼è®­ç»ƒå’Œå†…å­˜ä¼˜åŒ–")
        print(f"     å•GPUä¸è¶³ï¼Œéœ€è¦ZeRO-3 + CPUå¸è½½")
    else:
        print("  âœ… å•GPUç†è®ºä¸Šè¶³å¤Ÿï¼Œé—®é¢˜å¯èƒ½åœ¨å®ç°ç»†èŠ‚")
    
    # 6. ä¼˜åŒ–å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    for recommendation in get_optimization_recommendations():
        print(recommendation)
    
    # 7. å¿«é€Ÿå¯åŠ¨å‘½ä»¤
    print("\nğŸš€ æ¨èå¯åŠ¨å‘½ä»¤:")
    print("./launch_6gpu_extreme_safe.sh")
    print("")
    print("æˆ–æ‰‹åŠ¨:")
    print("export PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True,roundup_power2_divisions:16'")
    print("deepspeed --num_gpus=6 train_deepspeed.py --preset 7b_mamba --deepspeed_config deepspeed_6gpu_extreme.json --max_seq_length 512")

if __name__ == "__main__":
    main() 