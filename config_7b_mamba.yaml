# 真正的7B Mamba模型配置 - 6.89B参数
# ⚠️ 需要40GB显存/GPU或8张24GB GPU

# 基础设置
model_type: "mamba"
num_gpus: 8  # 建议8张24GB GPU分摊显存

# 模型配置 - 真正的7B参数 (6.89B)
model:
  vocab_size: 50257
  max_seq_length: 4096      # 7B模型应有的序列长度
  d_model: 4864            # 真正的7B隐层维度
  n_layers: 45              # 45层达到6.89B
  d_state: 16               # Mamba状态维度
  d_conv: 4                 # 卷积核大小
  expand: 2                 # 扩展因子
  dropout: 0.1

# 训练配置 - 针对大模型优化
training:
  dataset: "auto"
  batch_size: 1                    # 每GPU最小批大小
  gradient_accumulation_steps: 32  # 保持有效批大小
  eval_batch_size: 1
  max_length: 4096                 # 完整序列长度
  learning_rate: 1e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  max_steps: 200000
  warmup_steps: 10000
  eval_steps: 5000
  save_steps: 10000
  logging_steps: 100
  fp16: true                       # 启用混合精度
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  use_wandb: false
  wandb_project: "rag-transformer"
  run_name: "7b_mamba_real"

# 数据集配置
datasets:
  primary: "openwebtext"
  secondary: ["c4", "the_pile"]
  mixing_ratio: [0.5, 0.3, 0.2]

# 显存优化设置
optimization:
  gradient_checkpointing: true     # 必须启用
  cpu_offload: false
  pin_memory: true
  num_workers: 4

# 系统配置
system:
  auto_shutdown: false
  shutdown_delay: 300
  save_memory: true

# Mamba优化
mamba_config:
  use_fast_path: true
  selective_scan: true
  hardware_aware: true 