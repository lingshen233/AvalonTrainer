#!/usr/bin/env python3
"""
æ•°æ®é›†ç®¡ç†å™¨
æ”¯æŒå¤šç§é«˜è´¨é‡è®­ç»ƒæ•°æ®é›†çš„ä¸‹è½½å’Œå¤„ç†
"""

import os
from datasets import load_dataset
from typing import Dict, List, Optional
import torch

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨"""
    
    # å¯ç”¨çš„æ•°æ®é›†é…ç½®
    AVAILABLE_DATASETS = {
        'openwebtext': {
            'name': 'openwebtext',
            'description': 'OpenWebText - é«˜è´¨é‡ç½‘é¡µæ–‡æœ¬ï¼ˆ40GB+ï¼‰',
            'size': '40GB+',
            'quality': 'â­â­â­â­â­',
            'lang': 'en',
            'recommended_for': ['1B', '7B']
        },
        'c4': {
            'name': 'c4',
            'config': 'en',
            'description': 'C4 (Colossal Clean Crawled Corpus) - T5è®­ç»ƒæ•°æ®',
            'size': '750GB+', 
            'quality': 'â­â­â­â­â­',
            'lang': 'en',
            'recommended_for': ['7B']
        },
        'the_pile': {
            'name': 'the_pile',
            'description': 'The Pile - å¤šé¢†åŸŸå¤§è§„æ¨¡æ–‡æœ¬é›†åˆï¼ˆ800GBï¼‰',
            'size': '800GB',
            'quality': 'â­â­â­â­â­',
            'lang': 'en',
            'recommended_for': ['7B']
        },
        'wikitext': {
            'name': 'wikitext',
            'config': 'wikitext-103-raw-v1',
            'description': 'WikiText-103 - ç»´åŸºç™¾ç§‘æ–‡ç« ï¼ˆ500MBï¼‰',
            'size': '500MB',
            'quality': 'â­â­â­â­',
            'lang': 'en',
            'recommended_for': ['1B']
        },
        'bookcorpus': {
            'name': 'bookcorpus',
            'description': 'BookCorpus - 11000+æœ¬ä¹¦ç±æ–‡æœ¬ï¼ˆ5GBï¼‰',
            'size': '5GB',
            'quality': 'â­â­â­â­',
            'lang': 'en', 
            'recommended_for': ['1B']
        },
        'cc_news': {
            'name': 'cc_news',
            'description': 'CC-News - Common Crawlæ–°é—»æ–‡ç« ï¼ˆ76GBï¼‰',
            'size': '76GB',
            'quality': 'â­â­â­â­',
            'lang': 'en',
            'recommended_for': ['1B', '7B']
        },
        'squad': {
            'name': 'squad',
            'description': 'SQuAD - é—®ç­”æ•°æ®é›†ï¼ˆ35MBï¼‰- å¿«é€Ÿæµ‹è¯•',
            'size': '35MB',
            'quality': 'â­â­â­',
            'lang': 'en',
            'recommended_for': ['test']
        },
        'chinese_webtext': {
            'name': 'BAAI/chinese_webtext',
            'description': 'ä¸­æ–‡ç½‘é¡µæ–‡æœ¬ - æ¸…æ´—åçš„ä¸­æ–‡è¯­æ–™ï¼ˆ20GB+ï¼‰',
            'size': '20GB+',
            'quality': 'â­â­â­â­',
            'lang': 'zh',
            'recommended_for': ['1B', '7B']
        },
        'chinese_poetry': {
            'name': 'chinese_poetry_collection',
            'description': 'ä¸­å›½å¤è¯—è¯é›†åˆ - ä¼ ç»Ÿæ–‡åŒ–è¯­æ–™ï¼ˆ100MBï¼‰',
            'size': '100MB', 
            'quality': 'â­â­â­',
            'lang': 'zh',
            'recommended_for': ['1B']
        }
    }
    
    def __init__(self):
        self.cache_dir = "./data_cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def list_datasets(self, model_size: Optional[str] = None, language: Optional[str] = None):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
        print("\nğŸ“š å¯ç”¨è®­ç»ƒæ•°æ®é›†:")
        print("=" * 80)
        
        for dataset_id, info in self.AVAILABLE_DATASETS.items():
            # è¿‡æ»¤æ¡ä»¶
            if model_size and model_size not in info['recommended_for']:
                continue
            if language and info['lang'] != language:
                continue
                
            print(f"\nğŸ—‚ï¸  {dataset_id}")
            print(f"   æè¿°: {info['description']}")
            print(f"   å¤§å°: {info['size']}")
            print(f"   è´¨é‡: {info['quality']}")
            print(f"   è¯­è¨€: {info['lang']}")
            print(f"   æ¨è: {', '.join(info['recommended_for'])}")
    
    def get_dataset_recommendations(self, model_size: str, language: str = 'en'):
        """æ ¹æ®æ¨¡å‹å¤§å°æ¨èæ•°æ®é›†"""
        recommendations = []
        
        for dataset_id, info in self.AVAILABLE_DATASETS.items():
            if (model_size in info['recommended_for'] and 
                info['lang'] == language):
                recommendations.append(dataset_id)
        
        return recommendations
    
    def download_dataset(self, dataset_id: str, streaming: bool = True, subset_size: Optional[int] = None):
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_id not in self.AVAILABLE_DATASETS:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset_id}")
        
        info = self.AVAILABLE_DATASETS[dataset_id]
        print(f"\nğŸ“¥ ä¸‹è½½æ•°æ®é›†: {info['description']}")
        print(f"é¢„è®¡å¤§å°: {info['size']}")
        
        try:
            if 'config' in info:
                dataset = load_dataset(
                    info['name'], 
                    info['config'],
                    streaming=streaming,
                    cache_dir=self.cache_dir
                )
            else:
                dataset = load_dataset(
                    info['name'],
                    streaming=streaming, 
                    cache_dir=self.cache_dir
                )
            
            print(f"âœ… {dataset_id} ä¸‹è½½æˆåŠŸ")
            
            # å¦‚æœæŒ‡å®šäº†å­é›†å¤§å°ï¼Œè¿›è¡Œé‡‡æ ·
            if subset_size and not streaming:
                if 'train' in dataset:
                    total_size = len(dataset['train'])
                    if total_size > subset_size:
                        dataset['train'] = dataset['train'].select(range(subset_size))
                        print(f"ğŸ“Š å·²é‡‡æ · {subset_size:,} æ¡æ•°æ®ï¼ˆåŸå§‹: {total_size:,}ï¼‰")
            
            return dataset
            
        except Exception as e:
            print(f"âŒ {dataset_id} ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def prepare_mixed_dataset(self, dataset_ids: List[str], weights: Optional[List[float]] = None):
        """å‡†å¤‡æ··åˆæ•°æ®é›†"""
        print(f"\nğŸ”€ å‡†å¤‡æ··åˆæ•°æ®é›†: {', '.join(dataset_ids)}")
        
        datasets = []
        successful_ids = []
        
        for dataset_id in dataset_ids:
            dataset = self.download_dataset(dataset_id, streaming=True)
            if dataset is not None:
                datasets.append(dataset)
                successful_ids.append(dataset_id)
        
        if not datasets:
            print("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ•°æ®é›†")
            return None
        
        print(f"âœ… æˆåŠŸå‡†å¤‡ {len(datasets)} ä¸ªæ•°æ®é›†")
        return datasets, successful_ids

def get_dataset_config_for_model_size(model_size: str):
    """ä¸ºä¸åŒæ¨¡å‹å¤§å°æ¨èæ•°æ®é›†é…ç½®"""
    manager = DatasetManager()
    
    configs = {
        '1B': {
            'primary': ['wikitext', 'bookcorpus'],
            'secondary': ['cc_news'],
            'chinese': ['chinese_webtext', 'chinese_poetry'],
            'mix_ratio': [0.4, 0.3, 0.3]
        },
        '7B': {
            'primary': ['openwebtext', 'c4'],
            'secondary': ['the_pile', 'cc_news'],
            'chinese': ['chinese_webtext'],
            'mix_ratio': [0.5, 0.3, 0.2]
        },
        'test': {
            'primary': ['squad', 'wikitext'],
            'secondary': [],
            'chinese': ['chinese_poetry'],
            'mix_ratio': [0.8, 0.2]
        }
    }
    
    return configs.get(model_size, configs['1B'])

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    manager = DatasetManager()
    
    print("ğŸš€ æ•°æ®é›†ç®¡ç†å™¨æ¼”ç¤º")
    
    # åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
    manager.list_datasets()
    
    # ä¸º1Bæ¨¡å‹æ¨èæ•°æ®é›†
    print("\n" + "="*50)
    recommendations = manager.get_dataset_recommendations('1B', 'en')
    print(f"1Bæ¨¡å‹æ¨èæ•°æ®é›†: {recommendations}")
    
    # ä¸º7Bæ¨¡å‹æ¨èæ•°æ®é›†
    recommendations = manager.get_dataset_recommendations('7B', 'en')
    print(f"7Bæ¨¡å‹æ¨èæ•°æ®é›†: {recommendations}") 