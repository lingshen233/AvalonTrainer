#!/usr/bin/env python3
"""
æ¨¡å‹è§„æ¨¡é¢„è®¾é…ç½®
æ”¯æŒ1Bã€7Bç­‰ä¸åŒè§„æ¨¡çš„æ¨¡å‹é…ç½®
"""

from .base import ModelConfig, TrainingConfig

# æ¨¡å‹è§„æ¨¡é¢„è®¾
MODEL_PRESETS = {
    # 1Bæ¨¡å‹é…ç½®
    '1b_transformer': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,  # GPT-2 vocab
            max_seq_length=2048,
            d_model=2048,      # å¢å¤§éšå±‚ç»´åº¦
            n_layers=24,       # 24å±‚
            n_heads=16,        # 16ä¸ªæ³¨æ„åŠ›å¤´
            d_ff=8192,         # å‰é¦ˆç½‘ç»œç»´åº¦
            dropout=0.1
        ),
        'description': 'Transformer 1B - é€šç”¨è¯­è¨€æ¨¡å‹',
        'params': '1.0B',
        'memory_estimate': '12GB/GPU',
        'recommended_gpus': [1, 2, 4],
        'datasets': ['wikitext', 'bookcorpus', 'cc_news']
    },
    
    '1b_mamba': {
        'model': ModelConfig(
            model_type='mamba',
            vocab_size=50257,
            max_seq_length=2048,
            d_model=2048,
            n_layers=24,
            d_state=16,        # MambaçŠ¶æ€ç»´åº¦
            d_conv=4,          # å·ç§¯æ ¸å¤§å°
            expand=2,          # æ‰©å±•å› å­
            dropout=0.1
        ),
        'description': 'Mamba 1B - é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹',
        'params': '1.0B',
        'memory_estimate': '9GB/GPU',
        'recommended_gpus': [1, 2, 4],
        'datasets': ['wikitext', 'bookcorpus', 'openwebtext']
    },
    
    # 7Bæ¨¡å‹é…ç½®
    '7b_transformer': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,
            max_seq_length=4096,   # æ›´é•¿åºåˆ—
            d_model=4096,          # æ›´å¤§éšå±‚
            n_layers=32,           # 32å±‚
            n_heads=32,            # 32ä¸ªæ³¨æ„åŠ›å¤´
            d_ff=16384,            # æ›´å¤§å‰é¦ˆç½‘ç»œ
            dropout=0.1
        ),
        'description': 'Transformer 7B - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹',
        'params': '7.0B',
        'memory_estimate': '28GB/GPU',
        'recommended_gpus': [4, 8],
        'datasets': ['openwebtext', 'c4', 'the_pile']
    },
    
    '7b_mamba': {
        'model': ModelConfig(
            model_type='mamba',
            vocab_size=50257,
            max_seq_length=4096,     # 7Båº”æœ‰çš„åºåˆ—é•¿åº¦
            d_model=4864,            # å¾®è°ƒåˆ°7B
            n_layers=45,             # å¾®è°ƒå±‚æ•°
            d_state=16,
            d_conv=4,
            expand=2,
            dropout=0.1
        ),
        'description': 'Mamba 7B - çœŸæ­£çš„7BçŠ¶æ€ç©ºé—´æ¨¡å‹',
        'params': '7.0B',
        'memory_estimate': '35GB/GPU',
        'recommended_gpus': [8],
        'datasets': ['openwebtext', 'c4', 'the_pile'],
        'notes': [
            'âš ï¸ çœŸæ­£çš„7Bæ¨¡å‹ï¼Œéœ€è¦40GBæ˜¾å­˜/GPUæˆ–8å¼ 24GB GPU',
            'å»ºè®®ä½¿ç”¨8å¼ A100-24GBæˆ–4å¼ A100-40GB',
            'å¦‚æ˜¾å­˜ä¸è¶³ï¼Œè¯·ä½¿ç”¨3b_mambaé¢„è®¾'
        ]
    },
    
    # æ·»åŠ è¯šå®çš„3Bé…ç½®
    '3b_mamba': {
        'model': ModelConfig(
            model_type='mamba',
            vocab_size=50257,
            max_seq_length=2048,
            d_model=3584,
            n_layers=32,
            d_state=16,
            d_conv=4,
            expand=2,
            dropout=0.1
        ),
        'description': 'Mamba 3B - é€‚åˆ24GB GPUçš„å¤§æ¨¡å‹',
        'params': '2.8B',
        'memory_estimate': '15GB/GPU',
        'recommended_gpus': [2, 4],
        'datasets': ['openwebtext', 'c4', 'bookcorpus']
    },
    
    # æµ‹è¯•é…ç½®
    'test_small': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,
            max_seq_length=512,
            d_model=512,
            n_layers=6,
            n_heads=8,
            d_ff=2048,
            dropout=0.1
        ),
        'description': 'å°å‹æµ‹è¯•æ¨¡å‹ - å¿«é€ŸéªŒè¯',
        'params': '50M',
        'memory_estimate': '2GB/GPU',
        'recommended_gpus': [1],
        'datasets': ['squad', 'wikitext']
    }
}

def get_training_config_for_model_size(model_size: str, num_gpus: int = 1):
    """æ ¹æ®æ¨¡å‹è§„æ¨¡ç”Ÿæˆè®­ç»ƒé…ç½®"""
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    base_configs = {
        '1B': {
            'train_batch_size': 4 if num_gpus == 1 else 6,
            'gradient_accumulation_steps': 4,
            'learning_rate': 3e-4,
            'max_steps': 100000,
            'warmup_steps': 2000,
            'eval_steps': 2000,
            'save_steps': 5000
        },
        '7B': {
            'train_batch_size': 2 if num_gpus <= 2 else 4,
            'gradient_accumulation_steps': 8,
            'learning_rate': 1e-4,
            'max_steps': 300000,
            'warmup_steps': 5000,
            'eval_steps': 5000,
            'save_steps': 10000
        },
        'test': {
            'train_batch_size': 8,
            'gradient_accumulation_steps': 1,
            'learning_rate': 5e-4,
            'max_steps': 1000,
            'warmup_steps': 100,
            'eval_steps': 200,
            'save_steps': 500
        }
    }
    
    # ç¡®å®šæ¨¡å‹è§„æ¨¡ç±»åˆ«
    if '1b' in model_size.lower():
        config_type = '1B'
    elif '7b' in model_size.lower():
        config_type = '7B'
    else:
        config_type = 'test'
    
    base_config = base_configs[config_type]
    
    return TrainingConfig(
        dataset_name='auto',  # è‡ªåŠ¨é€‰æ‹©
        train_batch_size=base_config['train_batch_size'],
        eval_batch_size=base_config['train_batch_size'],
        gradient_accumulation_steps=base_config['gradient_accumulation_steps'],
        max_length=2048 if config_type != 'test' else 512,
        learning_rate=base_config['learning_rate'],
        weight_decay=0.01,
        max_grad_norm=1.0,
        max_steps=base_config['max_steps'],
        warmup_steps=base_config['warmup_steps'],
        eval_steps=base_config['eval_steps'],
        save_steps=base_config['save_steps'],
        logging_steps=100,
        fp16=True,
        output_dir="./outputs",
        checkpoint_dir="./checkpoints",
        use_wandb=False,
        wandb_project="rag-transformer",
        wandb_run_name=f"{model_size}_{config_type}",
        distributed=(num_gpus > 1),
        world_size=num_gpus
    )

def list_model_presets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é¢„è®¾"""
    print("\nğŸ¤– å¯ç”¨æ¨¡å‹é¢„è®¾:")
    print("=" * 80)
    
    for preset_id, config in MODEL_PRESETS.items():
        print(f"\nğŸ“‹ {preset_id}")
        print(f"   æè¿°: {config['description']}")
        print(f"   å‚æ•°é‡: {config['params']}")
        print(f"   æ˜¾å­˜éœ€æ±‚: {config['memory_estimate']}")
        print(f"   æ¨èGPU: {config['recommended_gpus']}")
        print(f"   æ¨èæ•°æ®é›†: {', '.join(config['datasets'])}")

def get_model_preset(preset_id: str):
    """è·å–æŒ‡å®šçš„æ¨¡å‹é¢„è®¾"""
    if preset_id not in MODEL_PRESETS:
        available = list(MODEL_PRESETS.keys())
        raise ValueError(f"æœªçŸ¥é¢„è®¾ '{preset_id}'ï¼Œå¯ç”¨é¢„è®¾: {available}")
    
    return MODEL_PRESETS[preset_id]

def calculate_model_parameters(config: ModelConfig):
    """å‡†ç¡®ä¼°ç®—æ¨¡å‹å‚æ•°é‡"""
    if config.model_type == 'transformer':
        # Transformerå‚æ•°ä¼°ç®—
        embedding_params = config.vocab_size * config.d_model
        
        # æ¯ä¸€å±‚çš„å‚æ•°
        attention_params = 4 * config.d_model * config.d_model  # Q,K,V,O
        ffn_params = 2 * config.d_model * config.d_ff
        layer_params = attention_params + ffn_params
        
        total_params = embedding_params + config.n_layers * layer_params
        
    elif config.model_type == 'mamba':
        # Mambaå‚æ•°ç²¾ç¡®ä¼°ç®—
        embedding_params = config.vocab_size * config.d_model
        
        # æ¯å±‚Mambaå—å‚æ•°
        d_inner = config.d_model * config.expand  # å†…éƒ¨ç»´åº¦
        
        # è¾“å…¥æŠ•å½±å±‚ (x_proj, z_proj)
        in_proj_params = config.d_model * (d_inner * 2)
        
        # å·ç§¯å±‚
        conv_params = d_inner * config.d_conv
        
        # çŠ¶æ€ç©ºé—´å‚æ•° (A, B, C, dt)
        ss_params = d_inner * config.d_state  # A matrix
        ss_params += d_inner * config.d_state  # B projection
        ss_params += d_inner  # C projection
        ss_params += d_inner  # dt projection
        
        # è¾“å‡ºæŠ•å½±
        out_proj_params = d_inner * config.d_model
        
        # å±‚å½’ä¸€åŒ–å‚æ•°
        norm_params = config.d_model
        
        # æ¯å±‚æ€»å‚æ•°
        layer_params = in_proj_params + conv_params + ss_params + out_proj_params + norm_params
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–å’Œè¯­è¨€æ¨¡å‹å¤´
        final_norm_params = config.d_model
        lm_head_params = config.vocab_size * config.d_model
        
        total_params = embedding_params + (config.n_layers * layer_params) + final_norm_params + lm_head_params
    
    else:
        total_params = 0
    
    return total_params

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from configs.base import ModelConfig, TrainingConfig
    
    print("ğŸš€ æ¨¡å‹é¢„è®¾é…ç½®æ¼”ç¤º")
    
    # åˆ—å‡ºæ‰€æœ‰é¢„è®¾
    list_model_presets()
    
    # è·å–1B Transformeré…ç½®
    print("\n" + "="*50)
    preset = get_model_preset('1b_transformer')
    print(f"1B Transformeré…ç½®: {preset['description']}")
    
    # è®¡ç®—å‚æ•°é‡
    params = calculate_model_parameters(preset['model'])
    print(f"ä¼°ç®—å‚æ•°é‡: {params:,} ({params/1e9:.1f}B)") 