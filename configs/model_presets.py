#!/usr/bin/env python3
"""
æ¨¡å‹è§„æ¨¡é¢„è®¾é…ç½®
æ”¯æŒ1Bã€7Bç­‰ä¸åŒè§„æ¨¡çš„æ¨¡å‹é…ç½®
"""

from .base import ModelConfig, TrainingConfig

# æ¨¡å‹è§„æ¨¡é¢„è®¾
MODEL_PRESETS = {
    # 1Bæ¨¡å‹é…ç½®
    '1b_transformer': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,  # GPT-2 vocab
            max_seq_length=2048,
            d_model=2048,      # å¢å¤§éšå±‚ç»´åº¦
            n_layers=24,       # 24å±‚
            n_heads=16,        # 16ä¸ªæ³¨æ„åŠ›å¤´
            d_ff=8192,         # å‰é¦ˆç½‘ç»œç»´åº¦
            dropout=0.1
        ),
        'description': 'Transformer 1B - é€šç”¨è¯­è¨€æ¨¡å‹',
        'params': '1.0B',
        'memory_estimate': '12GB/GPU',
        'recommended_gpus': [1, 2, 4],
        'datasets': ['wikitext', 'bookcorpus', 'cc_news']
    },
    
    '1b_mamba': {
        'model': ModelConfig(
            model_type='mamba',
            vocab_size=50257,
            max_seq_length=2048,
            d_model=2048,
            n_layers=24,
            d_state=16,        # MambaçŠ¶æ€ç»´åº¦
            d_conv=4,          # å·ç§¯æ ¸å¤§å°
            expand=2,          # æ‰©å±•å› å­
            dropout=0.1
        ),
        'description': 'Mamba 1B - é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹',
        'params': '1.0B',
        'memory_estimate': '9GB/GPU',
        'recommended_gpus': [1, 2, 4],
        'datasets': ['wikitext', 'bookcorpus', 'openwebtext']
    },
    
    # 7Bæ¨¡å‹é…ç½®
    '7b_transformer': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,
            max_seq_length=4096,   # æ›´é•¿åºåˆ—
            d_model=4096,          # æ›´å¤§éšå±‚
            n_layers=32,           # 32å±‚
            n_heads=32,            # 32ä¸ªæ³¨æ„åŠ›å¤´
            d_ff=16384,            # æ›´å¤§å‰é¦ˆç½‘ç»œ
            dropout=0.1
        ),
        'description': 'Transformer 7B - å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹',
        'params': '7.0B',
        'memory_estimate': '28GB/GPU',
        'recommended_gpus': [4, 8],
        'datasets': ['openwebtext', 'c4', 'the_pile']
    },
    
    '7b_mamba': {
        'model': ModelConfig(
            model_type='mamba',
            vocab_size=50257,
            max_seq_length=4096,
            d_model=4096,
            n_layers=32,
            d_state=16,
            d_conv=4,
            expand=2,
            dropout=0.1
        ),
        'description': 'Mamba 7B - å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´æ¨¡å‹',
        'params': '7.0B',
        'memory_estimate': '20GB/GPU',
        'recommended_gpus': [2, 4, 8],
        'datasets': ['openwebtext', 'c4', 'the_pile']
    },
    
    # æµ‹è¯•é…ç½®
    'test_small': {
        'model': ModelConfig(
            model_type='transformer',
            vocab_size=50257,
            max_seq_length=512,
            d_model=512,
            n_layers=6,
            n_heads=8,
            d_ff=2048,
            dropout=0.1
        ),
        'description': 'å°å‹æµ‹è¯•æ¨¡å‹ - å¿«é€ŸéªŒè¯',
        'params': '50M',
        'memory_estimate': '2GB/GPU',
        'recommended_gpus': [1],
        'datasets': ['squad', 'wikitext']
    },
    
    "7b_mamba": {
        "description": "7B Mambaæ¨¡å‹ - é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹",
        "estimated_params": "7.0B",
        "memory_per_gpu": "20GB",
        "recommended_gpus": 4,
        "model_type": "mamba",
        "config": {
            "vocab_size": 50257,
            "max_seq_length": 4096,
            "d_model": 4096,
            "n_layers": 32,
            "d_state": 16,
            "d_conv": 4,
            "expand": 2,
            "dropout": 0.1
        },
        "training": {
            "batch_size": 2,
            "gradient_accumulation_steps": 16,
            "max_length": 4096,
            "learning_rate": 1e-4,
            "fp16": True,
            "gradient_checkpointing": True
        },
        "datasets": ["openwebtext", "c4", "bookcorpus"],
        "notes": [
            "æ¯”7B Transformeræ˜¾å­˜æ•ˆç‡æ›´é«˜",
            "æ”¯æŒæ›´é•¿åºåˆ—å¤„ç†",
            "çº¿æ€§æ—¶é—´å¤æ‚åº¦",
            "é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡"
        ]
    }
}

def get_training_config_for_model_size(model_size: str, num_gpus: int = 1):
    """æ ¹æ®æ¨¡å‹è§„æ¨¡ç”Ÿæˆè®­ç»ƒé…ç½®"""
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    base_configs = {
        '1B': {
            'train_batch_size': 4 if num_gpus == 1 else 6,
            'gradient_accumulation_steps': 4,
            'learning_rate': 3e-4,
            'max_steps': 100000,
            'warmup_steps': 2000,
            'eval_steps': 2000,
            'save_steps': 5000
        },
        '7B': {
            'train_batch_size': 2 if num_gpus <= 2 else 4,
            'gradient_accumulation_steps': 8,
            'learning_rate': 1e-4,
            'max_steps': 300000,
            'warmup_steps': 5000,
            'eval_steps': 5000,
            'save_steps': 10000
        },
        'test': {
            'train_batch_size': 8,
            'gradient_accumulation_steps': 1,
            'learning_rate': 5e-4,
            'max_steps': 1000,
            'warmup_steps': 100,
            'eval_steps': 200,
            'save_steps': 500
        }
    }
    
    # ç¡®å®šæ¨¡å‹è§„æ¨¡ç±»åˆ«
    if '1b' in model_size.lower():
        config_type = '1B'
    elif '7b' in model_size.lower():
        config_type = '7B'
    else:
        config_type = 'test'
    
    base_config = base_configs[config_type]
    
    return TrainingConfig(
        dataset_name='auto',  # è‡ªåŠ¨é€‰æ‹©
        train_batch_size=base_config['train_batch_size'],
        eval_batch_size=base_config['train_batch_size'],
        gradient_accumulation_steps=base_config['gradient_accumulation_steps'],
        max_length=2048 if config_type != 'test' else 512,
        learning_rate=base_config['learning_rate'],
        weight_decay=0.01,
        max_grad_norm=1.0,
        max_steps=base_config['max_steps'],
        warmup_steps=base_config['warmup_steps'],
        eval_steps=base_config['eval_steps'],
        save_steps=base_config['save_steps'],
        logging_steps=100,
        fp16=True,
        output_dir="./outputs",
        checkpoint_dir="./checkpoints",
        use_wandb=False,
        wandb_project="rag-transformer",
        wandb_run_name=f"{model_size}_{config_type}",
        distributed=(num_gpus > 1),
        world_size=num_gpus
    )

def list_model_presets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹é¢„è®¾"""
    print("\nğŸ¤– å¯ç”¨æ¨¡å‹é¢„è®¾:")
    print("=" * 80)
    
    for preset_id, config in MODEL_PRESETS.items():
        print(f"\nğŸ“‹ {preset_id}")
        print(f"   æè¿°: {config['description']}")
        print(f"   å‚æ•°é‡: {config['params']}")
        print(f"   æ˜¾å­˜éœ€æ±‚: {config['memory_estimate']}")
        print(f"   æ¨èGPU: {config['recommended_gpus']}")
        print(f"   æ¨èæ•°æ®é›†: {', '.join(config['datasets'])}")

def get_model_preset(preset_id: str):
    """è·å–æŒ‡å®šçš„æ¨¡å‹é¢„è®¾"""
    if preset_id not in MODEL_PRESETS:
        available = list(MODEL_PRESETS.keys())
        raise ValueError(f"æœªçŸ¥é¢„è®¾ '{preset_id}'ï¼Œå¯ç”¨é¢„è®¾: {available}")
    
    return MODEL_PRESETS[preset_id]

def calculate_model_parameters(config: ModelConfig):
    """ä¼°ç®—æ¨¡å‹å‚æ•°é‡"""
    if config.model_type == 'transformer':
        # Transformerå‚æ•°ä¼°ç®—
        embedding_params = config.vocab_size * config.d_model
        
        # æ¯ä¸€å±‚çš„å‚æ•°
        attention_params = 4 * config.d_model * config.d_model  # Q,K,V,O
        ffn_params = 2 * config.d_model * config.d_ff
        layer_params = attention_params + ffn_params
        
        total_params = embedding_params + config.n_layers * layer_params
        
    elif config.model_type == 'mamba':
        # Mambaå‚æ•°ä¼°ç®—ï¼ˆç®€åŒ–ï¼‰
        embedding_params = config.vocab_size * config.d_model
        
        # æ¯å±‚å‚æ•°ï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰
        layer_params = config.d_model * config.d_model * config.expand * 2
        layer_params += config.d_state * config.d_model  # çŠ¶æ€å‚æ•°
        
        total_params = embedding_params + config.n_layers * layer_params
    
    else:
        total_params = 0
    
    return total_params

if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from configs.base import ModelConfig, TrainingConfig
    
    print("ğŸš€ æ¨¡å‹é¢„è®¾é…ç½®æ¼”ç¤º")
    
    # åˆ—å‡ºæ‰€æœ‰é¢„è®¾
    list_model_presets()
    
    # è·å–1B Transformeré…ç½®
    print("\n" + "="*50)
    preset = get_model_preset('1b_transformer')
    print(f"1B Transformeré…ç½®: {preset['description']}")
    
    # è®¡ç®—å‚æ•°é‡
    params = calculate_model_parameters(preset['model'])
    print(f"ä¼°ç®—å‚æ•°é‡: {params:,} ({params/1e9:.1f}B)") 